#! /bin/env python
#! /usr/bin/env python2.7

###
# main wrapper will take cmdline input and run programs
###
import os
from SEAseqLib.mainLibrary import *
MASTER = os.getpid()
version = 'ALPHA 1.2'

def main():
    import sys
    try: cmd = sys.argv[1]
    except IndexError:
	sys.stderr.write('Please specify a command, if you need help run SEAseq help.\n')
	cmd = None

    if cmd and cmd not in ['Help','help','-help','--help','-h']: indata = getindata(cmd)
    elif not cmd: return 1
    else:	sys.stdout.write(
				    '\nProgram: SEAseq\n'
				    +'Version: '+version+'\n'
				    +'For analysis of SEAseq data.\n\n'
				    +'Usage:\tSEAseq <command> -path <analysis folder> [options]\n\n'
				    +'Commands:\n'
				    +'\tinit'		+' '.join(['' for i in range(20-len('init'))])			+'Initiate a new anlysis\n'
				    +'\taddfqs'		+' '.join(['' for i in range(20-len('addfqs'))])		+'Add fastq files to be analysed\n'
				    +'\tclusterbarcodes'+' '.join(['' for i in range(20-len('clusterbarcodes'))])	+'cluster reads according to barcode sequence\n'
				    +'\tsortreads'	+' '.join(['' for i in range(20-len('sortreads'))])		+'links barcode cluster information to each read pair\n'
				    +'\tmeta'		+' '.join(['' for i in range(20-len('meta'))])			+'metagenomics amplicon analysis, EXPRIMENTAL!\n'
				    +'\tsbatch'		+' '.join(['' for i in range(20-len('sbatch'))])		+'creates sbatch files for uppmaxruns\n'
				    +'\thelp'		+' '.join(['' for i in range(20-len('help'))])			+'print this help message and exit\n'
				    )

    if cmd == 'init':			init(indata)
    elif cmd == 'addfqs':		addfqs(indata)
    elif cmd == 'clusterbarcodes':	clusterbarcodes(indata)
    elif cmd == 'sortreads':		sortreads(indata)
    elif cmd == 'meta':			meta(indata)
    elif cmd == 'sbatch':		sbatch(indata)
    elif cmd[0] in ['H','h']:		pass
    elif cmd == None:			pass
    else: sys.stderr.write('Invalid command. Please specify a command, if you need help run SEAseq help.\n')

def sbatch(indata):
    config = Configuration(indata.path, indata.cmd)
    config.openconnections()
    writelogheader(config.logfile)

    #settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()
    
    
    import os
    import sys

    config.logfile.write('Creating sbatch scripts.\n')
    
    f = open(config.path +'/sbatch.cluster','w')
    f.write(
	'#! /bin/bash -l'+'\n'+
	'#SBATCH -A b2011011'+'\n'+
	'#SBATCH -n 8 -p node'+'\n'+
	'#SBATCH -t 24:00:00'+'\n'+
	'#SBATCH -J clust_'+config.path+'\n'+
	'#SBATCH -e '+config.abspath+'/stderr.cluster.sbatch.txt'+'\n'+
	'#SBATCH -o '+config.abspath+'/stdout.cluster.sbatch.txt'+'\n'+
	'#SBATCH --mail-type=All'+'\n'+
	'#SBATCH --mail-user=erik.borgstrom@scilifelab.se'+'\n'+
	'echo "$(date) Running on: $(hostname)"'+'\n'+
	'cd '+os.getcwd()+'\n'+
	'module load python/2.7'+'\n'+
	sys.argv[0]+' clusterbarcodes -path '+config.path+' -bm 3 -hm 4 -seed 2000 -p 8'+'\n'
    )
    f.close()

    f = open( config.path +'/sbatch.sortreads','w')
    f.write(
	'#! /bin/bash -l'+'\n'+
	'#SBATCH -A b2011011'+'\n'+
	'#SBATCH -n 8 -p node'+'\n'+
	'#SBATCH -C fat'+'\n'+
	'#SBATCH -t 24:00:00'+'\n'+
	'#SBATCH -J sort_'+config.path+'\n'+
	'#SBATCH -e '+config.abspath+'/stderr.sortreads.sbatch.txt'+'\n'+
	'#SBATCH -o '+config.abspath+'/stdout.sortreads.sbatch.txt'+'\n'+
	'#SBATCH --mail-type=All'+'\n'+
	'#SBATCH --mail-user=erik.borgstrom@scilifelab.se'+'\n'+
	'echo "$(date) Running on: $(hostname)"'+'\n'+
	'cd '+os.getcwd()+'\n'+
	'module load python/2.7'+'\n'+
	sys.argv[0]+' sortreads -path '+config.path+' -p8'+'\n'
    )
    f.close()
    
    f = open( config.path +'/sbatch.meta','w')
    f.write(
	'#! /bin/bash -l'+'\n'+
	'#SBATCH -A b2011011'+'\n'+
	'#SBATCH -n 8 -p devel'+'\n'+
	'#SBATCH -t 01:00:00'+'\n'+
	'#SBATCH -J meta_'+config.path+'\n'+
	'#SBATCH -e '+config.abspath+'/stderr.meta.sbatch.txt'+'\n'+
	'#SBATCH -o '+config.abspath+'/stdout.meta.sbatch.txt'+'\n'+
	'#SBATCH --mail-type=All'+'\n'+
	'#SBATCH --mail-user=erik.borgstrom@scilifelab.se'+'\n'+
	'echo "$(date) Running on: $(hostname)"'+'\n'+
	'cd '+os.getcwd()+'\n'+
	'module load python/2.7'+'\n'+
	sys.argv[0]+' meta -path '+config.path+' -p8'+'\n'
    )
    f.close()
    
    send = True
    if send:
	config.logfile.write('Placing scripts in jobqueue.\n')
	import subprocess

	sbatch = subprocess.Popen( ['sbatch',config.path +'/sbatch.cluster'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	sbatch_out, errdata = sbatch.communicate()
	if sbatch.returncode != 0:
		print 'sbatch view Error code', sbatch.returncode, errdata
		print sbatch_out
		sys.exit()
	cluster_jobid = sbatch_out.split('\n')[0].split(' ')[3]
	config.logfile.write('Queued barcode clustering with JOBID '+cluster_jobid+'.\n')

	sbatch = subprocess.Popen( ['sbatch','--dependency=afterok:'+cluster_jobid,config.path +'/sbatch.sortreads'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	sbatch_out, errdata = sbatch.communicate()
	if sbatch.returncode != 0:
		print 'sbatch view Error code', sbatch.returncode, errdata
		print sbatch_out
		sys.exit()
	sort_jobid = sbatch_out.split('\n')[0].split(' ')[3]
	config.logfile.write('Queued sorting of reads with JOBID '+sort_jobid+'.\n')

	sbatch = subprocess.Popen( ['sbatch','--dependency=afterok:'+sort_jobid,config.path +'/sbatch.meta'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	sbatch_out, errdata = sbatch.communicate()
	if sbatch.returncode != 0:
		print 'sbatch view Error code', sbatch.returncode, errdata
		print sbatch_out
		sys.exit()
	config.logfile.write('Queued metagenomics analysis with JOBID '+meta_jobid+'.\n')
    
    config.logfile.write('Done.\n')
    return 0

def writelogheader(logfile):
    import sys
    import time
    logfile.write('----------------\n')
    logfile.write('Running program: '+' '.join(sys.argv)+'.\n')
    logfile.write('Version: '+version+'\n')
    logfile.write('time: '+time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+'\n')
    logfile.write('Master process id='+str(MASTER)+'\n')

def init(indata):
    
    import os
    import sys
    tmp_log = ''

    if os.path.exists(indata.path+'/'+'config'):
	sys.stderr.write('This analysis has already been initiated try another command.\n')
	return
    
    config = Configuration(indata.path, indata.cmd)
    
    try:
	os.mkdir(indata.path)
	tmp_log += 'Folder '+indata.path+' created sucessfully.\n'
    except OSError as inst:
	if inst[0] != 17: print inst; return
	_continue = ''
	while not _continue or _continue[0] not in ['Y','y', 'N','n']:
	    _continue = raw_input('WARNING: the folder '+indata.path+' already excists. Continue anyway? (yes/no) ')
	tmp_log += 'WARNING: the folder '+indata.path+' already excists. Continue anyway? (yes/no) '+_continue+'\nUsing already exsisting folder ...\n'
	if _continue[0] in ['Y','y']: pass
	elif _continue[0] in ['N','n']: return
	else:
	    sys.stderr.write('Error 1\n.'); return 1

    config.openconnections()

    import time
    writelogheader(config.logfile)
    config.logfile.write(tmp_log)

    config.logfile.write('Creating config file:\n')
    config.save()

    config.logfile.write('Analysis '+config.abspath+' sucesfully initiated.\n')
    return 0

def addfqs(indata):

    config = Configuration(indata.path, indata.cmd)
    config.openconnections()
    
    writelogheader(config.logfile)

    config.logfile.write('Reading current config ...\n')
    config.load()

    config.logfile.write('Adding filenames to config file.\n')
    import os
    import sys
    r1 = os.path.abspath(indata.reads1.name)
    r2 = os.path.abspath(indata.reads2.name)
    if r1 in config.infiles['r1'] + config.infiles['r2'] or r2 in config.infiles['r1'] + config.infiles['r2']:
	sys.stderr.write('ERROR:\nat least one of the files:\n'+r1+'\n'+r2+'\nare already in the config file.\n');
	config.logfile.write('ERROR:\nat least one of the files:\n'+r1+'\n'+r2+'\nare already in the config file.\nExiting withiout changes.\n');
	return 1
    config.infiles['r1'].append(r1)
    config.infiles['r2'].append(r2)
    
    config.logfile.write('Getting readcount ...\n')
    config.readcounts.append(bufcount(r1)/4)
    
    config.save()
    
    config.logfile.write('Files '+r1+' and '+r2+' sucessfully added to infiles dist in config.\n\n')
    config.logfile.close()
    return 0

def foreachread_cluster(tmp):

    # unpack info
    pair, config = tmp
    del tmp
    
    # convert to SEAseq readpair
    pair = SEAseqpair(pair.header, pair.r1, pair.r2)

    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
    pair.identify(C_HANDLE, config)
    pair.getN15()
    
    return pair

def clusterbarcodes(indata):

    config = Configuration(indata.path, indata.cmd, skip=indata.skip ,stop=indata.stop ,random=indata.n)
    config.openconnections()
    
    writelogheader(config.logfile)

    # settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()
    config.set('chandlemissmatch',indata.handlemm)
    config.set('barcodemissmatch',indata.bcmm)
    config.set('numberofseeds', indata.seed)
    config.save()

    config.logfile.write('Part1: identifying barcode sequences in reads.\n')
    
    #deciding if to run multiprocessing or single process for debugging

    if indata.debug: #single process // serial
	results=[] # create holder for processed reads
	config.logfile.write('Running in debug mode identifying handles ...\n')
	progress = Progress(config.reads2process, logfile=config.logfile) # creates a progress "bar" thingy
	with progress:
	    for tmp in getPairs(config): #itarate through reads and do the "magicFunction"
		progress.update()
		results.append(foreachread_cluster(tmp))
	config.logfile.write('handle identification finished.\n')

    else: # multiple processes in parallel, create worker pool that iterates through the reads and does the "magicFunction" and sends results to "results"
	import multiprocessing
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=1000000) 
	results = WorkerPool.imap_unordered(foreachread_cluster,getPairs(config),chunksize=100)
    if not indata.debug: config.logfile.write('Running in multiproccessing mode using '+str(indata.cpus)+' processes  ...\n')
    else: config.logfile.write('Running the multiprocessing results handeling in serial ... \n')
    progress = Progress(config.reads2process, logfile=config.logfile)
    summary = SEAseqSummary()
    with progress:
	for pair in results:
	    progress.update()
	    summary.add(pair)
    if not indata.debug:WorkerPool.close()
    if not indata.debug:WorkerPool.join()
    config.outfile.write(str( summary.part1() )+'\n')
    config.logfile.write('Part1: finished barcode sequences identified.\n')
    
    config.logfile.write('Part2: Clustering the identified barcodes.\n')
    summary.reducebarcodes(config)
    config.logfile.write('Writing cluster info to file.\n')
    f = open(config.clusters_file,'w')
    f.write(str(summary.clusters))
    f.close()
    config.load()
    config.set('clustercount', len(summary.clusters))
    config.save()
    config.logfile.write('Part2: Clustering barcodes END\n----------\n')
    return summary

def foreachread_sort(tmp):

    # unpack info
    pair, config = tmp
    del tmp
    
    # convert to SEAseq readpair
    pair = SEAseqpair(pair.header, pair.r1, pair.r2)
    
    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
    pair.identify(C_HANDLE, config)
    pair.getN15()
    
    pair.get_cid(config)
    
    return pair

def load_clusters(config,queue):
    import multiprocessing
    import os
    config.logfile.write('\nLoading clustering data ...\n')
    config.logfile.write( 'Loader pid='+str(os.getpid())+'\n')
    summary = SEAseqSummary()
    summary.loadclusters(config.clusters_file)
    clustercount = len(summary.clusters)
    config.logfile.write('Data Loaded, reformatting ...\n')
    progress = Progress(len(summary.clusters), logfile=config.logfile, unit = 'cluster', mem = True)
    chunksize = 5000
    with progress:
	for cluster_id, infodist in summary.clusters.iteritems():
	    progress.update()
	    if int(infodist['total']) >= config.read_count_per_barcode_cluster_cutoff:
		for barcode in infodist['barcodes']:
		    config.cid_by_bc[barcode] = cluster_id
	    else: pass#print 'low read cluster'
    summary.clusters.clear()
    queue.put(clustercount)
    config.logfile.write('Done returning to master process.\n')

def sortreads(indata):

    config = Configuration(indata.path, indata.cmd, skip=indata.skip ,stop=indata.stop ,random=indata.n)
    config.openconnections()
    writelogheader(config.logfile)

    #settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()

    import multiprocessing
    man = multiprocessing.Manager()
    config.cid_by_bc = man.dict()
#    chunk_list = man.list([[[cluster+chunk*chunksize,[],[], 0, 0] for cluster in xrange(chunksize)] for chunk in xrange(config.clustercount/chunksize+1)])
    queue = multiprocessing.Queue()
    p = multiprocessing.Process(target=load_clusters,args=(config,queue))
    p.start()
    clustercount = queue.get()
    p.join()
    
    import os
    try: os.makedirs(config.path+'/sortedReads')
    except OSError:pass
    #try: os.makedirs(config.path+'/sortedReads/barcodes')
    #except OSError:pass

    if indata.debug: #single process // serial
	results=[] # create holder for processed reads
	config.logfile.write('Part1: Sorting reads to clusters ...\n')
	config.logfile.write('Running in debug mode processing reads ...\n')
	progress = Progress(config.reads2process, logfile=config.logfile, mem=True) # creates a progress "bar" thingy
	#itarate through reads and do the "magicFunction"
	with progress:
	    for tmp in getPairs(config):
		progress.update()
		results.append(foreachread_sort(tmp))
	config.logfile.write('finished, now sorting reads to clusters in memory ... \n')
    else: # multiple processes in parallel
	import multiprocessing
	#create worker pool that iterates through the reads and does the "magicFunction" and sends results to "results"
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=10000)
	results = WorkerPool.imap_unordered(foreachread_sort,getPairs(config),chunksize=1000)

    config.logfile.write('Allocating sorting memory  ...\n')
    chunksize = 5000
    #out_list = [[i, [],[]] for i in xrange(clustercount+1)]
    chunk_list = [[[cluster+chunk*chunksize,[],[]] for cluster in xrange(chunksize)] for chunk in xrange(config.clustercount/chunksize+1)]
#    chunk_list[0][0] = [0,None,None]
    config.logfile.write(' done.\n')

    if not indata.debug: config.logfile.write('Sorting reads to cluster '+str(indata.cpus)+' processes  ...\n')
    progress = Progress(config.reads2process, logfile=config.logfile, mem = True)
    with progress:
	if config.sortformat == 'fq':
	    f1 = open(config.path+'/sortedReads/sorted_by_barcode_cluster.1.fq','w')
	    f2 = open(config.path+'/sortedReads/sorted_by_barcode_cluster.2.fq','w')
	elif config.sortformat == 'fa':
	    f1 = open(config.path+'/sortedReads/sorted_by_barcode_cluster.fa','w')
	for pair in results:
	    progress.update()
	    if pair.cid:
		if config.sortformat == 'fq':
		    chunk_list[pair.cid/chunksize][pair.cid%chunksize][1].append('_'.join(pair.r1.header.split(' ')) + '_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r1.seq+'\n+\n'+pair.r1.qual+'\n')
		    chunk_list[pair.cid/chunksize][pair.cid%chunksize][2].append('_'.join(pair.r2.header.split(' ')) + '_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r2.seq+'\n+\n'+pair.r2.qual+'\n')
		elif config.sortformat == 'fa':
		    f1.write('>' + '_'.join(pair.r1.header.split(' ')) + '_r1_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r1.seq+'\n'+
			     '>' + '_'.join(pair.r2.header.split(' ')) + '_r2_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r2.seq+'\n')

    if config.sortformat == 'fq':
	config.logfile.write('\nWriting to fastq files sorted by cluster id ...\n')
	progress = Progress(clustercount, logfile=config.logfile, unit = 'cluster')
	with progress:
	    for chunk in chunk_list:
		for cluster in chunk:
			if cluster[0] <= config.clustercount: progress.update()
			for i in xrange(len(cluster[1])):
			    f1.write(cluster[1][i])
			    f2.write(cluster[2][i])	

    #close connections
    if config.sortformat in ['fa','fq']:	f1.close()
    if config.sortformat == 'fq':		f2.close()

    if not indata.debug:WorkerPool.close()
    if not indata.debug:WorkerPool.join()
    config.logfile.write('Reads sorted into sortedReads/sorted_by_barcode_cluster.1.fq\n')
    config.logfile.write('Part1: Sorting reads to clusters END\n----------\n')
    config.logfile.close()
    config.outfile.close()
    return 0

def meta(indata):

    config = Configuration(indata.path, indata.cmd, skip=indata.skip ,stop=indata.stop ,random=indata.n)
    import os
    os.remove(config.outfile)
    config.openconnections()
    
    writelogheader(config.logfile)

    # settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()
    
    import multiprocessing as mp
    man = mp.Manager()
    clusterq = man.Queue()
    # Run subprocess that get read pars and adds clusters to queue last add a END
    reader = mp.Process(target=getClustersAndPairs,args=(config,clusterq))
    reader.start()
    
    # make a worker pool that works with the clusters returned from the generator
    if indata.debug: #single process // serial
	config.logfile.write('Running in debug mode ...\n')
	results=[] # create holder for processed reads
	progress = Progress(config.clustercount, logfile=config.logfile) # creates a progress "bar" thingy
	with progress:
	    for cluster_pairs in clusteriterator(clusterq):
		progress.update()
		foreachcluster_meta(cluster_pairs)
	config.logfile.write('finished, making summary ... \n')
    else: # multiple processes in parallel
	import multiprocessing
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=10000)
	results = WorkerPool.imap_unordered(foreachcluster_meta,clusteriterator(clusterq),chunksize=10)

    if not indata.debug: config.logfile.write('Part1: Per cluster action '+str(indata.cpus)+' processes  ...\n')
    progress = Progress(config.clustercount, logfile=config.logfile, unit='cluster')
    with progress:
	for cluster in results:
	    progress.update()
	    if cluster:config.outfile.write( cluster )

    reader.join()
	
    # create a nice run summary
    config.logfile.write('Done.\n')
    return 0

def foreachcluster_meta(cluster_pairs):
    if cluster_pairs[0] == 0: return
    config = cluster_pairs[2]
    cid = cluster_pairs[1][0].cid

    # for each cluster:
	# remove reads containing illumina adapter sequences
	# cut the N15 and C-handle
	# idetify if read primers are 16S, ITS, MIX or unknown
	# get consensus sequences for each class of primer combos
	# identify the species corresponding to each consensus
	# create cluster output/summary
    
    adaptercount = 0
    
    output = ''
    if len(cluster_pairs[1])>=0:
	output =  '\n###--- Cluster number '+str(cluster_pairs[1][0].cid)+' -> '+str(len(cluster_pairs[1]))+' pairs. ---###\n'
	reads = {1:{},2:{}}

	output += 'PAIRS:\n'
	import re
	
	f = open(config.path+'/sortedReads/temporary.'+str(cid)+'.fa','w')
	
	for pair in cluster_pairs[1]:
		
	    # convert to SEAseq readpair
	    pair = SEAseqpair(pair.header, pair.r1, pair.r2)
	    
	    output += pair.header +'\t'
	    
	    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
	    pair.identify(C_HANDLE, config)
	    pair.getN15()
	    pair.identifyIllumina(config)
	    
	    if (pair.r1.illuminaadapter or pair.r2.illuminaadapter): adaptercount +=1; output+='ADAPTER\n'; continue
	    
	    pair.p1 = None
	    p1_16S = re.match( 'GTG.CAGC.GCCGCGGTAA',    pair.r1.seq[pair.handle_end:])
	    p1_ITS = re.match( 'GG.CTTGTAC.CAC.GCCCGTC', pair.r1.seq[pair.handle_end:])
	    if	 p1_16S: output += '16S\t'; pair.p1 = '16S'
	    elif p1_ITS: output += 'ITS\t'; pair.p1 = 'ITS'
	    else:	 output += '???\t'; pair.p1 = '???'
	    
	    pair.p2 = None
	    p2_16S = re.match( 'ACA..TCAC..CACGAGCTGACGAC',pair.r2.seq)
	    p2_ITS = re.match( 'CTC..A.TGCC..GGCATCCACC',  pair.r2.seq)
	    if 	 p2_16S: output += '16S\t'; pair.p2 = '16S'
	    elif p2_ITS: output += 'ITS\t'; pair.p2 = 'ITS'
	    else:	 output += '???\t'; pair.p2 = '???'
	    
	    output += pair.r1.seq +' '+ pair.r2.seq+'\n'

#	    try:		reads[1][pair.r1.seq[pair.handle_end:]+'-(N)n-'+pair.r2.revcomp().seq].append(pair)
#	    except KeyError:	reads[1][pair.r1.seq[pair.handle_end:]+'-(N)n-'+pair.r2.revcomp().seq]=[pair]
	    
	    f.write(
		'>'+':'.join(pair.header.split(':')[5:-3])+'.p1='+str(pair.p1)+'.p2='+str(pair.p2)+'\n'+
		pair.r1.seq[pair.handle_end:]+'NNNNNNNNNN'+pair.r2.revcomp().seq+'\n'
	    )
	f.close()
#	import random
#	output += 'R1 Groups:\n'
#	seqs = reads[1].keys()
#	seqs.sort()
#	for read in seqs: pass
	    #if len(reads[1][read]) > 1:
		#output += '>'+str(len(reads[1][read]))+'st.rand='+str(random.randint(0,999999))+'.p1='+str(reads[1][read][0].p1)+'.p2= '+str(reads[1][read][0].p2)+'.'+str(cid)
		#output += '\n'+read+'\n'

####
	import subprocess
	from cStringIO import StringIO
	import time
	import multiprocessing
	tempo = time.time()
	#config.logfile.write('starting '+' '.join(['cd-hit',''])+'\n')
	cdhit = subprocess.Popen( ['cd-hit-454','-i',config.path+'/sortedReads/temporary.'+str(cid)+'.fa','-o',config.path+'/sortedReads/cluster.'+str(cid)+'.fa','-g','1','-c','0.9'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	cdhit_out, errdata = cdhit.communicate()
	if cdhit.returncode != 0:
		print 'cd-hit view Error code', cdhit.returncode, errdata
		sys.exit()
	#cd-hit_out = StringIO(cd-hit_out)
	#print cdhit_out
	seconds = round(time.time()-tempo,2)
	#config.logfile.write('cd-hit cluster '+str(cluster_pairs[1][0].cid)+' done after '+str(seconds/60/60)+'h '+str(seconds/60%60)+'min '+str(seconds%60)+'s, parsing result ... ')
	del cdhit

	ccc = subprocess.Popen( ['cdhit-cluster-consensus',config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr',config.path+'/sortedReads/temporary.'+str(cid)+'.fa',config.path+'/sortedReads/cluster.'+str(cid)+'.fa.consensus',config.path+'/sortedReads/cluster.'+str(cid)+'.fa.aligned'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	ccc_out, errdata = ccc.communicate()
	if ccc.returncode != 0:
		print 'ccc view Error code', ccc.returncode, errdata
		print ccc_out
		sys.exit()
	del ccc
####
	for info in [[config.path+'/sortedReads/cluster.'+str(cid)+'.fa','\nCD-HIT representative sequences:\n'], [config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr','\nClustering details:\n']]:
	    filename , message =info
	    f = open(filename)
	    tmp = f.read()
	    output += message
	    output += tmp
	    f.close()
    
	import os
	#os.remove(config.path+'/sortedReads/cluster.'+str(cid)+'.fa')
	#os.remove(config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr')
	#os.remove(config.path+'/sortedReads/temporary.'+str(cid)+'.fa')

	output += str(adaptercount)+' illumina adapters\n'

    return output#str(cluster_pairs[1][0].cid)+' has '+str(len(cluster_pairs[1]))+' read pairs'

def clusteriterator(clusterq):  # a generator that yields clusters from queue until it finds a cluster = END
    while True:
	pairs = clusterq.get()
	if pairs == 'END': break
	yield pairs
    
def getClustersAndPairs(config,clusterq):
    
    import os
    config.logfile.write('Reader initiated pid='+str(os.getpid())+'.\n')

    currentcluster = 0
    pairs = []
    config.infiles['r1'] = [config.path+'/sortedReads/sorted_by_barcode_cluster.1.fq']
    config.infiles['r2'] = [config.path+'/sortedReads/sorted_by_barcode_cluster.2.fq']
    for tmp in getPairs(config):

	pair, config = tmp
	del tmp
	
	# convert to SEAseq readpair
	pair = SEAseqpair(pair.header, pair.r1, pair.r2)
	
	#C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
	#pair.identify(C_HANDLE, config)
	#pair.getN15()
	
	pair.cid = int(pair.header.split(':')[-1].split('_')[1])
	
	if pair.cid == currentcluster:
	    pairs.append(pair)
	elif pair.cid == currentcluster+1:
	    clusterq.put([currentcluster,pairs,config])
	    currentcluster = pair.cid
	    pairs = [pair]
	else:
	    clusterq.put([currentcluster,pairs,config])
	    currentcluster = pair.cid
	    pairs = [pair]

    clusterq.put('END')
    config.logfile.write('Reader exiting.\n')

def initiate_file(filename, logfile, mode='w'):
    import os
    
    if type(logfile) == file and mode != 'r': logfile.write('Initiating '+filename+' ...\n')
    
    if mode == 'w' and os.path.exists(filename):
	tmp = filename
	filename = raw_input('WARNING: the file '+filename+' already excists. Enter an alternative filename (leave empty to overwrite):')
	if type(logfile) == file and mode != 'r': logfile.write('WARNING: the file '+filename+' already excists. Enter an alternative filename (leave empty to overwrite):')
	if not filename:
	    filename = tmp
	    if type(logfile) == file and mode != 'r': logfile.write('overwriting\n')
	#else:
	#    if type(indata.logfile) == file and mode != 'r': config.logfile.write('Creating file '+filename+'.\n')
    
    if mode =='ow': mode ='w'
    out = open(filename, mode,1)
    
    if type(logfile) == file and mode != 'r': logfile.write('File '+filename+' sucessfully initiated.\n')
    
    return out

def getindata(cmd):
    import argparse
    argparser = argparse.ArgumentParser(description='Analysis of SEAseq data.', formatter_class=argparse.RawTextHelpFormatter)
    argparser.add_argument('cmd')
    argparser.add_argument('--debug',	dest='debug', 	action='store_true', 			required=False,	default=False,	help='Debug (run as regular single process python script).')
    argparser.add_argument('-path',	dest='path',	metavar='<path>',	type=str,	required=True,	default=False,	help='Set the analysis path.')
    argparser.add_argument('--src',	dest='skipreadcounting', 	action='store_true', 			required=False,	default=False,	help='Skip read counting (for speeding up debug runs, sets readcount to 30M/infile).')
    if cmd == 'init': pass
    elif cmd == 'addfqs':
	argparser.add_argument(	'-r1',			dest='reads1',	metavar='FILE',				type=file,	required=True, 			help='Indata "fastq"-file read1.')
	argparser.add_argument(	'-r2',			dest='reads2',	metavar='FILE',				type=file,	required=True,	default=None,	help='Indata "fastq"-file read2.')
    elif cmd == 'clusterbarcodes':
	argparser.add_argument(	'-bm',			dest='bcmm',	metavar='N',				type=int,	required=False,	default=0,	help='Number off missmatches allowed in barcode sequence during clustering (default 0)')
	argparser.add_argument(	'-hm',			dest='handlemm',metavar='N',				type=int,	required=False,	default=0,	help='Number off missmatches allowed in handle sequence (default 0)')
        argparser.add_argument(	'-seed',		dest='seed',	metavar='N',				type=int,	required=False,	default=100,	help='Number of top barcodes (with most reads) to use as seeds in clustering(default 100)')
    elif cmd == 'sortreads':
        argparser.add_argument(	'-sortfmt',		dest='sortfmt',	metavar='[fa/fq]',			type=str,	required=False,	default='fq',	help='Format to output reads to fa=fasta or fq=fastq(default fastq)')
    if cmd == 'sortreads' or cmd == 'clusterbarcodes' or cmd == 'meta':
	argparser.add_argument(	'-p',			dest='cpus',	metavar='N',				type=int,	required=False,	default=1,	help='The number of processes to run in parallel (default 1).')
	argparser.add_argument(	'-skip',		dest='skip',	metavar='N',				type=int,	required=False,	default=0,	help='Skip the first N read pairs in files (default 0).')
	argparser.add_argument(	'-stop',		dest='stop',	metavar='N',				type=int,	required=False,	default=0,	help='Stop after N read pairs, set to 0 to disable (default 0).')
	argparser.add_argument(	'-random',		dest='n',	metavar='N',				type=int,	required=False,	default=0,	help='Use a random subset of N read pairs, this option is slower (default 0 = off). Can not be used in combination with "-skip" or "-stop"')
        argparser.add_argument(	'-mrc',			dest='mrc',	metavar='N',				type=int,	required=False,	default=1,	help='Minimum number of reads per cluster to consider it (default 1) DISABLED: tests from 10 to 1000')


    import sys
    indata = argparser.parse_args(sys.argv[1:])
    if indata.path[-1] == '/': indata.path=indata.path[:-1]

    return indata

class Configuration():
    
    def __init__ (self, path, cmd, stop=None, skip=None ,random=None ):

	# permanent
	self.path 		= path
	self.config		= self.path+'/'+'config'
	self.init_logfile	= self.path + '/' + 'init.log.txt'
	self.init_outfile	= self.path + '/' + 'init.out.txt'
	self.addfqs_logfile	= self.path + '/' + 'addfqs.log.txt'
	self.addfqs_outfile	= self.path + '/' + 'addfqs.out.txt'
	self.cluster_logfile	= self.path + '/' + 'cluster.log.txt'
	self.cluster_outfile	= self.path + '/' + 'cluster.out.txt'
	self.sortreads_logfile	= self.path + '/' + 'sort.log.txt'
	self.sortreads_outfile	= self.path + '/' + 'sort.out.txt'
	self.meta_logfile	= self.path + '/' + 'meta.log.txt'
	self.meta_outfile	= self.path + '/' + 'meta.out.txt'
	self.sbatch_logfile	= self.path + '/' + 'sbatch.log.txt'
	self.sbatch_outfile	= self.path + '/' + 'sbatch.out.txt'
	self.clusters_file	= self.path+'/barcode_clusters_dictionary'
	self.abspath		= None # not loaded or set

	# longlasting
	self.infiles		= {'r1':[],'r2':[]}
	self.readcounts		= []
	self.chandlemissmatch	= None
	self.barcodemissmatch	= None
	self.clustercount	= None
	self.numberofseeds	= None

	# for each run
	self.cmd		= cmd
	self.stop		= stop
	self.skip		= skip
	self.random		= random
	self.sortformat		= 'fq'
	self.read_count_per_barcode_cluster_cutoff = 1
	
	
	if cmd == 'init':
	    self.logfile = self.init_logfile
	    self.outfile = self.init_outfile
	elif cmd == 'addfqs':
	    self.logfile = self.addfqs_logfile
	    self.outfile = self.addfqs_outfile
	elif cmd == 'clusterbarcodes':
	    self.logfile = self.cluster_logfile
	    self.outfile = self.cluster_outfile
	elif cmd == 'sortreads':
	    self.logfile = self.sortreads_logfile
	    self.outfile = self.sortreads_outfile
	elif cmd == 'meta':
	    self.logfile = self.meta_logfile
	    self.outfile = self.meta_outfile
	elif cmd == 'sbatch':
	    self.logfile = self.sbatch_logfile
	    self.outfile = self.sbatch_outfile

    def getreads2process(self, ):
	
	self.logfile.write('Getting readcounts ...\n')
	total = 0
	for i in range(len(self.readcounts)):
	    rc = self.readcounts[i]
	    self.logfile.write(self.infiles['r1'][i]+' -> '+str(rc) +' reads.\n')
	    total += rc
	self.logfile.write(str(total)+' read pairs in fastq files.\n');
	
	# calculate the number of reads to process
	self.reads2process = total
	if self.skip: 	self.reads2process -= self.skip
	if self.stop: 	self.reads2process = self.stop
	if self.random:	self.reads2process = self.random

    def set(self,varname, value ):
	if varname == 'chandlemissmatch':	self.chandlemissmatch	= value
	elif varname == 'barcodemissmatch':	self.barcodemissmatch	= value
	elif varname == 'clustercount':		self.clustercount	= value
	elif varname == 'numberofseeds':	self.numberofseeds	= value
	elif varname == 'sortformat':		self.sortformat		= value
	elif varname == 'read_count_per_barcode_cluster_cutoff':self.read_count_per_barcode_cluster_cutoff = value
	else: raise ValueError

    def load(self, ):
	self.config = initiate_file(self.config, self.logfile , mode='r')
	for line in self.config:
	    if line.rstrip() == '# Absolute path:':
		self.abspath = self.config.next().rstrip()
	    if line.rstrip() == '# Infiles dictionary:':
		self.infiles = eval(self.config.next())
	    if line.rstrip() == '# Read counts list:':
		self.readcounts = eval(self.config.next())
	    if line.rstrip() == '# Number of cluster seeds:':
		self.numberofseeds = eval(self.config.next().rstrip())
	    if line.rstrip() == '# Number of barcode clusters identified:':
		self.clustercount = eval(self.config.next().rstrip())
	self.config.close()
	self.config = self.config.name

    def openconnections(self, ):
	if self.cmd == 'init':
	    self.outfile = initiate_file(self.outfile, self.logfile)
	    self.logfile = initiate_file(self.logfile, self.logfile)
	else:
	    self.outfile = initiate_file(self.outfile, self.logfile, mode='a')
	    self.logfile = initiate_file(self.logfile, self.logfile, mode='a')

    def save(self, ):

	if not os.path.exists(self.config):
	    self.config = initiate_file(self.config, self.logfile)
	    self.abspath = os.path.abspath(self.path)
	else:
	    self.config = initiate_file(self.config, self.logfile, mode='ow')

	self.logfile.write('Writing settings to config file ...\n')
	self.config.write(
	    '# Absolute path:\n'+str(self.abspath)+'\n'+
	    '# Infiles dictionary:\n'+str(self.infiles)+'\n'+
	    '# Read counts list:\n'+str(self.readcounts)+'\n'+
	    '# Number of cluster seeds:\n'+str(self.numberofseeds)+'\n'+
	    '# Number of barcode clusters identified:\n'+str(self.clustercount)+'\n'
	    )
	self.config.close()
	self.config = self.config.name

#####
#check if run or imported // call main() or not
#####
if __name__ == "__main__":
    main()
#END of script
