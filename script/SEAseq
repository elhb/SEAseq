#! /bin/env python
#! /usr/bin/env python2.7

###
# main wrapper will take cmdline input and run programs
###
import os
from SEAseqLib.mainLibrary import *
MASTER = os.getpid()
version = 'ALPHA 1.2'

def main():
    import sys
    try: cmd = sys.argv[1]
    except IndexError:
	sys.stderr.write('Please specify a command, if you need help run SEAseq help.\n')
	cmd = None

    if cmd and cmd not in ['Help','help','-help','--help','-h']: indata = getindata(cmd)
    elif not cmd: return 1
    else:	sys.stdout.write(
				    '\nProgram: SEAseq\n'
				    +'Version: '+version+'\n'
				    +'For analysis of SEAseq data.\n\n'
				    +'Usage:\tSEAseq <command> -path <analysis folder> [options]\n\n'
				    +'Commands:\n'
				    +'\tinit'		+' '.join(['' for i in range(20-len('init'))])			+'Initiate a new anlysis\n'
				    +'\taddfqs'		+' '.join(['' for i in range(20-len('addfqs'))])		+'Add fastq files to be analysed\n'
				    +'\tclusterbarcodes'+' '.join(['' for i in range(20-len('clusterbarcodes'))])	+'cluster reads according to barcode sequence\n'
				    +'\tsortreads'	+' '.join(['' for i in range(20-len('sortreads'))])		+'links barcode cluster information to each read pair\n'
				    +'\thelp'		+' '.join(['' for i in range(20-len('help'))])		+'print this help message and exit\n'
				    )

    if cmd == 'init':			init(indata)
    elif cmd == 'addfqs':		addfqs(indata)
    elif cmd == 'clusterbarcodes':	clusterbarcodes(indata)
    elif cmd == 'sortreads':		sortreads(indata)
    elif cmd[0] in ['H','h']:		pass
    elif cmd == None:			pass
    else: sys.stderr.write('Invalid command. Please specify a command, if you need help run SEAseq help.\n')

def init(indata):
    
    import os
    import sys
    tmp_log = ''

    if os.path.exists(indata.path+'/'+'config'):
	sys.stderr.write('This analysis has already been initiated try another command.\n')
	return
    
    try:
	os.mkdir(indata.path)
	tmp_log += 'Folder '+indata.path+' created sucessfully.\n'
    except OSError as inst:
	if inst[0] != 17: print inst; return
	_continue = ''
	while not _continue or _continue[0] not in ['Y','y', 'N','n']:
	    _continue = raw_input('WARNING: the folder '+indata.path+' already excists. Continue anyway? (yes/no) ')
	tmp_log += 'WARNING: the folder '+indata.path+' already excists. Continue anyway? (yes/no) '+_continue+'\nUsing already exsisting folder ...\n'
	if _continue[0] in ['Y','y']: pass
	elif _continue[0] in ['N','n']: return
	else:
	    sys.stderr.write('Error 1\n.'); return 1

    indata.outfile = initiate_file(indata.outfile, indata)
    indata.logfile = initiate_file(indata.logfile, indata)

    import time
    indata.logfile.write('Running program: '+' '.join(sys.argv)+'.\n')
    indata.logfile.write('Version: '+version+'\n')
    indata.logfile.write('time: '+time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+'\n')
    indata.logfile.write('Master process id='+str(MASTER)+'\n')
    indata.logfile.write(tmp_log)

    indata.logfile.write('Creating config file:\n')
    indata.config = initiate_file(indata.path+'/'+'config', indata)

    abspath = os.path.abspath(indata.path)
    infiles = {'r1':[],'r2':[]}

    indata.logfile.write('Writing settings to config file ...\n')
    indata.config.write(
	'# Absolute path:\n'+abspath+'\n'+
	'# Infiles dictionary:\n'+str(infiles)+'\n'
	)

    indata.logfile.write('Analysis '+abspath+' sucesfully initiated.\n')
    return 0

def addfqs(indata):

    indata.outfile = initiate_file(indata.outfile, indata, mode='a')
    indata.logfile = initiate_file(indata.logfile, indata, mode='a')

    import time
    import sys
    indata.logfile.write('Running program: '+' '.join(sys.argv)+'.\n')
    indata.logfile.write('Version: '+version+'\n')
    indata.logfile.write('time: '+time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+'\n')
    indata.logfile.write('Master process id='+str(MASTER)+'\n')

    indata.logfile.write('Reading current config ...\n')
    indata.config = initiate_file(indata.path+'/'+'config',indata , mode='r')
    before = ''
    after = ''
    for line in indata.config:
	if line.rstrip() == '# Infiles dictionary:':
	    infiles = eval(indata.config.next())
	    after = '\n'
	elif not after: before += line
	elif after:after += line
    indata.config.close()

    indata.logfile.write('Adding filenames to config file.\n')
    import os
    import sys
    r1 = os.path.abspath(indata.reads1.name)
    r2 = os.path.abspath(indata.reads2.name)
    if r1 in infiles['r1'] + infiles['r2'] or r2 in infiles['r1'] + infiles['r2']:
	sys.stderr.write('ERROR:\nat least one of the files:\n'+r1+'\n'+r2+'\nare already in the config file.\n');
	indata.logfile.write('ERROR:\nat least one of the files:\n'+r1+'\n'+r2+'\nare already in the config file.\nExiting withiout changes.\n');
	return 1
    infiles['r1'].append(r1)
    infiles['r2'].append(r2)
    indata.config = initiate_file(indata.path+'/'+'config', indata, mode='ow')
    indata.config.write(before+'# Infiles dictionary:\n'+str(infiles)+after)
    indata.config.close()
    indata.logfile.write('Files '+r1+' and '+r2+' sucessfully added to infiles dist in config.\n\n')
    indata.logfile.close()
    return 0

def foreachread_cluster(tmp):

    # unpack info
    pair, indata = tmp
    del tmp
    
    # convert to SEAseq readpair
    pair = SEAseqpair(pair.header, pair.r1, pair.r2)

    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
    pair.identify(C_HANDLE, indata)
    pair.getN15()
    
    #if indata.cid_by_bc: pair.get_cid(indata)
    
    return pair

def clusterbarcodes(indata):

    indata.outfile = initiate_file(indata.outfile, indata, mode='a')
    indata.logfile = initiate_file(indata.logfile, indata, mode='a')

    import time
    import sys
    indata.logfile.write('Running program: '+' '.join(sys.argv)+'.\n')
    indata.logfile.write('Version: '+version+'\n')
    indata.logfile.write('time: '+time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+'\n')
    indata.logfile.write('Master process id='+str(MASTER)+'\n')

    indata.logfile.write('Get infiles from config-file.\n')
    indata = readConfig(indata)
    
    # get the readcount
    indata = getreadcount(indata)

    indata.logfile.write('Part1: identifying barcode sequences in reads.\n')
    #deciding if to run multiprocessing or single process for debugging
    if indata.debug: #single process // serial
	results=[] # create holder for processed reads
	indata.logfile.write('Running in debug mode identifying handles ...\n')
	progress = Progress(indata.reads2process, logfile=indata.logfile) # creates a progress "bar" thingy
	with progress:
	    for tmp in getPairs(indata): #itarate through reads and do the "magicFunction"
		progress.update()
		results.append(foreachread_cluster(tmp))
	indata.logfile.write('handle identification finished.\n')
    else: # multiple processes in parallel
	import multiprocessing
	#create worker pool that iterates through the reads and does the "magicFunction" and sends results to "results"
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=1000000)
	results = WorkerPool.imap_unordered(foreachread_cluster,getPairs(indata),chunksize=100)
    if not indata.debug: indata.logfile.write('Running in multiproccessing mode using '+str(indata.cpus)+' processes  ...\n')
    else: indata.logfile.write('Running the multiprocessing results handeling in serial ... \n')
    progress = Progress(indata.reads2process, logfile=indata.logfile)
    summary = SEAseqSummary()
    with progress:
	for pair in results:
	    progress.update()
	    summary.add(pair)
    if not indata.debug:WorkerPool.close()
    if not indata.debug:WorkerPool.join()
    indata.outfile.write(str( summary.part1() )+'\n')
    indata.logfile.write('Part1: finished barcode sequences identified.\n')
    
    indata.logfile.write('Part2: Clustering the identified barcodes.\n')
    summary.reducebarcodes(indata)
    indata.logfile.write('Writing cluster info to file.\n')
    f = open(indata.path+'/clusters.tempfile','w')
    f.write(str(summary.clusters))
    f.close()
    indata.logfile.write('Part2: Clustering barcodes END\n----------\n')
    return summary

def foreachread_sort(tmp):

    # unpack info
    pair, indata = tmp
    del tmp
    
    # convert to SEAseq readpair
    pair = SEAseqpair(pair.header, pair.r1, pair.r2)
    
    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
    pair.identify(C_HANDLE, indata)
    pair.getN15()
    
    pair.get_cid(indata)
    
    return pair

def load_clusters(indata,queue):
    import multiprocessing
    import os
    indata.logfile.write('\nLoading clustering data ...\n')
    indata.logfile.write( 'Loader pid='+str(os.getpid())+'\n')
    summary = SEAseqSummary()
    summary.loadclusters(indata.path+'/clusters.tempfile')
    clustercount = len(summary.clusters)
    indata.logfile.write('Data Loaded, reformatting ...\n')
    progress = Progress(len(summary.clusters), logfile=indata.logfile, unit = 'cluster')
    with progress:
	for cluster_id, infodist in summary.clusters.iteritems():
	    progress.update()
	    if int(infodist['total']) >= indata.mrc:
		for barcode in infodist['barcodes']:
		    indata.cid_by_bc[barcode] = cluster_id
	    else: pass#print 'low read cluster'
    summary.clusters.clear()
    queue.put(clustercount)
    indata.logfile.write('Done returning to master process.\n')

def sortreads(indata):

    indata.outfile = initiate_file(indata.outfile, indata, mode='a')
    indata.logfile = initiate_file(indata.logfile, indata, mode='a')

    import time
    import sys
    import multiprocessing
    
    indata.logfile.write('Running program: '+' '.join(sys.argv)+'.\n')
    indata.logfile.write('Version: '+version+'\n')
    indata.logfile.write('time: '+time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+'\n')
    indata.logfile.write('Master process id='+str(MASTER)+'\n')

    indata.logfile.write('Get infiles from config-file.\n')
    indata = readConfig(indata)
    
    # get the readcount
    indata = getreadcount(indata)

    man = multiprocessing.Manager()
    indata.cid_by_bc = man.dict()
    queue = multiprocessing.Queue()
    p = multiprocessing.Process(target=load_clusters,args=(indata,queue))
    p.start()
    clustercount = queue.get()
    p.join()
    
    import os
    try: os.makedirs(indata.path+'/sortedReads')
    except OSError:pass
    #try: os.makedirs(indata.path+'/sortedReads/barcodes')
    #except OSError:pass

    indata.logfile.write('Allocating sorting memory  ...')
    out_dict = {}
    for i in xrange(clustercount+1):
	out_dict[i] = [i, [],[]]
    indata.logfile.write(' done.\n')
    
    if indata.debug: #single process // serial
	results=[] # create holder for processed reads
	indata.logfile.write('Part1: Sorting reads to clusters ...\n')
	indata.logfile.write('Running in debug mode sorting reads to clusters ...\n')
	progress = Progress(indata.reads2process, logfile=indata.logfile) # creates a progress "bar" thingy
	#itarate through reads and do the "magicFunction"
	with progress:
	    for tmp in getPairs(indata):
		progress.update()
		results.append(foreachread_sort(tmp))
	indata.logfile.write('sort finished, start writing to files ... \n')
    else: # multiple processes in parallel
	import multiprocessing
	#create worker pool that iterates through the reads and does the "magicFunction" and sends results to "results"
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=10000)
	results = WorkerPool.imap_unordered(foreachread_sort,getPairs(indata),chunksize=1000)

    if not indata.debug: indata.logfile.write('Part1: Sorting reads to cluster '+str(indata.cpus)+' processes  ...\n')
    progress = Progress(indata.reads2process, logfile=indata.logfile)
    with progress:
	if indata.sortfmt == 'fq':
	    f1 = open(indata.path+'/sortedReads/sorted.reads.1.fq','w')
	    f2 = open(indata.path+'/sortedReads/sorted.reads.2.fq','w')
	elif indata.sortfmt == 'fa':
	    f1 = open(indata.path+'/sortedReads/sorted.reads.fa','w')
	for pair in results:
	    progress.update()
	    if pair.cid:
		if indata.sortfmt == 'fq':
		    out_dict[pair.cid][1].append('_'.join(pair.r1.header.split(' ')) + '_' + str(pair.cid) + '_' + pair.n15.seq + '\n'
				+pair.r1.seq+'\n+\n'
				+pair.r1.qual+'\n')
		    out_dict[pair.cid][2].append('_'.join(pair.r2.header.split(' ')) + '_' + str(pair.cid) + '_' + pair.n15.seq + '\n'
				+pair.r2.seq+'\n+\n'
				+pair.r2.qual+'\n')
		elif indata.sortfmt == 'fa':
		    f1.write('>' + '_'.join(pair.r1.header.split(' ')) + '_r1_' + str(pair.cid) + '_' + pair.n15.seq + '\n'
			    +pair.r1.seq+'\n'+
			    '>' + '_'.join(pair.r2.header.split(' ')) + '_r2_' + str(pair.cid) + '_' + pair.n15.seq + '\n'
			    +pair.r2.seq+'\n')

	if indata.sortfmt == 'fq':
            indata.logfile.write('\nWriting to sorted byCid-fq-files ...\n')
	    progress = Progress(clustercount, logfile=indata.logfile, unit = 'cluster')
	    with progress:
		for cid,cluster in out_dict.iteritems():
			if cluster[0] <= clustercount: progress.update()
			for i in xrange(len(cluster[1])):
			    f1.write(cluster[1][i])
			    f2.write(cluster[2][i])

	#close connections
	if indata.sortfmt in ['fa','fq']:	f1.close()
	if indata.sortfmt == 'fq':		f2.close()

    if not indata.debug:WorkerPool.close()
    if not indata.debug:WorkerPool.join()
    indata.logfile.write('Reads sorted into cluster tempfiles\n')
    indata.logfile.write('Part1: Sorting reads to clusters END\n----------\n')
    return 0

def getreadcount(indata):
    indata.logfile.write('Getting readcounts ...\n')
    indata.numreads = 0
    for filename in indata.config['infiles']['r1']:
	if not indata.skipreadcounting: tmp = bufcount(filename)/4
	else: tmp = 30000000
	indata.logfile.write(filename+' -> '+str(tmp) +' reads.\n')
	indata.numreads += tmp
    indata.logfile.write(str(indata.numreads)+' read pairs in fastq files.\n');
    # calculate the number of reads to process
    indata.reads2process = indata.numreads
    if indata.skip: indata.reads2process -= indata.skip
    if indata.stop: indata.reads2process = indata.stop
    if indata.n:    indata.reads2process = indata.n
    return indata

def readConfig(indata):
    indata.logfile.write('Reading config-file ...\n')
    indata.config = initiate_file(indata.path+'/'+'config',indata , mode='r')
    for line in indata.config:
	if	line.rstrip() == '# Absolute path:':	    	abspath = indata.config.next().rstrip()
	elif	line.rstrip() == '# Infiles dictionary:':	infiles = eval(indata.config.next())
    indata.config.close()
    indata.config = {'abspath':abspath,'infiles':infiles}
    return indata

def initiate_file(filename, indata, mode='w'):
    import os
    
    if type(indata.logfile) == file and mode != 'r': indata.logfile.write('Initiating '+filename+' ...\n')
    
    if mode == 'w' and os.path.exists(filename):
	tmp = filename
	filename = raw_input('WARNING: the file '+filename+' already excists. Enter an alternative filename (leave empty to overwrite):')
	if type(indata.logfile) == file and mode != 'r': indata.logfile.write('WARNING: the file '+filename+' already excists. Enter an alternative filename (leave empty to overwrite):')
	if not filename:
	    filename = tmp
	    if type(indata.logfile) == file and mode != 'r': indata.logfile.write('overwriting\n')
	#else:
	#    if type(indata.logfile) == file and mode != 'r': indata.logfile.write('Creating file '+filename+'.\n')
    
    if mode =='ow': mode ='w'
    out = open(filename, mode,1)
    
    if type(indata.logfile) == file and mode != 'r': indata.logfile.write('File '+filename+' sucessfully initiated.\n')
    
    return out

def getindata(cmd):
    import argparse
    argparser = argparse.ArgumentParser(description='Analysis of SEAseq data.', formatter_class=argparse.RawTextHelpFormatter)
    argparser.add_argument('cmd')
    argparser.add_argument('--debug',	dest='debug', 	action='store_true', 			required=False,	default=False,	help='Debug (run as regular single process python script).')
    argparser.add_argument('-path',	dest='path',	metavar='<path>',	type=str,	required=True,	default=False,	help='Set the analysis path.')
    argparser.add_argument('--src',	dest='skipreadcounting', 	action='store_true', 			required=False,	default=False,	help='Skip read counting (for speeding up debug runs, sets readcount to 30M/infile).')
    if cmd == 'init': pass
    elif cmd == 'addfqs':
	argparser.add_argument(	'-r1',			dest='reads1',	metavar='FILE',				type=file,	required=True, 			help='Indata "fastq"-file read1.')
	argparser.add_argument(	'-r2',			dest='reads2',	metavar='FILE',				type=file,	required=True,	default=None,	help='Indata "fastq"-file read2.')
    elif cmd == 'clusterbarcodes':
	argparser.add_argument(	'-bm',			dest='bcmm',	metavar='N',				type=int,	required=False,	default=0,	help='Number off missmatches allowed in barcode sequence during clustering (default 0)')
        argparser.add_argument(	'-seed',		dest='seed',	metavar='N',				type=int,	required=False,	default=100,	help='Number of top barcodes (with most reads) to use as seeds in clustering(default 100)')
    elif cmd == 'sortreads':
        argparser.add_argument(	'-sortfmt',		dest='sortfmt',	metavar='[fa/fq]',			type=str,	required=False,	default='fq',	help='Format to output reads to fa=fasta or fq=fastq(default fastq)')
    if cmd == 'sortreads' or cmd == 'clusterbarcodes':
	argparser.add_argument(	'-p',			dest='cpus',	metavar='N',				type=int,	required=False,	default=1,	help='The number of processes to run in parallel (default 1).')
	argparser.add_argument(	'-skip',		dest='skip',	metavar='N',				type=int,	required=False,	default=0,	help='Skip the first N read pairs in files (default 0).')
	argparser.add_argument(	'-stop',		dest='stop',	metavar='N',				type=int,	required=False,	default=0,	help='Stop after N read pairs, set to 0 to disable (default 0).')
	argparser.add_argument(	'-random',		dest='n',	metavar='N',				type=int,	required=False,	default=0,	help='Use a random subset of N read pairs, this option is slower (default 0 = off). Can not be used in combination with "-skip" or "-stop"')
        argparser.add_argument(	'-mrc',			dest='mrc',	metavar='N',				type=int,	required=False,	default=1,	help='Minimum number of reads per cluster to consider it (default 1) DISABLED: tests from 10 to 1000')
	argparser.add_argument(	'-hm',			dest='handlemm',metavar='N',				type=int,	required=False,	default=0,	help='Number off missmatches allowed in handle sequence (default 0)')

    import sys
    indata = argparser.parse_args(sys.argv[1:])
    if indata.path[-1] == '/': indata.path=indata.path[:-1]

    indata.selftest = False

    if cmd == 'init':
	indata.outfile = indata.path + '/' + 'init.out.txt'
	indata.logfile = indata.path + '/' + 'init.log.txt'
    elif cmd == 'addfqs':
	assert indata.reads1 != indata.reads2, 'Error: read 1 and read 2 cannot be same file!\n'
	indata.outfile = indata.path + '/' + 'addfqs.out.txt'
	indata.logfile = indata.path + '/' + 'addfqs.log.txt'
    elif cmd == 'clusterbarcodes':
	indata.outfile = indata.path + '/' + 'clusterBarcodes.out.txt'
	indata.logfile = indata.path + '/' + 'clusterBarcodes.log.txt'    
    elif cmd == 'sortreads':
	indata.outfile = indata.path + '/' + 'sortReads.out.txt'
	indata.logfile = indata.path + '/' + 'sortReads.log.txt'    

    return indata

#####
#check if run or imported // call main() or not
#####
if __name__ == "__main__":
    main()
#END of script
