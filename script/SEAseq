#! /bin/env python
#! /usr/bin/env python2.7

###
# main wrapper will take cmdline input and run programs
###
import os
from SEAseqLib.mainLibrary import *
MASTER = os.getpid()
version = 'ALPHA 1.3'

#coloring constants:
color = True
if color:bs = "\033[1m";be = "\033[0;0m";PURPLE = '\033[95m';BLUE = '\033[94m';GREEN = '\033[92m';YELLOW = '\033[93m';RED = '\033[91m';ENDC = '\033[0m';BLACK = '\33['+'0;30'+'m';CYAN = '\33['+'0;36'+'m';GRAY = '\33['+'0;37'+'m';BBLUE = '\33['+'1;34'+'m';BRED = '\33['+'1;31'+'m';BGREEN = '\33['+'1;32'+'m';BCYAN = '\33['+'1;36'+'m';BPURPLE = '\33['+'1;35'+'m';BYELLOW = '\33['+'1;33'+'m';DGRAY = '\33['+'1;30'+'m'
else:bs = '';be = '';HEADER = '';BLUE = '';GREEN = '';YELLOW = '';RED = '';ENDC = '';PURPLE='';BLACK='';CYAN='';GRAY='';BBLUE='';BRED='';BGREEN='';BCYAN='';BPURPLE='';BYELLOW='';DGRAY='';

def main():
    import sys
    try: cmd = sys.argv[1]
    except IndexError:
	sys.stderr.write('Please specify a command, if you need help run SEAseq help.\n')
	cmd = None

    if cmd and cmd not in ['Help','help','-help','--help','-h']: indata = getindata(cmd)
    elif not cmd: return 1
    else:	sys.stdout.write(
				    '\nProgram: SEAseq\n'
				    +'Version: '+version+'\n'
				    +'For analysis of SEAseq data.\n\n'
				    +'Usage:\tSEAseq <command> -path <analysis folder> [options]\n\n'
				    +'Commands:\n'
				    +'\tinit'		+' '.join(['' for i in range(20-len('init'))])			+'Initiate a new anlysis\n'
				    +'\taddfqs'		+' '.join(['' for i in range(20-len('addfqs'))])		+'Add fastq files to be analysed\n'
				    +'\tclusterbarcodes'+' '.join(['' for i in range(20-len('clusterbarcodes'))])	+'cluster reads according to barcode sequence\n'
				    +'\tsortreads'	+' '.join(['' for i in range(20-len('sortreads'))])		+'links barcode cluster information to each read pair\n'
				    +'\tmeta'		+' '.join(['' for i in range(20-len('meta'))])			+'metagenomics amplicon analysis, EXPRIMENTAL!\n'
				    +'\tsbatch'		+' '.join(['' for i in range(20-len('sbatch'))])		+'creates and queues sbatch files for the clusterbarcodes, sortreads and meta steps\n'
				    +'\tmetagraph'	+' '.join(['' for i in range(20-len('metagraph'))])		+'generates pdf graphs from meta stats\n'
				    +'\tclassifymeta'	+' '.join(['' for i in range(20-len('classifymeta'))])		+'classify the consensussequences from the meta clusters\n'
				    +'\thelp'		+' '.join(['' for i in range(20-len('help'))])			+'print this help message and exit\n'
				    )

    if cmd == 'init':			init(indata)
    elif cmd == 'addfqs':		addfqs(indata)
    elif cmd == 'clusterbarcodes':	clusterbarcodes(indata)
    elif cmd == 'sortreads':		sortreads(indata)
    elif cmd == 'meta':			meta(indata)
    elif cmd == 'sbatch':		sbatch(indata)
    elif cmd == 'metagraph':		metagraph(indata)    
    elif cmd == 'classifymeta':		classifymeta(indata)
    elif cmd[0] in ['H','h']:		pass
    elif cmd == None:			pass
    else: sys.stderr.write('Invalid command. Please specify a command, if you need help run SEAseq help.\n')

def metagraph(indata):
    
    config = Configuration(indata.path, indata.cmd)
    config.openconnections()
    writelogheader(config.logfile)

    import os
    try:
	os.mkdir(config.path+'/graphs')
    except: pass


    # settings
    config.load()
    
    if indata.graphs == 'all': indata.graphs = 'abcdefghijklmnopqrstuvxyz';
    indata.graphs = list(indata.graphs)
    
    config.logfile.write('Loading data ... \n')
    f = open(config.path+'/meta.statstable','r')
    data = f.read()
    f.close()
    header = data.split('\n')[0].split('\t')
    data = data.split('\n')[1:]
    tmp = {}
    incomplete = False
    for cluster in data:
	cluster = cluster.split('\t')
	cid = cluster[0]
	#assert len(cluster) == len(header), '\nhead='+ str(header) +'\nclust='+ str(cluster)+'\n'
	if len(cluster) == len(header): tmp[cid] = {}
	else:
	    incomplete = True
	    print 'Warning errounus row in stat table.'
	    config.logfile.write('Warning errounus row in stat table, you are creating graphs for a incomplete dataset, SEAseq meta is probably still running.\n')
	    continue
	for i in xrange(len(header)):
	    name = header[i]
	    value = cluster[i]
	    tmp[cid][name] = eval(value)
	    #try: tmp[cid][name] = eval(value)
	    #except TypeError: print cid,name,value
    data = tmp
    config.logfile.write('Loaded.\n')

    config.logfile.write('Setting scale and update intervall ... \n')
    xscale = [int(i) for i in indata.xscale.split('-')]
    yscale = [int(i) for i in indata.yscale.split('-')]
    
    #['clusterid','number of reads in total','number of adaper reads','number of strange primers','its reads','16s reads','its','16s','its monoclonal','16s monoclonal','number of consensus types','number of consensus types with good support']
    
   
    if not indata.highres:
	step = xscale[1]/200
	if indata.step:
	    step = indata.step
	step = max([step,1])
	x_range = range(xscale[0],xscale[1]+1, step)
	config.logfile.write('Updates y every '+str(step)+'th x value (x = '+', '.join([	str(i) for i in x_range[ : min( [ 5,len(x_range) ] ) ]	])+' ... '+str(xscale[1])+').\n')
    else:
	x_range = range(xscale[0],xscale[1]+1)
	config.logfile.write('Highres: updates y every x value (step = 1).\n')


    config.logfile.write('Using a total of '+str(len(data))+' clusters.\n')
    
    graph_info = {'good':{},'total':{}}
    for x_current in x_range:
	for rc_type in ['total','good']:
	    graph_info[rc_type][ x_current ] = {'all':0,'16s':0,'its':0,'both':0,'None':0,'16s_mono':0,'its_mono':0,'both_mono':0,'both_16s_mono':0,'both_its_mono':0}

    config.logfile.write('Counting clusters ... \n')
    for cid in data:
	for x_current in x_range:
	    breaker = []
	    compare_pairs = [	['total',data[cid]['number of reads in total']],    ['good',data[cid]['number of reads in total']-data[cid]['number of adaper reads']-data[cid]['number of strange primers']]    ]
	    for tmp in compare_pairs:
		[rc_type, comp_value] = tmp
	    
		if comp_value > x_current:
		    graph_info[rc_type][ x_current ]['all'] += 1

		    if data[cid]['16s'] and not data[cid]['its']:
			graph_info[rc_type][ x_current ]['16s'] += 1
			if data[cid]['16s monoclonal']: graph_info[rc_type][ x_current ]['16s_mono'] += 1
		    
		    if data[cid]['its'] and not data[cid]['16s']:
			graph_info[rc_type][ x_current ]['its'] += 1
			if data[cid]['its monoclonal']: graph_info[rc_type][ x_current ]['its_mono'] += 1
			
		    if data[cid]['16s'] and data[cid]['its']:
			graph_info[rc_type][ x_current ]['both'] += 1
			if data[cid]['16s monoclonal'] and data[cid]['its monoclonal']: graph_info[rc_type][ x_current ]['both_mono'] += 1
			if not data[cid]['16s monoclonal'] and data[cid]['its monoclonal']: graph_info[rc_type][ x_current ]['both_its_mono'] += 1
			if data[cid]['16s monoclonal'] and not data[cid]['its monoclonal']: graph_info[rc_type][ x_current ]['both_16s_mono'] += 1

		    if not data[cid]['16s'] and not data[cid]['its']:
			graph_info[rc_type][ x_current ]['None'] += 1
		else: breaker.append(True)
	    if len(breaker) == len(compare_pairs): break

    config.logfile.write('Calculating percentages ... \n')
    for rc_type in ['total','good']:
	f = open( config.path+'/graphs/'+rc_type+'_read_pairs_per_barcode_cluster.x_scale_'+str(xscale[0])+'-'+str(xscale[1])+'.y_scale_'+str(yscale[0])+'-'+str(yscale[1])+'.values' ,'w' )
	f.write(
	    'x'			+'\t'+ 
	    'all'			+'\t'+ 
	    'None'			+'\t'+ 
	    '16s'			+'\t'+ 
	    '16s_mono'		+'\t'+ 
	    '16s_mono %'		+'\t'+ 
	    'its'			+'\t'+ 
	    'its_mono'		+'\t'+ 
	    'its_mono %'		+'\t'+ 
	    'both'			+'\t'+ 
	    'both_mono'		+'\t'+ 
	    'both_mono %'		+'\t'+ 
	    'both'			+'\t'+ 
	    'both_16s_mono'		+'\t'+ 
	    'both_16s_mono %'	+'\t'+ 
	    'both'			+'\t'+ 
	    'both_its_mono'		+'\t'+ 
	    'both_its_mono %'	+'\n'
	)
	f.close()
    for x_current in x_range:
	for rc_type in ['total','good']:
	    f = open( config.path+'/graphs/'+rc_type+'_read_pairs_per_barcode_cluster.x_scale_'+str(xscale[0])+'-'+str(xscale[1])+'.y_scale_'+str(yscale[0])+'-'+str(yscale[1])+indata.sample+'.values' ,'a' )
	    f.write(str(x_current) +'\t'+     str(graph_info[rc_type][ x_current ]['all'])	+'\t'+	str(graph_info[rc_type][ x_current ]['None'])	+'\t'	)

	    for [total_id,count_id] in [['16s','16s_mono'],['its','its_mono'],['both','both_mono'],['both','both_16s_mono'],['both','both_its_mono']]:
		total  = graph_info[rc_type][ x_current ][total_id]
		count = graph_info[rc_type][ x_current ][count_id]
		
		f.write(	str(total)+'\t'+	str(count)+'\t'    )
		if total:
		    graph_info[rc_type][ x_current ][count_id] = 100*round(float(count)/float(total),2)
		    f.write(str(100*round(float(count)/float(total),2))+'\t')
		else:
		    graph_info[rc_type][ x_current ][count_id] = 0.0
		    f.write('0.0\t')
	    f.write('\t'+str(x_current)+'\n')
	    f.close()

    if 'a' in indata.graphs:
	config.logfile.write('Preparing variables for plotting ... \n')
	for rc_type in ['total','good']:

	    temp_x=graph_info[rc_type].keys()
	    temp_x.sort()
	    x =[];y1=[];y2=[];y3=[];y4=[];y5=[];y6=[];y7=[];y8=[];y9=[];y10=[]
	    for i in temp_x:
		    x.append(i)
		    y1.append(graph_info[rc_type][i]['all'])
		    y2.append(graph_info[rc_type][i]['both'])
		    y3.append(graph_info[rc_type][i]['16s'])
		    y4.append(graph_info[rc_type][i]['its'])
		    y5.append(graph_info[rc_type][i]['None'])
		    y6.append(graph_info[rc_type][i]['16s_mono'])
		    y7.append(graph_info[rc_type][i]['its_mono'])
		    y8.append(graph_info[rc_type][i]['both_mono'])
		    y9.append(graph_info[rc_type][i]['both_its_mono'])
		    y10.append(graph_info[rc_type][i]['both_16s_mono'])

	    config.logfile.write('Creating graphics ... \n')
	    import numpy as np
	    import matplotlib.pyplot as plt
	    from matplotlib import rc
		    
	    fig = plt.figure(figsize=(20, 15), dpi=100)
	    ax = fig.add_subplot(111)
	    if incomplete: ax.set_title('WARNING: incomplete dataset! '+ indata.sample+' ' +config.path)
	    else : ax.set_title(indata.sample+' ' +config.path)
	    ax.plot(x, y6, '--g', label = 'Percentage monoclonal 16S cluster')
	    ax.plot(x, y7, '--y', label = 'Percentage monoclonal ITS cluster')
	    ax.plot(x, y8, '--b', label = 'Perc. both mono. both in cluster')
	    ax.plot(x, y9, '--r', label = 'Perc. ITS mono. both in cluster')
	    ax.plot(x, y10, '--k', label = 'Perc. 16S mono. both in cluster')
	    ax2 = ax.twinx()
	    ax2.plot(x, y1, '-r', label = 'Total number of clusters')
	    ax2.plot(x, y2, '-b', label = 'Number of 16S/ITS clusters')
	    ax2.plot(x, y3, '-g', label = 'Number of 16S only clusters')
	    ax2.plot(x, y4, '-y', label = 'Number of ITS only clusters')
	    ax2.plot(x, y5, '-k', label = 'Number of undefined clusters')
	    
	    lines, labels   = ax.get_legend_handles_labels()
	    lines2, labels2 = ax2.get_legend_handles_labels()
	    ax2.legend(lines + lines2, labels + labels2, loc=7)
	    #ax.legend(lines, labels, loc=7)

	    ax.grid(b=True, which='both')
	    ax.set_xlabel('Read pairs per Barcode Cluster ( '+rc_type+' reads )')
	    ax.set_ylabel('Percentage Monoclonal')
	    ax2.set_ylabel('Number of Clusters')
	    
	    ax.set_ylim(0,100)
	    ax2.set_ylim(yscale[0],yscale[1])
	    
	    ax.set_xlim(xscale[0],xscale[1])
	    ax2.set_xlim(xscale[0],xscale[1])

	    ax.set_xticks(np.arange(xscale[0],xscale[1]+1,xscale[1]/10))
	    ax2.set_yticks(np.arange(yscale[0],yscale[1]+1,min(100,yscale[1]/10)))
	    ax.set_yticks(np.arange(0,101,5))

	    plt.savefig(config.path+'/graphs/'+rc_type+'_read_pairs_per_barcode_cluster.x_scale_'+str(xscale[0])+'-'+str(xscale[1])+'.y_scale_'+str(yscale[0])+'-'+str(yscale[1])+indata.sample+'.pdf')
	    config.logfile.write('Created: '+config.path+'/graphs/'+rc_type+'_read_pairs_per_barcode_cluster.x_scale_'+str(xscale[0])+'-'+str(xscale[1])+'.y_scale_'+str(yscale[0])+'-'+str(yscale[1])+indata.sample+' (.pdf and .values)'+'.\n')
	    plt.close()

    if 'b' in indata.graphs:
	config.logfile.write('Preparing variables for plotting ... \n')
	for cluster_type in ['its','16s','both']:

	    temp_x=graph_info['total'].keys()
	    temp_x.sort()
	    
	    x =[];y1=[];y2=[];y3=[];y4=[];y5=[];y6=[];y7=[];y8=[];

	    for i in temp_x:
		    x.append(i)
		    y1.append(graph_info['good' ][i][cluster_type])
		    y2.append(graph_info['total'][i][cluster_type])
		    y3.append(graph_info['good' ][i][cluster_type+'_mono'])
		    y4.append(graph_info['total'][i][cluster_type+'_mono'])
		    if cluster_type == 'both':
			y5.append(graph_info['good' ][i][cluster_type+'_its_mono'])
			y6.append(graph_info['good' ][i][cluster_type+'_16s_mono'])
			y7.append(graph_info['total'][i][cluster_type+'_its_mono'])
			y8.append(graph_info['total'][i][cluster_type+'_16s_mono'])

	    config.logfile.write('Creating graphics ... \n')
	    import numpy as np
	    import matplotlib.pyplot as plt
	    from matplotlib import rc
		    
	    fig = plt.figure(figsize=(20, 15), dpi=100)
	    ax = fig.add_subplot(111)
	    
	    if incomplete:	ax.set_title('WARNING: incomplete dataset! '+'Clusters with '+cluster_type+' consensus sequence(s)'+'. '+indata.sample+' '+config.path)
	    else:		ax.set_title(				     'Clusters with '+cluster_type+' consensus sequence(s)'+'. '+indata.sample+' '+config.path)

	    ax.plot(x, y3, '--b', label =     'Perc. mono. good pairs')
	    ax.plot(x, y4, '--r', label =     'Perc. mono. totalpairs')
	    if cluster_type == 'both':
		ax.plot(x, y5, '--g', label = '% mono.its good pairs')
		ax.plot(x, y7, '--m', label = '% mono.its totalpairs')
		ax.plot(x, y6, '--c', label = '% mono.16s good pairs')
		ax.plot(x, y8, '--y', label = '% mono.16s totalpairs')

	    ax2 = ax.twinx()
	    ax2.plot(x, y1, '-b', label =     'Number of good pairs')
	    ax2.plot(x, y2, '-r', label =     'Number of totalpairs')
	    
	    lines, labels   = ax.get_legend_handles_labels()
	    lines2, labels2 = ax2.get_legend_handles_labels()
	    ax2.legend(lines + lines2, labels + labels2, loc=7)
	    #ax.legend(lines, labels, loc=7)

	    ax.grid(b=True, which='both')
	    ax.set_xlabel('Read pairs per Barcode Cluster')
	    ax.set_ylabel('Percentage Monoclonal')
	    ax2.set_ylabel('Number of Clusters')
	    
	    ax.set_ylim(0,100)
	    ax2.set_ylim(yscale[0],yscale[1])
	    
	    ax.set_xlim(xscale[0],xscale[1])
	    ax2.set_xlim(xscale[0],xscale[1])

	    ax.set_xticks(np.arange(xscale[0],xscale[1]+1,xscale[1]/10))
	    ax2.set_yticks(np.arange(yscale[0],yscale[1]+1,min(100,yscale[1]/10)))
	    ax.set_yticks(np.arange(0,101,5))

	    plt.savefig(                     config.path+'/graphs/'+cluster_type+'_read_pairs_per_barcode_cluster.x_scale_'+str(xscale[0])+'-'+str(xscale[1])+'.y_scale_'+str(yscale[0])+'-'+str(yscale[1])+indata.sample+'.pdf')
	    config.logfile.write('Created: '+config.path+'/graphs/'+cluster_type+'_read_pairs_per_barcode_cluster.x_scale_'+str(xscale[0])+'-'+str(xscale[1])+'.y_scale_'+str(yscale[0])+'-'+str(yscale[1])+indata.sample+' (.pdf and .values)'+'.\n')
	    plt.close()

    config.logfile.write( 'done\n')
    return 0

def sbatch(indata):
    config = Configuration(indata.path, indata.cmd)
    config.openconnections()
    writelogheader(config.logfile)

    #settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()
    
    
    import os
    import sys

    config.logfile.write('Creating sbatch scripts.\n')
    
    f = open(config.path +'/sbatch.cluster.sh','w')
    f.write(
	'#! /bin/bash -l'+'\n'+
	'#SBATCH -A b2011011'+'\n'+
	'#SBATCH -n 8 -p node'+'\n'+
	'#SBATCH -t 24:00:00'+'\n'+
	'#SBATCH -J clust_'+config.path+'\n'+
	'#SBATCH -e '+config.abspath+'/sbatch.cluster.stderr.txt'+'\n'+
	'#SBATCH -o '+config.abspath+'/sbatch.cluster.stdout.txt'+'\n'+
	'#SBATCH --mail-type=All'+'\n'+
	'#SBATCH --mail-user=erik.borgstrom@scilifelab.se'+'\n'+
	'echo "$(date) Running on: $(hostname)"'+'\n'+
	'cd '+os.getcwd()+'\n'+
	'module load python/2.7'+'\n'+
	sys.argv[0]+' clusterbarcodes -path '+config.path+' -bm 3 -hm 4 -seed 2000 -p 8'+'\n'
    )
    f.close()

    f = open( config.path +'/sbatch.sortreads.sh','w')
    f.write(
	'#! /bin/bash -l'+'\n'+
	'#SBATCH -A b2011011'+'\n'+
	'#SBATCH -n 8 -p node'+'\n'+
	'#SBATCH -C fat'+'\n'+
	'#SBATCH -t 24:00:00'+'\n'+
	'#SBATCH -J sort_'+config.path+'\n'+
	'#SBATCH -e '+config.abspath+'/sbatch.sortreads.stderr.txt'+'\n'+
	'#SBATCH -o '+config.abspath+'/sbatch.sortreads.stdout.txt'+'\n'+
	'#SBATCH --mail-type=All'+'\n'+
	'#SBATCH --mail-user=erik.borgstrom@scilifelab.se'+'\n'+
	'echo "$(date) Running on: $(hostname)"'+'\n'+
	'cd '+os.getcwd()+'\n'+
	'module load python/2.7'+'\n'+
	sys.argv[0]+' sortreads -path '+config.path+' -p8'+'\n'
    )
    f.close()
    
    f = open( config.path +'/sbatch.meta.sh','w')
    f.write(
	'#! /bin/bash -l'+'\n'+
	'#SBATCH -A b2011011'+'\n'+
	'#SBATCH -n 8 -p node'+'\n'+
	'#SBATCH -t 10:00:00'+'\n'+
	'#SBATCH -J meta_'+config.path+'\n'+
	'#SBATCH -e '+config.abspath+'/sbatch.meta.stderr.txt'+'\n'+
	'#SBATCH -o '+config.abspath+'/sbatch.meta.stdout.txt'+'\n'+
	'#SBATCH --mail-type=All'+'\n'+
	'#SBATCH --mail-user=erik.borgstrom@scilifelab.se'+'\n'+
	'echo "$(date) Running on: $(hostname)"'+'\n'+
	'cd '+os.getcwd()+'\n'+
	'module load python/2.7'+'\n'+
	sys.argv[0]+' meta -path '+config.path+' -p8'+'\n'
    )
    f.close()
    
    if indata.send:
	config.logfile.write('Placing scripts in jobqueue.\n')
	import subprocess

	sbatch = subprocess.Popen( ['sbatch',config.path +'/sbatch.cluster.sh'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	sbatch_out, errdata = sbatch.communicate()
	if sbatch.returncode != 0:
		print 'sbatch view Error code', sbatch.returncode, errdata
		print sbatch_out
		sys.exit()
	cluster_jobid = sbatch_out.split('\n')[0].split(' ')[3]
	config.logfile.write('Queued barcode clustering with JOBID '+cluster_jobid+'.\n')

	sbatch = subprocess.Popen( ['sbatch','--dependency=afterok:'+cluster_jobid,config.path +'/sbatch.sortreads.sh'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	sbatch_out, errdata = sbatch.communicate()
	if sbatch.returncode != 0:
		print 'sbatch view Error code', sbatch.returncode, errdata
		print sbatch_out
		sys.exit()
	sort_jobid = sbatch_out.split('\n')[0].split(' ')[3]
	config.logfile.write('Queued sorting of reads with JOBID '+sort_jobid+'.\n')

	sbatch = subprocess.Popen( ['sbatch','--dependency=afterok:'+sort_jobid,config.path +'/sbatch.meta.sh'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	sbatch_out, errdata = sbatch.communicate()
	if sbatch.returncode != 0:
		print 'sbatch view Error code', sbatch.returncode, errdata
		print sbatch_out
		sys.exit()
	meta_jobid = sbatch_out.split('\n')[0].split(' ')[3]
	config.logfile.write('Queued metagenomics analysis with JOBID '+meta_jobid+'.\n')
    
    config.logfile.write('Done.\n')
    return 0

def writelogheader(logfile):
    import sys
    import time
    logfile.write('----------------\n')
    logfile.write('Running program: '+' '.join(sys.argv)+'.\n')
    logfile.write('Version: '+version+'\n')
    logfile.write('time: '+time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+'\n')
    logfile.write('Master process id='+str(MASTER)+'\n')

def init(indata):
    
    import os
    import sys
    tmp_log = ''

    if os.path.exists(indata.path+'/'+'config'):
	sys.stderr.write('This analysis has already been initiated try another command.\n')
	return
    
    config = Configuration(indata.path, indata.cmd)
    
    try:
	os.mkdir(indata.path)
	tmp_log += 'Folder '+indata.path+' created sucessfully.\n'
    except OSError as inst:
	if inst[0] != 17: print inst; return
	_continue = ''
	while not _continue or _continue[0] not in ['Y','y', 'N','n']:
	    _continue = raw_input('WARNING: the folder '+indata.path+' already excists. Continue anyway? (yes/no) ')
	tmp_log += 'WARNING: the folder '+indata.path+' already excists. Continue anyway? (yes/no) '+_continue+'\nUsing already exsisting folder ...\n'
	if _continue[0] in ['Y','y']: pass
	elif _continue[0] in ['N','n']: return
	else:
	    sys.stderr.write('Error 1\n.'); return 1

    config.openconnections()

    import time
    writelogheader(config.logfile)
    config.logfile.write(tmp_log)

    config.logfile.write('Creating config file:\n')
    config.save()

    config.logfile.write('Analysis '+config.abspath+' sucesfully initiated.\n')
    return 0

def addfqs(indata):

    config = Configuration(indata.path, indata.cmd)
    config.openconnections()
    
    writelogheader(config.logfile)

    config.logfile.write('Reading current config ...\n')
    config.load()

    config.logfile.write('Adding filenames to config file.\n')
    import os
    import sys
    r1 = os.path.abspath(indata.reads1.name)
    r2 = os.path.abspath(indata.reads2.name)
    if r1 in config.infiles['r1'] + config.infiles['r2'] or r2 in config.infiles['r1'] + config.infiles['r2']:
	sys.stderr.write('ERROR:\nat least one of the files:\n'+r1+'\n'+r2+'\nare already in the config file.\n');
	config.logfile.write('ERROR:\nat least one of the files:\n'+r1+'\n'+r2+'\nare already in the config file.\nExiting withiout changes.\n');
	return 1
    config.infiles['r1'].append(r1)
    config.infiles['r2'].append(r2)
    
    config.logfile.write('Getting readcount ...\n')
    config.readcounts.append(bufcount(r1)/4)
    
    config.save()
    
    config.logfile.write('Files '+r1+' and '+r2+' sucessfully added to infiles dist in config.\n\n')
    config.logfile.close()
    return 0

def foreachread_cluster(tmp):

    # unpack info
    pair, config = tmp
    del tmp
    
    # convert to SEAseq readpair
    pair = SEAseqpair(pair.header, pair.r1, pair.r2)

    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
    pair.identify(C_HANDLE, config)
    pair.getN15()
    
    return pair

def clusterbarcodes(indata):

    config = Configuration(indata.path, indata.cmd, skip=indata.skip ,stop=indata.stop ,random=indata.n)
    config.openconnections()
    
    writelogheader(config.logfile)

    # settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()
    config.set('chandlemissmatch',indata.handlemm)
    config.set('barcodemissmatch',indata.bcmm)
    config.set('numberofseeds', indata.seed)
    config.save()

    config.logfile.write('Part1: identifying barcode sequences in reads.\n')
    
    #deciding if to run multiprocessing or single process for debugging

    if indata.debug: #single process // serial
	results=[] # create holder for processed reads
	config.logfile.write('Running in debug mode identifying handles ...\n')
	progress = Progress(config.reads2process, logfile=config.logfile) # creates a progress "bar" thingy
	with progress:
	    for tmp in getPairs(config): #itarate through reads and do the "magicFunction"
		progress.update()
		results.append(foreachread_cluster(tmp))
	config.logfile.write('handle identification finished.\n')

    else: # multiple processes in parallel, create worker pool that iterates through the reads and does the "magicFunction" and sends results to "results"
	import multiprocessing
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=1000000) 
	results = WorkerPool.imap_unordered(foreachread_cluster,getPairs(config),chunksize=100)
    if not indata.debug: config.logfile.write('Running in multiproccessing mode using '+str(indata.cpus)+' processes  ...\n')
    else: config.logfile.write('Running the multiprocessing results handeling in serial ... \n')
    progress = Progress(config.reads2process, logfile=config.logfile)
    summary = SEAseqSummary()
    with progress:
	for pair in results:
	    progress.update()
	    summary.add(pair)
    if not indata.debug:WorkerPool.close()
    if not indata.debug:WorkerPool.join()
    config.outfile.write(str( summary.part1() )+'\n')
    config.logfile.write('Part1: finished barcode sequences identified.\n')
    
    config.logfile.write('Part2: Clustering the identified barcodes.\n')
    summary.reducebarcodes(config)
    config.logfile.write('Writing cluster info to file.\n')
    f = open(config.clusters_file,'w')
    f.write(str(summary.clusters))
    f.close()
    config.load()
    config.set('clustercount', len(summary.clusters))
    config.save()
    config.logfile.write('Part2: Clustering barcodes END\n----------\n')
    return summary

def foreachread_sort(tmp):

    # unpack info
    pair, config = tmp
    del tmp
    
    # convert to SEAseq readpair
    pair = SEAseqpair(pair.header, pair.r1, pair.r2)
    
    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
    pair.identify(C_HANDLE, config)
    pair.getN15()
    
    pair.get_cid(config)
    
    return pair

def load_clusters(config,queue):
    import multiprocessing
    import os
    config.logfile.write('\nLoading clustering data ...\n')
    config.logfile.write( 'Loader pid='+str(os.getpid())+'\n')
    summary = SEAseqSummary()
    summary.loadclusters(config.clusters_file)
    clustercount = len(summary.clusters)
    config.logfile.write('Data Loaded, reformatting ...\n')
    progress = Progress(len(summary.clusters), logfile=config.logfile, unit = 'cluster', mem = True)
    chunksize = 5000
    with progress:
	for cluster_id, infodist in summary.clusters.iteritems():
	    progress.update()
	    if int(infodist['total']) >= config.read_count_per_barcode_cluster_cutoff:
		for barcode in infodist['barcodes']:
		    config.cid_by_bc[barcode] = cluster_id
	    else: pass#print 'low read cluster'
    summary.clusters.clear()
    queue.put(clustercount)
    config.logfile.write('Done returning to master process.\n')

def sortreads(indata):

    config = Configuration(indata.path, indata.cmd, skip=indata.skip ,stop=indata.stop ,random=indata.n)
    config.openconnections()
    writelogheader(config.logfile)

    #settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()

    import multiprocessing
    man = multiprocessing.Manager()
    config.cid_by_bc = man.dict()
#    chunk_list = man.list([[[cluster+chunk*chunksize,[],[], 0, 0] for cluster in xrange(chunksize)] for chunk in xrange(config.clustercount/chunksize+1)])
    queue = multiprocessing.Queue()
    p = multiprocessing.Process(target=load_clusters,args=(config,queue))
    p.start()
    clustercount = queue.get()
    p.join()
    
    import os
    try: os.makedirs(config.path+'/sortedReads')
    except OSError:pass
    #try: os.makedirs(config.path+'/sortedReads/barcodes')
    #except OSError:pass

    if indata.debug: #single process // serial
	results=[] # create holder for processed reads
	config.logfile.write('Part1: Sorting reads to clusters ...\n')
	config.logfile.write('Running in debug mode processing reads ...\n')
	progress = Progress(config.reads2process, logfile=config.logfile, mem=True) # creates a progress "bar" thingy
	#itarate through reads and do the "magicFunction"
	with progress:
	    for tmp in getPairs(config):
		progress.update()
		results.append(foreachread_sort(tmp))
	config.logfile.write('finished, now sorting reads to clusters in memory ... \n')
    else: # multiple processes in parallel
	import multiprocessing
	#create worker pool that iterates through the reads and does the "magicFunction" and sends results to "results"
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=10000)
	results = WorkerPool.imap_unordered(foreachread_sort,getPairs(config),chunksize=1000)

    config.logfile.write('Allocating sorting memory  ...\n')
    chunksize = 5000
    #out_list = [[i, [],[]] for i in xrange(clustercount+1)]
    chunk_list = [[[cluster+chunk*chunksize,[],[]] for cluster in xrange(chunksize)] for chunk in xrange(config.clustercount/chunksize+1)]
#    chunk_list[0][0] = [0,None,None]
    config.logfile.write(' done.\n')

    if not indata.debug: config.logfile.write('Sorting reads to cluster '+str(indata.cpus)+' processes  ...\n')
    progress = Progress(config.reads2process, logfile=config.logfile, mem = True)
    with progress:
	if config.sortformat == 'fq':
	    f1 = open(config.path+'/sortedReads/sorted_by_barcode_cluster.1.fq','w')
	    f2 = open(config.path+'/sortedReads/sorted_by_barcode_cluster.2.fq','w')
	elif config.sortformat == 'fa':
	    f1 = open(config.path+'/sortedReads/sorted_by_barcode_cluster.fa','w')
	for pair in results:
	    progress.update()
	    if pair.cid:
		if config.sortformat == 'fq':
		    chunk_list[pair.cid/chunksize][pair.cid%chunksize][1].append('_'.join(pair.r1.header.split(' ')) + '_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r1.seq+'\n+\n'+pair.r1.qual+'\n')
		    chunk_list[pair.cid/chunksize][pair.cid%chunksize][2].append('_'.join(pair.r2.header.split(' ')) + '_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r2.seq+'\n+\n'+pair.r2.qual+'\n')
		elif config.sortformat == 'fa':
		    f1.write('>' + '_'.join(pair.r1.header.split(' ')) + '_r1_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r1.seq+'\n'+
			     '>' + '_'.join(pair.r2.header.split(' ')) + '_r2_' + str(pair.cid) + '_' + pair.n15.seq + '\n'+pair.r2.seq+'\n')

    if config.sortformat == 'fq':
	config.logfile.write('\nWriting to fastq files sorted by cluster id ...\n')
	progress = Progress(clustercount, logfile=config.logfile, unit = 'cluster')
	with progress:
	    for chunk in chunk_list:
		for cluster in chunk:
			if cluster[0] <= config.clustercount: progress.update()
			for i in xrange(len(cluster[1])):
			    f1.write(cluster[1][i])
			    f2.write(cluster[2][i])	

    #close connections
    if config.sortformat in ['fa','fq']:	f1.close()
    if config.sortformat == 'fq':		f2.close()

    if not indata.debug:WorkerPool.close()
    if not indata.debug:WorkerPool.join()
    config.logfile.write('Reads sorted into sortedReads/sorted_by_barcode_cluster.1.fq\n')
    config.logfile.write('Part1: Sorting reads to clusters END\n----------\n')
    config.logfile.close()
    config.outfile.close()
    return 0

def meta(indata):

    config = Configuration(indata.path, indata.cmd, skip=indata.skip ,stop=indata.stop ,random=indata.n)
    import os
    import sys
    if os.path.exists(config.outfile): os.remove(config.outfile)
    config.openconnections()
    
    writelogheader(config.logfile)

    # settings
    config.logfile.write('Get infiles from config-file.\n')
    config.load()
    config.getreads2process()
    
    import multiprocessing as mp
    man = mp.Manager()
    clusterq = man.Queue(1000)
    # Run subprocess that get read pars and adds clusters to queue last add a END
    reader = mp.Process(target=getClustersAndPairs,args=(config,clusterq))
    reader.start()
    
    # make a worker pool that works with the clusters returned from the generator
    if indata.debug: #single process // serial
	config.logfile.write('debugging: ');
	sys.stdout.write('debugging: ')
	config.logfile.write('Running in debug mode ')
	results=[] # create holder for processed reads
	progress = Progress(config.clustercount, logfile=config.logfile) # creates a progress "bar" thingy
	with progress:
	    for cluster_pairs in clusteriterator(clusterq):
		progress.update()
		results.append(foreachcluster_meta(cluster_pairs))
		#config.logfile.write('.');
		#sys.stdout.write('.')
	config.logfile.write('finished, making summary ... \n')
    else: # multiple processes in parallel
	import multiprocessing
	WorkerPool = multiprocessing.Pool(indata.cpus,maxtasksperchild=10000)
	#results = WorkerPool.imap_unordered(foreachcluster_meta,clusteriterator(clusterq),chunksize=10)
	results = WorkerPool.imap(foreachcluster_meta,clusteriterator(clusterq),chunksize=10)

    nonecount = 0
    processed = 0
    lowread = 0
    onlyjunk = 0
    typecounter = {'ITS':0,'16S':0,'None':0,'both ITS and 16S':0}
    monoclonal = {'ITS':0,'16S':0,'None':'NA','both':{'only ITS':0,'only 16S':0,'None':0,'both':0}}
    if not indata.debug: config.logfile.write('Part1: Per cluster action '+str(indata.cpus)+' processes  ...\n')
    progress = Progress(config.clustercount, logfile=config.logfile, unit='cluster')
    statstable = open(config.path+'/meta.statstable','w',1)
    statsheader = ['clusterid','number of reads in total','number of adaper reads','number of strange primers','its reads','16s reads','its','16s','its monoclonal','16s monoclonal','number of consensus types','number of consensus types with good support']
    statstable.write('\t'.join(statsheader))
    seqdump = open(config.path+'/meta.sequences.fa','w',1)
    with progress:
	for tmp in results:
	    seqdict = {}
	    progress.update()
	    if tmp == 'INITIAL':
		nonecount+=1
		if nonecount > 1: print 'Error!!! to many none clusters!!'; raise ValueError
		return_info = False
	    elif tmp[0] == 'LOW READ':
		processed += 1
		lowread += 1
		return_info = tmp[1]
	    elif tmp[0] == 'ONLY JUNK':
		processed += 1
		onlyjunk += 1
		return_info = tmp[1]
	    else:
		[output, _its, _16s, return_info, seqdict] = tmp
		config.outfile.write( output )
		processed += 1

		#print tmp[1:]		
		if _its != None and _16s != None:
		    typecounter['both ITS and 16S']+=1
		    if _its and _16s: monoclonal['both']['both'] += 1
		    if _its and not _16s: monoclonal['both']['only ITS'] += 1
		    if _16s and not _its: monoclonal['both']['only 16S'] += 1
		    if not _16s and not _its: monoclonal['both']['None'] += 1
		elif _its != None:
		    typecounter['ITS'] += 1
		    if _its: monoclonal['ITS'] += 1
		elif _16s != None:
		    typecounter['16S'] += 1
		    if _16s: monoclonal['16S'] += 1
		else:
		    typecounter['None'] += 1

		# SEQDICT printing to file ONLY for both monoclonal
		if _its and _16s:
		    for cons_type in seqdict:
			for consensus in seqdict[cons_type]:
			    seqdump.write(seqdict[cons_type][consensus])

	    if return_info:
		statstable.write( '\n'+'\t'.join([str(return_info[stat]) for stat in statsheader]) )
		assert len(return_info) == len(statsheader)
	seqdump.close()
	statstable.close()

    reader.join()
    
    config.outfile.write('##### SUMMARY #####'+'\n')
    config.outfile.write(  str(processed)+ ' clusters processed, out of these were:'+'\n')
    config.outfile.write(  str(onlyjunk) + ' clusters of only adapter sequences or faulty primers'+'\n')
    config.outfile.write(  str(lowread)  + ' clusters with to few reads to be analysed'+'\n')
    for name,count in typecounter.iteritems():
	if name == 'both ITS and 16S':
	    config.outfile.write(  str(count)+' '+ name+' '+ 'whereof:'+'\n')
	    for name,count2 in monoclonal['both'].iteritems():
		config.outfile.write(  '\t'+' '+str(count2)+' '+'were monoclonal for'+' '+name+'\n')
	    continue
	config.outfile.write(  str(count)+' '+ name+' '+ 'whereof'+' '+ str(monoclonal[name])+' '+'were monoclonal'+'\n')
	
	
    # create a nice run summary
    config.logfile.write('Done.\n')
    return 0

def foreachcluster_meta(cluster_pairs):
    if cluster_pairs[0] == 0: return 'INITIAL'
    config = cluster_pairs[2]
    cid = cluster_pairs[1][0].cid
    verb = 0

    # for each cluster:
	# remove reads containing illumina adapter sequences
	# cut the N15 and C-handle
	# idetify if read primers are 16S, ITS, MIX or unknown
	# get consensus sequences for each class of primer combos
	# identify the species corresponding to each consensus
	# create cluster output/summary
    
    return_info = {
	'clusterid':cid,
	'number of reads in total':len(cluster_pairs[1]),
	'number of adaper reads':None,
	'number of strange primers':None,
	'number of consensus types':None,
	'number of consensus types with good support':None,
	'its reads':None,
	'16s reads':None,
	'its':None,
	'16s':None,
	'its monoclonal':None,
	'16s monoclonal':None
    }
    
    adaptercount = 0
    primererror = 0
    lowreadcutoff = 1
    output = ''
    output =  '\n###--- Cluster number '+str(cluster_pairs[1][0].cid)+' -> '+str(len(cluster_pairs[1]))+' pairs. ---###\n'
    if len(cluster_pairs[1])<lowreadcutoff:
	output += 'Less than '+str(lowreadcutoff)+' read pairs.\n'
	return ['LOW READ',return_info]
    else:
	reads = {}

	if verb >1: output += 'PAIRS:\n'
	import re
	
	f = open(config.path+'/sortedReads/temporary.'+str(cid)+'.fa','w')
	tmpcounter = 0
	int2header = {}
	for pair in cluster_pairs[1]:
		
	    # convert to SEAseq readpair
	    pair = SEAseqpair(pair.header, pair.r1, pair.r2)
	    
	    tmpcounter += 1
	    #if verb >1: output += str(tmpcounter)+'\t'+pair.header +'\t'
	    if verb >1: output += str(tmpcounter)+'\t'
	    
	    #identify subparts of read
	    C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
	    pair.identify(C_HANDLE, config)
	    pair.getN15()
	    pair.identifyIllumina(config)
	    
	    #match adaptersequence
	    if (pair.r1.illuminaadapter or pair.r2.illuminaadapter):
		adaptercount +=1;
		if verb >1: output+='ADAPTER\n';
		continue
	    
	    #determine fwd primer
	    pair.p1 = None
	    p1_16S = re.match( 'GTG.CAGC.GCCGCGGTAA',    pair.r1.seq[pair.handle_end:])
	    p1_ITS = re.match( 'GG.CTTGTAC.CAC.GCCCGTC', pair.r1.seq[pair.handle_end:])
	    if	 p1_16S:
		if verb >1: output += '16S\t';
		pair.p1 = '16S'
	    elif p1_ITS:
		if verb >1: output += 'ITS\t';
		pair.p1 = 'ITS'
	    else:
		if verb >1: output += '???\t';
		pair.p1 = '???'
	    
	    #determine reverse primer
	    pair.p2 = None
	    p2_16S = re.match( 'ACA..TCAC..CACGAGCTGACGAC',pair.r2.seq)
	    p2_ITS = re.match( 'CTC..A.TGCC..GGCATCCACC',  pair.r2.seq)
	    if 	 p2_16S:
		if verb >1: output += '16S\t';
		pair.p2 = '16S'
	    elif p2_ITS:
		if verb >1: output += 'ITS\t';
		pair.p2 = 'ITS'
	    else:
		if verb >1: output += '???\t';
		pair.p2 = '???'
	    
	    # check that primers match
	    if pair.p1 == '???' or pair.p1 != pair.p2:
		primererror+=1;
		if verb >1: output+='PRIMER ERROR\n';
		continue 

	    #look for unexpected primer sequences
	    if pair.p1 == '16S':
	        fwd16S_in_r2  = re.search( 'GTG.CAGC.GCCGCGGTAA',      pair.r2.seq) #16 fwd in read 2
		rev16S_in_r1  = re.search( 'ACA..TCAC..CACGAGCTGACGAC',pair.r1.seq[pair.handle_end:]) #16 rev in read1
		fwdITS_in_any = re.search( 'GG.CTTGTAC.CAC.GCCCGTC',   pair.r1.seq[pair.handle_end:]+'NNNNN'+pair.r2.seq) # its fwd in any read
		revITS_in_any = re.search( 'CTC..A.TGCC..GGCATCCACC',  pair.r1.seq[pair.handle_end:]+'NNNNN'+pair.r2.seq) # its rev in any read
		if fwd16S_in_r2 or rev16S_in_r1 or fwdITS_in_any or revITS_in_any:
		    primererror+=1;
		    if verb >1: output+='PRIMER ERROR\n';
		    #print 'Happened'
		    continue
	    elif pair.p1 == 'ITS':
	        fwd16S_in_any = re.search( 'GTG.CAGC.GCCGCGGTAA',      pair.r1.seq[pair.handle_end:]+'NNNNN'+pair.r2.seq) 
		rev16S_in_any = re.search( 'ACA..TCAC..CACGAGCTGACGAC',pair.r1.seq[pair.handle_end:]+'NNNNN'+pair.r2.seq) 
		fwdITS_in_r2  = re.search( 'GG.CTTGTAC.CAC.GCCCGTC',   pair.r2.seq) 
		revITS_in_r1  = re.search( 'CTC..A.TGCC..GGCATCCACC',  pair.r1.seq[pair.handle_end:]) 
		if fwd16S_in_any or rev16S_in_any or fwdITS_in_r2 or revITS_in_r1:
		    primererror+=1;
		    if verb >1: output+='PRIMER ERROR\n';
		    #print 'Happened'
		    continue

	    #16S_fwd primer	GTGBCAGCMGCCGCGGTAA	19	6
	    #ITS_fwd primer	GGBCTTGTACWCACYGCCCGTC	22	6
	    #16S_rev primer	ACAHYTCACRRCACGAGCTGACGAC	25	24
	    #ITS_rev primer	CTCYDANTGCCSRGGCATCCACC	23	384

	    #output +='\n'
	    if verb >1: output += pair.r1.seq +' '+ pair.r2.seq+'\n'
	    
	    # prepare for clustering
	    if   pair.p1 == '16S': tem_seq = pair.r1.seq[pair.handle_end:][20:]+'NNNNNNNNNN'+pair.r2.revcomp().seq[:-26]
	    elif pair.p1 == 'ITS': tem_seq = pair.r1.seq[pair.handle_end:][23:]+'NNNNNNNNNN'+pair.r2.revcomp().seq[:-24]
	    int2header[tmpcounter] = pair.header
	    reads[pair.header] = pair
	    f.write(
		#'>'+':'.join(pair.header.split(':')[5:-3])+'.p1='+str(pair.p1)+'.p2='+str(pair.p2)+'\n'+
		'>'+str(tmpcounter)+'\n'+
		tem_seq +'\n'
	    )
	f.close()
	
	return_info['number of adaper reads'] = adaptercount
	return_info['number of strange primers'] = primererror

	#check that there is data to work with
	if adaptercount+primererror == len(cluster_pairs[1]):
	    output += 'All adapter and/or primer error.\n'
	    import os
	    os.remove( f.name )
	    return ['ONLY JUNK',return_info]

	#cluster reads
	import subprocess
	from cStringIO import StringIO
	import time
	import multiprocessing
	tempo = time.time()
	#config.logfile.write('starting '+' '.join(['cd-hit',''])+'\n')
	cdhit = subprocess.Popen( ['cd-hit-454','-i',config.path+'/sortedReads/temporary.'+str(cid)+'.fa','-o',config.path+'/sortedReads/cluster.'+str(cid)+'.fa','-g','1','-c','0.97'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	cdhit_out, errdata = cdhit.communicate()
	if cdhit.returncode != 0:
		print 'cd-hit cluster='+str(cid)+' view Error code', cdhit.returncode, errdata
		sys.exit()
	#cd-hit_out = StringIO(cd-hit_out)
	#print cdhit_out
	seconds = round(time.time()-tempo,2)
	#config.logfile.write('cd-hit cluster '+str(cluster_pairs[1][0].cid)+' done after '+str(seconds/60/60)+'h '+str(seconds/60%60)+'min '+str(seconds%60)+'s, parsing result ... ')

	#build consensus
	ccc = subprocess.Popen( ['cdhit-cluster-consensus',config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr',config.path+'/sortedReads/temporary.'+str(cid)+'.fa',config.path+'/sortedReads/cluster.'+str(cid)+'.consensus',config.path+'/sortedReads/cluster.'+str(cid)+'.aligned'], stdout=subprocess.PIPE, stderr=subprocess.PIPE )
	ccc_out, errdata = ccc.communicate()
	if ccc.returncode != 0:
		print 'cluster='+str(cid)+'ccc view Error code', ccc.returncode, errdata
		print ccc_out
		sys.exit()

	# output info
	if verb >5: 
	    for info in [
		[config.path+'/sortedReads/cluster.'+str(cid)+'.consensus.fasta','\nCD-HIT consensus sequences:\n'],
		[config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr','\nClustering details:\n'],
		[config.path+'/sortedReads/cluster.'+str(cid)+'.aligned','\nAlignment details:\n']
		]:
		filename , message =info
		f = open(filename)
		tmp = f.read()
		output += message
		output += tmp
		f.close()
	
	# get alignments from file
	f = open(config.path+'/sortedReads/cluster.'+str(cid)+'.aligned')
	data = f.read()
	f.close()
	consensuses = {}
	for cluster_aln in data.split('===========================================\n'):
	    for part in cluster_aln.split('\n\n'):
		for line in part.split('\n'):
		    line=line.rstrip()
		    if not line: continue
		    if line[0] == 'A':
			#print '### HERE: '+line
			consensusid=line.split(' ')[-1].split(':')[0];
			consensuses[consensusid] = {}
			continue
		    read_id = line.split(':  +  ')[0].rstrip()
		    try: seq = line.split(':  +  ')[1]
		    except: print line+'<- Problematic';raise ValueError
		    if read_id == 'Consensus': seq = seq.split(' ')[0]
		    try:		consensuses[consensusid][read_id][0] += seq
		    except KeyError:	consensuses[consensusid][read_id] = [seq,'NotSet']

#	get identity from file
	f = open(config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr')
	data = f.read()
	f.close()
	for consensus_identities in data.split('>Cluster '):
		consensusid = consensus_identities.split('\n')[0]
		if consensusid not in consensuses: consensuses[consensusid] = {}
		for line in consensus_identities.split('\n')[1:]:
		    line=line.rstrip()
		    if not line: continue
		    read_id     = line.split('>')[1].split('.')[0]
		    identity = line.split('/')[-1]
		    try:		consensuses[consensusid][read_id][1] = identity
		    except KeyError:	consensuses[consensusid][read_id] = ['SEQUENCE NOT LOADED',identity]

	##load singelton consensus sequences
	f = open(config.path+'/sortedReads/cluster.'+str(cid)+'.consensus.fasta')
	data = f.read()
	f.close()
	for consensus_identities in data.split('>')[1:]:
		if consensus_identities[0] == 'c': continue
		consensusid = consensus_identities.split(' ')[0].split('_')[-1]
		read_id     = consensus_identities.split(' ')[1].split('\n')[0]
		if consensusid not in consensuses: consensuses[consensusid] = {}
		if 'Consensus' not in consensuses[consensusid]: consensuses[consensusid]['Consensus'] = ['', 'NotSet']
		if consensuses[consensusid][read_id][0] == 'SEQUENCE NOT LOADED': consensuses[consensusid][read_id][0] = ''
		for line in consensus_identities.split('\n')[1:]:
		    line=line.rstrip()
		    consensuses[consensusid][read_id][0] += line
		    consensuses[consensusid]['Consensus'][0] += line

	#make output for alnignments
	if verb >2: output += '\n'
	types = {'ITS':{'total':0},'16S':{'total':0}}
	for consensusid in consensuses:
	    if  not consensusid: continue
	    primerpairs = []
	    if 'Consensus' in consensuses[consensusid]:
		if verb >2: output += 'Consensus number '+consensusid+' from '+str(len(consensuses[consensusid])-1)+' read pairs'
		if verb >2: output += ':\t\t\t'+consensuses[consensusid]['Consensus'][0]+'\n'
	    for read_id, tmp in consensuses[consensusid].iteritems():
		[seq, identity] = tmp
		if identity[-1] == '*': identity = 'SEED'
	        if read_id == 'Consensus': continue#output +=read_id+'\t'+seq+'\n'
	        else:
		    #if verb >2: output += str(read_id)+'\t'+int2header[int(read_id)].split('_')[0]+'   \t'+identity+'\t'+reads[int2header[int(read_id)]].p1+'\t'+seq
		    if verb >2: output += 'Read pair id = '+str(read_id)+'    \t'+identity+'\t'+reads[int2header[int(read_id)]].p1+'\t'+seq
		    if verb >2: output += '\n'
		    primerpairs.append(reads[int2header[int(read_id)]].p1)
	    if   primerpairs and primerpairs.count(primerpairs[0]) == len(primerpairs):
		if verb >2: output += 'consensus is '+primerpairs[0]
	    elif primerpairs and primerpairs.count(primerpairs[0]) != len(primerpairs):
		if verb >2: output += 'WARNING: mixed consensus clustering!'
	    try:
		types[primerpairs[0]][consensusid]={'sequence':consensuses[consensusid]['Consensus'][0], 'support':len(consensuses[consensusid])-1}
		types[primerpairs[0]]['total']+=len(consensuses[consensusid])-1
	    except KeyError: raise ERIKERROR
#		types[primerpairs[0]] = {consensusid:{'sequence':consensuses[consensusid]['Consensus'][0], 'support':len(consensuses[consensusid])-1}}
#		types[primerpairs[0]]['total']=len(consensuses[consensusid])-1
	    if verb >2: output += '\n\n'

	perccutoff = 5.0
	countcutoff = 5
	#remove less than 5% (should I remove singletons aswell?)
	tmp = {}
	typecounter={}
	for cons_type, consensuses in types.iteritems():
	    tmp[cons_type] = {}
	    tmp[cons_type]['mono'] = False
	    types[cons_type]['mono'] = False
	    for consensus, data in consensuses.iteritems():
		if consensus == 'total' or consensus == 'mono': tmp[cons_type][consensus] = data;continue
		#print cons_type,consensus,data, consensuses['total']
		percentage = round(100*float(data['support'])/consensuses['total'],2)
		if percentage >= perccutoff and data['support'] >= countcutoff:
		    tmp[cons_type][consensus] = data
		    typecounter[cons_type] = True
	types = tmp	

	seqdict = {}
	# make cluster summary and seqdict for dumping to fasta
	output += 'There are '+str(adaptercount)+' illumina adapter reads.\n'
	output += 'There are '+str(primererror)+' primer missmatch reads.\n'
	#output += 'There are '+str(len(types))+' type(s) of amplicon(s) in cluster:\n'
	output += str(len(typecounter))+' amplicon(s) have enough data (>=1 cons with >= '+str(perccutoff)+'% support and >= '+str(countcutoff)+' reads):\n'
	for cons_type, consensuses in types.iteritems():
	    types[cons_type]['mono'] = False
	    output += '\t'+cons_type+' '+str(consensuses['total'])+' reads in total.\n'
	    seqdict[cons_type] = {}
	    for consensus, data in consensuses.iteritems():
		if consensus == 'total' or consensus == 'mono':continue
		percentage = round(100*float(data['support'])/consensuses['total'],2)
		output += '\t\tConsensus '+consensus+' supported by '+str(percentage)+'% of readpop ('+str(data['support'])+' reads)\t'+data['sequence'].replace('-','')+'\n'
		#if percentage >= 95.0: types[cons_type]['mono'] = True
		try:
		    seqdict[cons_type][consensus] = '\n>cluster='+str(cid)+'.amplicon='+str(cons_type)+'.consensus='+str(consensus)+'_r1.'+str(percentage)+'%_of_'+str(consensuses['total'])+'reads\n'+data['sequence'].replace('-','').split('NNNNNNNNNN')[0]
		    seqdict[cons_type][consensus] += '\n>cluster='+str(cid)+'.amplicon='+str(cons_type)+'.consensus='+str(consensus)+'_r2.'+str(percentage)+'%_of_'+str(consensuses['total'])+'reads\n'+data['sequence'].replace('-','').split('NNNNNNNNNN')[1]
		except IndexError:
		    output += 'WARNING: consensus sequence not properly splittable into r1 and r2, not dumping this cluster ('+str(cid)+').\n'
		#    try:
		#	seqdict[cons_type][consensus] = '\n>cluster='+str(cid)+'.amplicon='+str(cons_type)+'.consensus='+str(consensus)+'_r1.'+str(percentage)+'%_of_'+str(consensuses['total'])+'reads\n'+data['sequence'].replace('-','').split('NNNNNNNNN')[0]
		#	seqdict[cons_type][consensus] += '\n>cluster='+str(cid)+'.amplicon='+str(cons_type)+'.consensus='+str(consensus)+'_r2.'+str(percentage)+'%_of_'+str(consensuses['total'])+'reads\n'+data['sequence'].replace('-','').split('NNNNNNNNN')[1]
		#    except IndexError:
		#	print '\n\n###Cluster ==',cid;
		#	print 'Erro when splitting sequence:'
		#	print 'cluster='+str(cid)+'.amplicon='+str(cons_type)+'.consensus='+str(consensus)+'.'+str(percentage)+'%_of_'+str(consensuses['total'])+'reads'
		#	print data['sequence']
		#	raise ValueError
		if len(consensuses)-2 == 1: types[cons_type]['mono'] = True
	output += '\n'

	return_info['number of consensus types'] = len(types)
	return_info['number of consensus types with good support'] = len(typecounter)
	return_info['its reads'] = types['ITS']['total']
	return_info['16s reads'] = types['16S']['total']
	return_info['its'] = bool('ITS' in typecounter)
	return_info['16s'] = bool('16S' in typecounter)
	return_info['its monoclonal'] = types['ITS']['mono']
	return_info['16s monoclonal'] = types['16S']['mono']

	import os
	os.remove(config.path+'/sortedReads/temporary.'+str(cid)+'.fa')
	os.remove(config.path+'/sortedReads/cluster.'+str(cid)+'.fa')
	os.remove(config.path+'/sortedReads/cluster.'+str(cid)+'.fa.clstr')
	os.remove(config.path+'/sortedReads/cluster.'+str(cid)+'.consensus.fasta')
	os.remove(config.path+'/sortedReads/cluster.'+str(cid)+'.aligned')

	_its = None
	_16s = None
	if 'ITS' in typecounter:
	    _its = types['ITS']['mono']
	    if _its: output+='Cluster is monoclonal for ITS.\n'
	if '16S' in typecounter:
	    _16s = types['16S']['mono']
	    if _16s:output+='Cluster is monoclonal for 16S.\n'

	return [output, _its,_16s,return_info,seqdict]#str(cluster_pairs[1][0].cid)+' has '+str(len(cluster_pairs[1]))+' read pairs'

def clusteriterator(clusterq):  # a generator that yields clusters from queue until it finds a cluster = END
    while True:
	pairs = clusterq.get()
	if pairs == 'END': break
	yield pairs
    
def getClustersAndPairs(config,clusterq):
    
    import os
    import time
    config.logfile.write('Reader initiated pid='+str(os.getpid())+'.\n')

    currentcluster = 0
    pairs = []
    config.infiles['r1'] = [config.path+'/sortedReads/sorted_by_barcode_cluster.1.fq']
    config.infiles['r2'] = [config.path+'/sortedReads/sorted_by_barcode_cluster.2.fq']
    for tmp in getPairs(config):

	pair, config = tmp
	del tmp
	
	# convert to SEAseq readpair
	pair = SEAseqpair(pair.header, pair.r1, pair.r2)
	
	#C_HANDLE = sequence('c handle',"CTAAGTCCATCCGCACTCCT","CTAAGTCCATCCGCACTCCT")
	#pair.identify(C_HANDLE, config)
	#pair.getN15()
	
	pair.cid = int(pair.header.split(':')[-1].split('_')[1])
	
	if pair.cid == currentcluster:
	    pairs.append(pair)
	elif pair.cid == currentcluster+1:
	    while clusterq.qsize() > 160 or clusterq.full(): time.sleep(1); #config.logfile.write('waiting for queue ...\n')
	    clusterq.put([currentcluster,pairs,config])
	    currentcluster = pair.cid
	    pairs = [pair]
	else:
	    while clusterq.qsize() > 160 or clusterq.full(): time.sleep(1); #config.logfile.write('waiting for queue ...\n')
	    clusterq.put([currentcluster,pairs,config])
	    currentcluster = pair.cid
	    pairs = [pair]

    clusterq.put('END')
    config.logfile.write('Reader exiting.\n')

def initiate_file(filename, logfile, mode='w'):
    import os
    
    if type(logfile) == file and mode != 'r': logfile.write('Initiating '+filename+' ...\n')
    
    if mode == 'w' and os.path.exists(filename):
	tmp = filename
	filename = raw_input('WARNING: the file '+filename+' already excists. Enter an alternative filename (leave empty to overwrite):')
	if type(logfile) == file and mode != 'r': logfile.write('WARNING: the file '+filename+' already excists. Enter an alternative filename (leave empty to overwrite):')
	if not filename:
	    filename = tmp
	    if type(logfile) == file and mode != 'r': logfile.write('overwriting\n')
	#else:
	#    if type(indata.logfile) == file and mode != 'r': config.logfile.write('Creating file '+filename+'.\n')
    
    if mode =='ow': mode ='w'
    import re
    #if re.search('log',filename):	out = open(filename, mode,0)
    #else:				out = open(filename, mode,1)
    out = open(filename, mode,1)
    
    if type(logfile) == file and mode != 'r': logfile.write('File '+filename+' sucessfully initiated.\n')
    
    return out

def getindata(cmd):
    import argparse
    argparser = argparse.ArgumentParser(description='Analysis of SEAseq data.', formatter_class=argparse.RawTextHelpFormatter)
    argparser.add_argument('cmd')
    argparser.add_argument('--debug',	dest='debug', 	action='store_true', 			required=False,	default=False,	help='Debug (run as regular single process python script).')
    argparser.add_argument('-path',	dest='path',	metavar='<path>',	type=str,	required=True,	default=False,	help='Set the analysis path.')
    argparser.add_argument('--src',	dest='skipreadcounting', 	action='store_true', 			required=False,	default=False,	help='Skip read counting (for speeding up debug runs, sets readcount to 30M/infile).')
    if cmd == 'init': pass
    elif cmd == 'sbatch':
	argparser.add_argument('--send',	dest='send', 	action='store_true', 			required=False,	default=False,	help='Send sbatch scripts to job-queue.')
    elif cmd == 'addfqs':
	argparser.add_argument(	'-r1',			dest='reads1',	metavar='FILE',				type=file,	required=True, 			help='Indata "fastq"-file read1.')
	argparser.add_argument(	'-r2',			dest='reads2',	metavar='FILE',				type=file,	required=True,	default=None,	help='Indata "fastq"-file read2.')
    elif cmd == 'clusterbarcodes':
	argparser.add_argument(	'-bm',			dest='bcmm',	metavar='N',				type=int,	required=False,	default=0,	help='Number off missmatches allowed in barcode sequence during clustering (default 0)')
	argparser.add_argument(	'-hm',			dest='handlemm',metavar='N',				type=int,	required=False,	default=0,	help='Number off missmatches allowed in handle sequence (default 0)')
        argparser.add_argument(	'-seed',		dest='seed',	metavar='N',				type=int,	required=False,	default=100,	help='Number of top barcodes (with most reads) to use as seeds in clustering(default 100)')
    elif cmd == 'sortreads':
        argparser.add_argument(	'-sortfmt',		dest='sortfmt',	metavar='[fa/fq]',			type=str,	required=False,	default='fq',	help='Format to output reads to fa=fasta or fq=fastq(default fastq)')
    if cmd == 'sortreads' or cmd == 'clusterbarcodes' or cmd == 'meta':
	argparser.add_argument(	'-p',			dest='cpus',	metavar='N',				type=int,	required=False,	default=1,	help='The number of processes to run in parallel (default 1).')
	argparser.add_argument(	'-skip',		dest='skip',	metavar='N',				type=int,	required=False,	default=0,	help='Skip the first N read pairs in files (default 0).')
	argparser.add_argument(	'-stop',		dest='stop',	metavar='N',				type=int,	required=False,	default=0,	help='Stop after N read pairs, set to 0 to disable (default 0).')
	argparser.add_argument(	'-random',		dest='n',	metavar='N',				type=int,	required=False,	default=0,	help='Use a random subset of N read pairs, this option is slower (default 0 = off). Can not be used in combination with "-skip" or "-stop"')
        argparser.add_argument(	'-mrc',			dest='mrc',	metavar='N',				type=int,	required=False,	default=1,	help='Minimum number of reads per cluster to consider it (default 1) DISABLED')
    if cmd == 'metagraph':
	argparser.add_argument(	'-step',		dest='step',	metavar='N',				type=int,	required=False,	default=0,	help='Update intervall. Updates y value every "step" x value (default "max(x)/200" ie 200 datapoints along x-axis)')
	argparser.add_argument(	'-x',			dest='xscale',	metavar='N-N',				type=str,	required=False,	default='0-1000',help='x axis scale (default 0-1000)')
	argparser.add_argument(	'-y',			dest='yscale',	metavar='N-N',				type=str,	required=False,	default='0-1000',help='y axis scale (default 0-1000)')
	argparser.add_argument(	'-sample',		dest='sample',	metavar='str',				type=str,	required=False,	default='0-1000',help='Sample name to put in header and filename')
	argparser.add_argument('--highres',	dest='highres', 	action='store_true', 			required=False,	default=False,	help='Use all x points to get high res data.')
	argparser.add_argument(	'-g',			dest='graphs',	metavar='N',				type=str,	required=False,	default='a',	help='"all" or str with graph signs (default a).\n\n'+
				'Available graph signs:\n\n'+
				'a --> x  = "Minimum number of read pairs per cluster"\n'+
				'      y1-5  = "Number of barcode cluster" for Total, 16S, ITS, BOTH and None clusters\n'+
				'      y6-10 = "Percentage monoclonality"  for 16S, ITS and BOTH clusters\n'+
				'      Makes one graph for "good pairs" and one for "total read pairs" per cluster.\n\n'+
				'b --> x  = "Minimum number of read pairs per cluster"\n'+
				'      y1 = "Number of barcode cluster with consensus sequences of TYPE, total readpairs"\n'+
				'      y2 = "Percentage barcode clusters monoclonal for TYPE, total readpairs"\n'+
				'      y3 = "Number of barcode cluster with consensus sequences of TYPE, good readpairs"\n'+
				'      y4 = "Percentage barcode clusters monoclonal for TYPE, good readpairs"\n'+
				'      Makes one graph each for TYPE = ITS, 16S and BOTH.\n\n'+
				'c --> Nothing yet.\n\n'
			       )
	


    import sys
    indata = argparser.parse_args(sys.argv[1:])
    if indata.path[-1] == '/': indata.path=indata.path[:-1]

    return indata

class Configuration():
    
    def __init__ (self, path, cmd, stop=None, skip=None ,random=None ):

	# permanent
	self.path 		= path
	self.config		= self.path+'/'+'config'
	self.init_logfile	= self.path + '/' + 'init.log.txt'
	self.init_outfile	= self.path + '/' + 'init.out.txt'
	self.addfqs_logfile	= self.path + '/' + 'addfqs.log.txt'
	self.addfqs_outfile	= self.path + '/' + 'addfqs.out.txt'
	self.cluster_logfile	= self.path + '/' + 'cluster.log.txt'
	self.cluster_outfile	= self.path + '/' + 'cluster.out.txt'
	self.sortreads_logfile	= self.path + '/' + 'sort.log.txt'
	self.sortreads_outfile	= self.path + '/' + 'sort.out.txt'
	self.meta_logfile	= self.path + '/' + 'meta.log.txt'
	self.meta_outfile	= self.path + '/' + 'meta.out.txt'
	self.sbatch_logfile	= self.path + '/' + 'sbatch.log.txt'
	self.sbatch_outfile	= self.path + '/' + 'sbatch.out.txt'
	self.metagraph_logfile	= self.path + '/' + 'graph.log.txt'
	self.metagraph_outfile	= self.path + '/' + 'graph.out.txt'
	self.classifymeta_logfile= self.path + '/' + 'classify.log.txt'
	self.classifymeta_outfile= self.path + '/' + 'classify.out.txt'
	self.clusters_file	= self.path+'/barcode_clusters_dictionary'
	self.abspath		= None # not loaded or set

	# longlasting
	self.infiles		= {'r1':[],'r2':[]}
	self.readcounts		= []
	self.chandlemissmatch	= None
	self.barcodemissmatch	= None
	self.clustercount	= None
	self.numberofseeds	= None

	# for each run
	self.cmd		= cmd
	self.stop		= stop
	self.skip		= skip
	self.random		= random
	self.sortformat		= 'fq'
	self.read_count_per_barcode_cluster_cutoff = 1
	
	
	if cmd == 'init':
	    self.logfile = self.init_logfile
	    self.outfile = self.init_outfile
	elif cmd == 'addfqs':
	    self.logfile = self.addfqs_logfile
	    self.outfile = self.addfqs_outfile
	elif cmd == 'clusterbarcodes':
	    self.logfile = self.cluster_logfile
	    self.outfile = self.cluster_outfile
	elif cmd == 'sortreads':
	    self.logfile = self.sortreads_logfile
	    self.outfile = self.sortreads_outfile
	elif cmd == 'meta':
	    self.logfile = self.meta_logfile
	    self.outfile = self.meta_outfile
	elif cmd == 'sbatch':
	    self.logfile = self.sbatch_logfile
	    self.outfile = self.sbatch_outfile
	elif cmd == 'metagraph':
	    self.logfile = self.metagraph_logfile
	    self.outfile = self.metagraph_outfile
	elif cmd == 'classifymeta':
	    self.logfile = self.classifymeta_logfile
	    self.outfile = self.classifymeta_outfile

    def getreads2process(self, ):
	
	self.logfile.write('Getting readcounts ...\n')
	total = 0
	for i in range(len(self.readcounts)):
	    rc = self.readcounts[i]
	    self.logfile.write(self.infiles['r1'][i]+' -> '+str(rc) +' reads.\n')
	    total += rc
	self.logfile.write(str(total)+' read pairs in fastq files.\n');
	
	# calculate the number of reads to process
	self.reads2process = total
	if self.skip: 	self.reads2process -= self.skip
	if self.stop: 	self.reads2process = self.stop
	if self.random:	self.reads2process = self.random

    def set(self,varname, value ):
	if varname == 'chandlemissmatch':	self.chandlemissmatch	= value
	elif varname == 'barcodemissmatch':	self.barcodemissmatch	= value
	elif varname == 'clustercount':		self.clustercount	= value
	elif varname == 'numberofseeds':	self.numberofseeds	= value
	elif varname == 'sortformat':		self.sortformat		= value
	elif varname == 'read_count_per_barcode_cluster_cutoff':self.read_count_per_barcode_cluster_cutoff = value
	else: raise ValueError

    def load(self, ):
	self.config = initiate_file(self.config, self.logfile , mode='r')
	for line in self.config:
	    if line.rstrip() == '# Absolute path:':
		self.abspath = self.config.next().rstrip()
	    if line.rstrip() == '# Infiles dictionary:':
		self.infiles = eval(self.config.next())
	    if line.rstrip() == '# Read counts list:':
		self.readcounts = eval(self.config.next())
	    if line.rstrip() == '# Number of cluster seeds:':
		self.numberofseeds = eval(self.config.next().rstrip())
	    if line.rstrip() == '# Number of barcode clusters identified:':
		self.clustercount = eval(self.config.next().rstrip())
	self.config.close()
	self.config = self.config.name

    def openconnections(self, ):
	if self.cmd == 'init':
	    self.outfile = initiate_file(self.outfile, self.logfile)
	    self.logfile = initiate_file(self.logfile, self.logfile)
	else:
	    self.outfile = initiate_file(self.outfile, self.logfile, mode='a')
	    self.logfile = initiate_file(self.logfile, self.logfile, mode='a')

    def save(self, ):

	if not os.path.exists(self.config):
	    self.config = initiate_file(self.config, self.logfile)
	    self.abspath = os.path.abspath(self.path)
	else:
	    self.config = initiate_file(self.config, self.logfile, mode='ow')

	self.logfile.write('Writing settings to config file ...\n')
	self.config.write(
	    '# Absolute path:\n'+str(self.abspath)+'\n'+
	    '# Infiles dictionary:\n'+str(self.infiles)+'\n'+
	    '# Read counts list:\n'+str(self.readcounts)+'\n'+
	    '# Number of cluster seeds:\n'+str(self.numberofseeds)+'\n'+
	    '# Number of barcode clusters identified:\n'+str(self.clustercount)+'\n'
	    )
	self.config.close()
	self.config = self.config.name

def classifymeta(indata):

    config = Configuration(indata.path, indata.cmd)
    import os
    if os.path.exists(config.outfile): os.remove(config.outfile)
    config.openconnections()
    writelogheader(config.logfile)

    # settings
    config.load()

    #database="../reference/4amplicons/4amplicons.fasta"
    from Bio.Blast.Applications import NcbiblastnCommandline
    from Bio.Blast import NCBIXML
    from cStringIO import StringIO
    import time

    #setting up blast
    database='/bubo/proj/b2011011/SEAseq/reference/NCBI_HUMAN_MICROBIOM_ALL/all.fa'
    #database='/bubo/proj/b2011011/SEAseq/references/NCBI_BACTERIA_ALL/all.fa'
    infile = config.path+'/meta.sequences.fa'
    cline = NcbiblastnCommandline(query=infile, db=database ,evalue=0.001, outfmt=5, num_threads=8,perc_identity=97)#, out=infile+'.blastout')
    #cline = NcbiblastnCommandline(query=infile, db=database ,evalue=0.001, outfmt=5, dust='no',perc_identity=80, task='blastn', out=infile+'.'+config.blastid+'.blastout')

    print '### blasting'
    #Blasting all reads
    blast_handle = cline.__call__()
    print '### done'

    blast_handle = StringIO(blast_handle[0])
    blast_handle.seek(0)

    #f = open(infile+'.'+config.blastid+'.blastout')
    #records = NCBIXML.parse(f)
    records = NCBIXML.parse(blast_handle)

    config.printblast = True
    if config.printblast:o=''
    
    data = {}
    for blast_record in records:

	#>cluster=1.amplicon=ITS.consensus=83_r1.27.08%_of_2626reads
	cluster	= int(	blast_record.query.split('.')[0].split('=')[1]	)
	amplicon	= blast_record.query.split('.')[1].split('=')[1]
	consensus	= int(	blast_record.query.split('.')[2].split('=')[1].split('_')[0]	)
	readnumber	= int(	blast_record.query.split('.')[2].split('=')[1].split('_')[1].split('r')[1]	)
	
	if cluster not in data: data[cluster] = {'ITS':{},'16S':{}}

	try:			data[cluster][amplicon][consensus][readnumber] = blast_record
	except KeyError:	data[cluster][amplicon][consensus] = { readnumber : blast_record }

    noblasthit_its = 0
    noblasthit_16s = 0
    blasthitsagree = 0
    hitsdonotagree = 0
    for cluster_id, amplicons in data.iteritems():
	
	print '### Cluster '+str(cluster_id)+' ###'
	identity_cutoff = 97#%
	alignment_length_cutoff = 95#%
	
	#for amplicon, consensuses in amplicons.iteritems():
	in_both_reads = {'ITS':[],'16S':[]}
	for amplicon in ['ITS','16S']:
	    broken = False
	    consensuses = amplicons[amplicon]
	    print '\t'+amplicon
	    if not consensuses:
		print '\t\tNo support'
		if amplicon == 'ITS': break
	    if len(consensuses) != 1: print '\t\tNot monoclonal';break
	
	    for consensus in consensuses:

		r1 = consensuses[consensus][1]
		r2 = consensuses[consensus][2]
		
		amplicon	= 	r1.query.split('.')[1].split('=')[1]
		support	= float(r1.query.split('r1.')[-1].split('_')[0].replace('%','')	)
		readpop = int(r1.query.split('r1.')[-1].split('_')[-1].replace('reads','')	)

		print '\t\tConsensus '+str(consensus)+': '+amplicon+' supported by '+str(support) +'% ('+str(readpop)+')'

		in_r1 = {}
		for alignment in r1.alignments:
		    for hsp in alignment.hsps:
			perc_identity = float(hsp.identities) 	/	float(hsp.align_length)	*100
			perc_coverage = float(hsp.align_length)	/	float(r1.query_letters)	*100
			if perc_identity >= identity_cutoff and perc_coverage >= alignment_length_cutoff: in_r1[alignment.title] = alignment

		in_r2 = {}
		for alignment in r2.alignments:
		    for hsp in alignment.hsps:
			perc_identity = float(hsp.identities)	/	float(hsp.align_length)	*100
			perc_coverage = float(hsp.align_length)	/	float(r2.query_letters)	*100
			if perc_identity >= identity_cutoff and perc_coverage >= alignment_length_cutoff: in_r2[alignment.title] = alignment

		
		for alignment in in_r1:
		    if alignment in in_r2:
			in_both_reads[amplicon].append(alignment)

		for alignment in in_both_reads[amplicon]:
		    print '\t\t\t'+alignment
		if not in_both_reads[amplicon]:
		    print '\t\t\tNo alignment supported by both reads with >='+str(identity_cutoff)+'% identity and '+str(alignment_length_cutoff)+'% alignment length coverage'
		    if amplicon == '16S':
			noblasthit_16s +=1
			broken = True;
		    elif amplicon == 'ITS':
			noblasthit_its += 1
			broken = True;
			break;
	    if broken: break

	
	hitagree = False
	print '\nSupported by both:'
	for alignment in in_both_reads['ITS']:
	    if alignment in in_both_reads['16S']:
		print '\t'+alignment+' is present in both'
		hitagree = True
	
	if hitagree:	blasthitsagree += 1
	elif not broken:
	    hitsdonotagree += 1
	    print '\tno hit is present in both'
	print ''
	
    assert len(data) - noblasthit_its - noblasthit_16s - blasthitsagree - hitsdonotagree == 0, '\n\nError '+str(len(data))+' - '+str(noblasthit_its)+' - '+str(noblasthit_16s)+' - '+str(blasthitsagree)+' - '+str(hitsdonotagree)+' != 0\n\n'
    print 'out of '+str(len(data))+' analyzed clusters (with both amplicons present and monoclonal for both) were:'
    print str(noblasthit_its) +' ('+str(round(100*float(noblasthit_its)/float(len(data)),2))+'%) removed because the ITS sequence gave no hits supported by both reads with >='+str(identity_cutoff)+'% identity and '+str(alignment_length_cutoff)+'% alignment length coverage.'
    print str(noblasthit_16s) +' ('+str(round(100*float(noblasthit_16s)/float(len(data)),2))+'%) removed because the 16S sequence gave no hits supported by both reads with >='+str(identity_cutoff)+'% identity and '+str(alignment_length_cutoff)+'% alignment length coverage.'
    print str(hitsdonotagree) +' ('+str(round(100*float(hitsdonotagree)/float(len(data)),2))+'%) removed because none of the ITS and 16S hits did agree.'
    print 'For '+str(blasthitsagree) +' ('+str(round(100*float(blasthitsagree)/float(len(data)),2))+'%) clusters did the ITS and 16S hits agree at least once.'
	#if blast_record.alignments:
	#	if config.printblast:
	#		for alignment in blast_record.alignments[:10]:
	#		    #o+='\t'+ '****Alignment****'+'\n'
	#		    #o+='\t'+ 'sequence: '+ alignment.title+'\n'
	#		    #o+='\t'+ 'length: '+ str(alignment.length)+'\n'
	#
	#		    data[cluster][consensus][readnumber]['hits'].append(alignment.title)
	#		    data[cluster][consensus][readnumber][alignment.title] = alignment



#    for cluster in data:
#	print '\ncluster',cluster
#	for consensus in data[cluster]:
#	    print '\tconsensus',consensus
#	    for alignment_name in data[cluster][consensus][1]['hits']:
#	        if alignment_name in data[cluster][consensus][2]:
#		    print '\t\tboth r1 and r2 aligns to '+alignment_name
#		    print '\t\t\tr1:'
#		    for hsp in data[cluster][consensus][1][alignment_name].hsps:
#			print '\t\t\t\t'+ 'e value: '+ str(hsp.expect)
#			print '\t\t\t\tq: '+str(hsp.query_start)+'\t'+ hsp.query
#			print '\t\t\t\tm:  \t'+ hsp.match
#			print '\t\t\t\ts: '+str(hsp.sbjct_start)+'\t'+ hsp.sbjct
#			if len(hsp.query) < 40 : print '\tTo short will ba counted as "No Hit"'
#		    print '\t\t\tr2:'
#		    for hsp in data[cluster][consensus][2][alignment_name].hsps:
#			print '\t\t\t\t'+ 'e value: '+ str(hsp.expect)
#			print '\t\t\t\tq: '+str(hsp.query_start)+'\t'+ hsp.query
#			print '\t\t\t\tm:  \t'+ hsp.match
#			print '\t\t\t\ts: '+str(hsp.sbjct_start)+'\t'+ hsp.sbjct
#			if len(hsp.query) < 40 : print '\tTo short will ba counted as "No Hit"'
    return 0

#####
#check if run or imported // call main() or not
#####
if __name__ == "__main__":
    main()
#END of script
