#! /bin/env python

import os
import sys
MASTER = os.getpid()

man = """
    #\033[1m Pipeline for analysing "SEADseq" data: \033[0m
    # Usage: SEAseq2 <command> <path> [options]
    #
    # Available commands:
    #
    # 1. initiateAnalysis -- Initiate the analysis.
    #   Usage: SEAseq2 initiateAnalysis <path> <type>
    #   Initiation of the analysis is needed to set the path for storage of data, results and files used during runtime.
    #   During initiation the type of analysis also needs to be set either to Whole Fragment Analysis (WFA), rRNA Amplicon Classification (RAC) or iSeq analysis (iSeq).
    #   
    #
    # 2. addData -- Add data to be analyzed.
    #   Usage: SEAseq2 addData <path>
    #
    # 3. changeSettings -- Define the settings to use.
    #   Usage: SEAseq2 changeSettings <path>
    #   3.1 General settings.
    #       3.1.1 Barcode sequence identification
    #       3.1.2 Barcode sequence clustering
    #       3.1.3 Parallelization - UPPMAX/LOCAL
    #       3.1.4 Subsets and debugging
    #   3.2 Settings specific for the RAC mode.
    #   3.3 Settings specific for the WFA mode.
    #   3.4 Define a none standard design.
    #        Exprimental! will most probably not work.
    #
    # 4. startAnalysis -- Start the pipe
    #   Usage: SEAseq2 startAnalysis <path> <part>
    #   Specify <part> as "all" or any of the ones included in the analysis mode (see below).
    #
    #   4.1 In any mode the pipe will perform the following parts:
    #       4.1.1 readFastqs
    #            Reads the input files, identifies subsequences within each readpair such as barcode sequences (3.1.1),
    #            primers or adapter sequences, finally adds the information to the database.
    #            This part of the program run multiple processes in parallel.
    #       4.1.2 clusterBarcodeSequences
    #            Cluster the barcode sequences identified in 4.1.1 based on sequence similarity to identify all reads originating from the same original bead.
    #            This part of the program run one single process.
    #
    #   4.2 In RAC mode the pipe will perform the following parts:
    #       4.2.1 findContigs
    #            Find all variants of amplicons present within each cluster
    #
    #   4.3 In WFA mode NOT IMPLEMENTED
    #
    #   4.3 In iSeq mode the pipe will perform the following parts:
    #       4.3.1 findiSeqBarcodes
    """

###############
#  Functions  #
###############

def main():
    currentRun = SEAseqPipeLine()

def bufcount(filename):
	""" returns the number of lines in a file
	"""
	import gzip
	if filename.split('.')[-1] in ['gz','gzip']: f = gzip.open(filename)
	else: f = open(filename)
	lines = 0
	buf_size = 1024 * 1024
	read_f = f.read # loop optimization
	
	buf = read_f(buf_size)
	while buf:
		lines += buf.count('\n')
		buf = read_f(buf_size)
		f.close
	return lines

def hamming_distance(s1, s2):
	assert len(s1) == len(s2), 'Error: '+str(len(s1)) + ' != ' + str(len(s2))
	return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))

def levenshtein(s1, s2):
	if len(s1) < len(s2):
		return levenshtein(s2, s1)
	if not s1:
		return len(s2)
	previous_row = xrange(len(s2) + 1)
	for i, c1 in enumerate(s1):
		current_row = [i + 1]
		for j, c2 in enumerate(s2):
			insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer
			deletions = current_row[j] + 1       # than s2
			substitutions = previous_row[j] + (c1 != c2)
			if c1 == 'N' or c2 == 'N': substitutions -= 1 #if N then no mismatch
			current_row.append(min(insertions, deletions, substitutions))
		previous_row = current_row
	return previous_row[-1]

def foreachReadPairFromFastq(pair):
    
    pair.identifyBarcode()
    pair.isIlluminaAdapter()
    if SEAseqPipeLine.settings.mode == 'iSeq': pair.getiSeqBarcode()
    
    return pair

def foreachiSeqCluster(clusterId):

    #print clusterId
    cluster = BarcodeCluster(clusterId)
    cluster.loadClusterInfo()

    if cluster.readPairCount >= SEAseqPipeLine.settings.readsPerClusterCutOff:# and cluster.readPairCount < 110:
	cluster.findiSeqBarcodes()
	return ['OK',cluster]
    else:
	cluster.findiSeqBarcodes()
	return ['LOWREADCOUNT',cluster]

def revcomp(string):
	''' Takes a sequence and reversecomplements it'''
	complementary = comp(string)
	return complementary[::-1]

def comp(string):
	''' Takes a sequence and complements it'''
	complement = {'A':'T','T':'A',
				  'C':'G','G':'C',
				  'N':'N',
				  'R':'Y','Y':'R',
				  'K':'M','M':'K',
				  'B':'V','V':'B',
				  'D':'H','H':'D',
				  }
	compseq = "".join([complement.get(nt.upper(), '') for nt in string])
	return compseq

def foreachFindContigsClusters(clusterId):
    if not clusterId and clusterId != 0: return None
    cluster = BarcodeCluster(clusterId)
    cluster.loadClusterInfo()
    if cluster.readPairCount > 2: #and cluster.readPairCount < 110: # HARDCODED CUTOFF CHANGE
	#print cluster.id, cluster.barcodeSequence, cluster.readPairCount
	cluster.findContigs()
	#print 'generated'
    else:
	pass#print cluster.id, cluster.barcodeSequence, cluster.readPairCount,'to few reads skipping contig generation.'

    return cluster

def getMemMB():
    import re
    meminfo = open('/proc/meminfo').read()
    matched = re.search(r'^MemTotal:\s+(\d+)', meminfo)
    if matched:
	mem_total_kB = int(matched.groups()[0])
	return mem_total_kB/1024
    else: return None

#############
#  Classes  #
#############

class ReadPair(object):
    
    def __init__(self, currentRead, header1, header2, sequence1, sequence2, qual1, qual2,handleCoordinates,clusterId,annotations, fastq1):
	self.id = currentRead
	self.r1Header   = header1
	self.r2Header   = header2
	self.r1Seq      = sequence1
	self.r2Seq      = sequence2
	self.r1Qual     = qual1
	self.r2Qual     = qual2
	self.fileOrigin = fastq1
	self.handleCoordinates = handleCoordinates
	self.annotations= annotations    

    @property
    def databaseTuple(self, ):
	""" returning a tuple that is formated to be added to sql database"""
	# data base has following info:
	#       (id,          header,  sequence1, sequence2, quality1,quality2,handleCoordinates,clusterId,annotation,fromFastq)
	
	# Dumping the quality values
	return  (self.id,     self.r1Header,self.r1Seq,self.r2Seq,self.r1Qual,self.r2Qual,   str(self.handleCoordinates),None,     str(self.annotations),self.fileOrigin)

    def matchSequence(self, readsequence, matchsequence, maxDistance, matchfunk=hamming_distance):
	
	import re
	#matchfunk = hamming_distance

	startPosition = None
	endPosition   = None
	missmatches   = None

	perfect_match = re.search(matchsequence, readsequence)
	if perfect_match:
	    startPosition = perfect_match.start()
	    endPosition = perfect_match.end()
	    missmatches = 0
	
	elif int(maxDistance):
	    mindist = [10000,-1]
	    for i in range(len(readsequence)):
		
		if i+len(matchsequence) <= len(readsequence): dist = matchfunk(matchsequence,readsequence[i:i+len(matchsequence)])
		else: dist = 10001
		
		if dist < mindist[0]: mindist =[dist,i]

	    if mindist[0] < int(maxDistance)+1:
		startPosition = mindist[1]
		endPosition = mindist[1]+len(matchsequence)
		missmatches = mindist[0]
	    else:
		startPosition = None
		endPosition = None
		missmatches = None

	return [startPosition,endPosition,missmatches]

    def isIlluminaAdapter(self, ):
	
	import math
	
	for i in [15]:#range(15):
	    handleStartPosition,handleEndPosition,handleMissMatches = self.matchSequence(self.r1Seq, 'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC'[:35-i], int(math.ceil(float(35-i)*0.1)))
	    if handleStartPosition:
		self.annotations['Read1IsIlluminaAdapter'] = str(handleMissMatches)+':'+ str(handleStartPosition)
		break
	
	for i in [15]:#range(15):
	    handleStartPosition,handleEndPosition,handleMissMatches = self.matchSequence(self.r2Seq, 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT'[:34-i], int(math.ceil(float(34-i)*0.1)))
	    if handleStartPosition:
		self.annotations['Read2IsIlluminaAdapter'] = str(handleMissMatches)+':'+ str(handleStartPosition)
		break

    def identifyBarcode(self, ):

	handleStartPosition,handleEndPosition,handleMissMatches = self.matchSequence(self.r1Seq, SEAseqPipeLine.settings.handleSequence, SEAseqPipeLine.settings.maxHandleMissMatches)
	
	if handleMissMatches > 0 or handleMissMatches == None: self.annotations['handleMissMatches'] = handleMissMatches
	elif handleMissMatches == 0:self.annotations['handlePerfectMatch'] = True
	else: raise ValueError, 'handleMissMatches must be integer >= 0 or None'
	
	if handleStartPosition == SEAseqPipeLine.settings.barcodeLength:
	    self.handleCoordinates = [handleStartPosition,handleEndPosition]
	elif handleStartPosition == None: self.annotations['handleNotFound'] = True
	else: self.annotations['ErronousHandlePosition'] = handleStartPosition

    def identifyPrimerPair(self,):
	pass

    def getiSeqBarcode(self, ):
	
	funkVersion = 2

	if funkVersion == 1:
	# OLD VERSION OF FUNCTION NEW VERSION FOR DAVID AND MAHYAS NEW DESIGN CAN BE FOUND BELOW
	    X = 'CAACTACACGGCTCACCTG'
	    Y = 'CTGCTACGACCACTGGATG'
	    fwdhandleXStartPosition,fwdhandleXEndPosition,fwdhandleXMissMatches = self.matchSequence(self.r1Seq, X, SEAseqPipeLine.settings.maxHandleMissMatches)
	    fwdhandleYStartPosition,fwdhandleYEndPosition,fwdhandleYMissMatches = self.matchSequence(self.r1Seq, Y, SEAseqPipeLine.settings.maxHandleMissMatches)
	    revhandleXStartPosition,revhandleXEndPosition,revhandleXMissMatches = self.matchSequence(self.r2Seq, revcomp(X), SEAseqPipeLine.settings.maxHandleMissMatches)
	    revhandleYStartPosition,revhandleYEndPosition,revhandleYMissMatches = self.matchSequence(self.r2Seq, revcomp(Y), SEAseqPipeLine.settings.maxHandleMissMatches)
	    iSeqBarcodes ={
		revcomp('TGATGC'):revcomp('TGATGC'),
		revcomp('TACGTC'):revcomp('TACGTC'),
		revcomp('ACTAGC'):revcomp('ACTAGC'),
		revcomp('GTCGAT'):revcomp('GTCGAT')
	    }
	    if fwdhandleXEndPosition and fwdhandleYStartPosition and revhandleXStartPosition and revhandleYEndPosition:
		if self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition] == revcomp(self.r2Seq[revhandleYEndPosition:revhandleXStartPosition]):
		    if fwdhandleYStartPosition-fwdhandleXEndPosition == 6:
			try: self.annotations['iSeqBarcode'] = iSeqBarcodes[ self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition] ]
			except KeyError: self.annotations['iSeqBarcode'] = 'OTHER:'+self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition]
		    elif fwdhandleYStartPosition-fwdhandleXEndPosition < 6: self.annotations['iSeqBarcode'] = 'ShortBarcode:'+self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition]
		    elif fwdhandleYStartPosition-fwdhandleXEndPosition > 6: self.annotations['iSeqBarcode'] = 'LongBarcode:'+self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition]
		else: self.annotations['iSeqBarcode'] = 'NoPEMatch'
	    else:self.annotations['iSeqBarcode'] = 'HandleSequencesNotFound'
	    return 0

	if funkVersion == 2:
	#   NEW VERSION:
	    X = 'CAACTACACGGCTCACCTG'
	    Y = 'CTGCTACGACCACTGGATG'
    
	    fwdhandleXStartPosition,fwdhandleXEndPosition,fwdhandleXMissMatches = self.matchSequence(self.r1Seq, X, SEAseqPipeLine.settings.maxHandleMissMatches)
	    fwdhandleYStartPosition,fwdhandleYEndPosition,fwdhandleYMissMatches = self.matchSequence(self.r1Seq, Y, SEAseqPipeLine.settings.maxHandleMissMatches)
	    revhandleXStartPosition,revhandleXEndPosition,revhandleXMissMatches = self.matchSequence(self.r2Seq, revcomp(X), SEAseqPipeLine.settings.maxHandleMissMatches)
	    revhandleYStartPosition,revhandleYEndPosition,revhandleYMissMatches = self.matchSequence(self.r2Seq, revcomp(Y), SEAseqPipeLine.settings.maxHandleMissMatches)
    
	    iSeqBarcodes ={
		revcomp('ACAGTC'):'mBc_01',#revcomp('ACAGTC'), # cISEQ_mBc01
		revcomp('ATACGC'):'mBc_02',#revcomp('ATACGC'), # cISEQ_mBc02
		revcomp('TGATGC'):'mBc_03',#revcomp('TGATGC'), # cISEQ_mBc03
		revcomp('TACGTC'):'mBc_04',#revcomp('TACGTC'), # cISEQ_mBc04
		revcomp('ACTAGC'):'mBc_05',#revcomp('ACTAGC'), # cISEQ_mBc05
		revcomp('GTCGAT'):'mBc_06'#revcomp('GTCGAT'), # cISEQ_mBc06
	    }
    
	    self.annotations['umi'] = 'NotIdentified'
    
	    if fwdhandleXEndPosition and fwdhandleYStartPosition and revhandleXStartPosition and revhandleYEndPosition:
		if self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition] == revcomp(self.r2Seq[revhandleYEndPosition:revhandleXStartPosition]):
		    if fwdhandleYStartPosition-fwdhandleXEndPosition == 6+15:
			try: self.annotations['iSeqBarcode'] = iSeqBarcodes[ self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition-15] ]
			except KeyError: self.annotations['iSeqBarcode'] = 'OTHER:'+self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition-15]
			self.annotations['umi'] = self.r1Seq[fwdhandleXEndPosition+6:fwdhandleYStartPosition]
		    elif fwdhandleYStartPosition-fwdhandleXEndPosition < 6+15: self.annotations['iSeqBarcode'] = 'ShortBarcode:'+self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition]
		    elif fwdhandleYStartPosition-fwdhandleXEndPosition > 6+15: self.annotations['iSeqBarcode'] = 'LongBarcode:'+self.r1Seq[fwdhandleXEndPosition:fwdhandleYStartPosition]
		else: self.annotations['iSeqBarcode'] = 'NoPEMatch'
	    else:self.annotations['iSeqBarcode'] = 'HandleSequencesNotFound'
	    return 0

class BarcodeCluster(object):
    
    def __init__(self, clusterId):
	
	self.id = clusterId
	self.readPairCount = None
	self.contigCount = None
	self.barcodeSequence = None
	self.barcodeQuality = None
	self.readPairIdsList = []
	self.contigIdsList = []
	self.annotations = None

	self.readPairs = []
	self.readPairsById = {}
	self.readPairIdentities = []
	self.readPairsPassFilter = []
	self.readPairsNotPassingFilter = []

	self.contigSequences = []
	self.contigSequencesPassFilter = []
	self.contigSequencesNotPassingFilter = []
	self.nonSingletonContigs = None
	
	self.filesCreated = []

    def loadClusterInfo(self, ):
	
	SEAseqPipeLine.database.getConnection()
	info = SEAseqPipeLine.database.c.execute('SELECT clusterId,clusterTotalReadCount,readPairsList,readBarcodeIdentitiesList,clusterBarcodeSequence,clusterBarcodeQuality,contigSequencesList,annotations FROM barcodeClusters WHERE clusterId=?',(self.id,)).fetchall()
	SEAseqPipeLine.database.commitAndClose()
	assert len(info) == 1, 'More than one ('+str(len(info))+') clusters found with id '+str(self.id)
	(clusterId,clusterTotalReadCount,readPairsList,readBarcodeIdentitiesList,clusterBarcodeSequence,clusterBarcodeQuality,contigSequencesList,annotations) = info[0]
	assert clusterId == self.id
	self.readPairCount      = int(clusterTotalReadCount)
	self.readPairIdsList    = eval(readPairsList)
	self.readPairIdentities = eval(readBarcodeIdentitiesList)
	self.barcodeSequence    = clusterBarcodeSequence
	self.barcodeQuality     = clusterBarcodeQuality
	if contigSequencesList:
	    self.contigIdsList  = eval(contigSequencesList)
	if annotations:
	    self.annotations    = eval(annotations)
	return 0

    def loadReadPairs(self, ):
	
	import sqlite3
	try:
	    for readPair in SEAseqPipeLine.database.getReadPairs(self.readPairIdsList):
		self.readPairs.append(readPair)
		self.readPairsById[readPair.id] = readPair
	except sqlite3.OperationalError: print 'ERROR: BarcodeCluster.loadReadPairs() is giving a sqlite3.OperationalError!!'
	
	return 0

    def loadContigSequences(self, ):
	print 'Not implemented function BarcodeCluster.loadContigSequences()'
	return 0

    def generateReadPairsFastq(self, oneFile=True, concatenate=True,r1Part='nobc'):
	
	#
	# Create files
	#
	if oneFile:
	    fastqFile = open(SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.fastq','w')
	    self.filesCreated.append(fastqFile.name)
	else:
	    fastqFileR1 = open(SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.R1.fastq','w')
	    fastqFileR2 = open(SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.R2.fastq','w')
	    self.filesCreated.append(fastqFileR1.name)
	    self.filesCreated.append(fastqFileR2.name)

	if not self.readPairs: self.loadReadPairs()
	
	#
	# check readcount
	#
	assert len(self.readPairs) == self.readPairCount, 'BarcodeCluster.readcount does not match number of reads for cluster='+self.id
	
	#
	# create output and write to file(s)
	#
	for readPair in self.readPairs:

	    # remove barcode sequence and handle if requested
	    if r1Part == 'full':
		r1Seq  = readPair.r1Seq
		r1Qual = readPair.r1Qual
	    elif r1Part == 'nobc':
		r1Seq  = readPair.r1Seq[ readPair.handleCoordinates[1]:SEAseqPipeLine.results.minR1readLength]
		r1Qual = readPair.r1Qual[readPair.handleCoordinates[1]:SEAseqPipeLine.results.minR1readLength]
	    
	    # concatenated output or not
	    if concatenate:
		r1Header = '@'+str(readPair.id)#readPair.r1Header+' '+str(readPair.annotations)
		for annotation in readPair.annotations: r1Header += ' '+annotation+'='+str(readPair.annotations[annotation])
		r1 = r1Header+'\n'+r1Seq+'CCCCCCCCCCGGGGGGGGGG'+readPair.r2Seq[:SEAseqPipeLine.results.minR2readLength]+'\n'+'+'+'\n'+r1Qual+'CCCCCCCCCCGGGGGGGGGG'+readPair.r2Qual[:SEAseqPipeLine.results.minR2readLength]+'\n'
		r2 = ''
	    else:
		r1 = readPair.r1Header.split(' ')[0]+'/1 '+' '.join(readPair.r1Header.split(' ')[1:])+'\n'+r1Seq+'\n'+'+'+'\n'+r1Qual+'\n'
		r2 = readPair.r2Header.split(' ')[0]+'/2 '+' '.join(readPair.r1Header.split(' ')[1:])+'\n'+readPair.r2Seq[:SEAseqPipeLine.results.minR2readLength]+'\n'+'+'+'\n'+readPair.r2Qual[:SEAseqPipeLine.results.minR2readLength]+'\n'
	    
	    # write to file(s)
	    if oneFile:
		    fastqFile.write(r1+r2)
	    else:
		fastqFileR1.write(r1)
		fastqFileR2.write(r2)
	    
	#
	# close file connection(s)
	#
	if oneFile:
	    fastqFile.close()
	else:
	    fastqFileR1.close()
	    fastqFileR2.close()
	
	return 0

    def contigsByConcatenateCDHIT(self,):

	##
	## imports
	##
	import subprocess

	self.generateReadPairsFastq(oneFile=True, concatenate=True,r1Part='nobc')
	
	clusteringProgramsLogfile = open(SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.cdHitlog.txt','w')
	memory = (getMemMB()-10*1024)/(SEAseqPipeLine.settings.parallelProcesses-1)
	
	# cluster raw sequences
	command = ['cd-hit-454',
		    '-i',SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.fastq',
		    '-o',SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.contigs',
		    '-g','1',      # mode
		    '-n','5',      # wrodsize
		    '-M',str(memory),      # memory limit
		    '-t',str(SEAseqPipeLine.settings.parallelProcesses),      # threads
		    '-c',str(0.95) # identity cutoff THIS SHOULD BE A SETTING CHANGE NEEDED!
		    ]
	#SEAseqPipeLine.logfile.write('Starting command: '+' '.join(command)+'\n')
	cdhit = subprocess.Popen(command,stdout=clusteringProgramsLogfile,stderr=subprocess.PIPE )
	errdata = cdhit.communicate()
	if cdhit.returncode != 0:
		print 'cmd: '+' '.join( command )
		print 'cd-hit view Error code', cdhit.returncode, errdata
		sys.exit()
	
	# Build consensus sequences for read pair clusters
	command = ['cdhit-cluster-consensus',
		SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.contigs.clstr',
		SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.fastq',
		SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.consensus',
		SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.aligned'
		]
	#SEAseqPipeLine.logfile.write('Starting command: '+' '.join(command)+'\n')
	ccc = subprocess.Popen(command,stdout=clusteringProgramsLogfile,stderr=subprocess.PIPE )
	errdata = ccc.communicate()
	if ccc.returncode != 0:
		print 'cmd: '+' '.join( command )
		print 'cdhit-cluster-consensus view Error code', ccc.returncode, errdata
		sys.exit()
	clusteringProgramsLogfile.close()
	self.filesCreated.append( SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.aligned' )
	self.filesCreated.append( SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.cdHitlog.txt' )
	self.filesCreated.append( SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.consensus.fastq' )
	self.filesCreated.append( SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.contigs' )
	self.filesCreated.append( SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.contigs.clstr' )

	#
	# inititate variables
	#
	totalContigsCount = 0
	contigs = {}
	singletonContigs = {}
	nonSingletonContigs = {}
	
	#
	# open file connections
	#
	consensusFile = open(SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.consensus.fastq')
	clstrFile = open(SEAseqPipeLine.tempFileFolder+'/cluster_'+str(self.id)+'.contigs.clstr')
	
	#
	# load cluster ids and consensus sequences
	#
	while True:
	    header = consensusFile.readline().rstrip()
	    sequence = consensusFile.readline().rstrip()
	    junk = consensusFile.readline().rstrip()
	    quality = consensusFile.readline().rstrip()
	    if header == '': break
	    totalContigsCount += 1
	    header = header.split('_cluster_')
	    contigId = int(header[1].split(' ')[0])
	    splitOk = True
	    try:
		r1Seq = sequence.split('CCCCCCCCCCGGGGGGGGGG')[0]
		r2Seq = sequence.split('CCCCCCCCCCGGGGGGGGGG')[1]
		r1Qual = quality[:len(r1Seq)]
		r2Qual = quality[-len(r2Seq):]
	    except IndexError:
		print 'Cannot split contig',contigId,' for cluster',self.id,'contig skipped!\n(nosplit! ->',sequence,',list',sequence.split('CCCCCCCCCCGGGGGGGGGG'),')'
		splitOk=False#continue
	    if header[0][:2] == '@s':
		singletonContigs[contigId] = {'contigReadCount':1,'readPairs':[],'identities':[],'consensusSequence':sequence,'consensusQuality':quality,'annotations':{'Singleton':True}}
		contigs[contigId] = singletonContigs[contigId]
	    elif header[0][:2] == '@c':
		nonSingletonContigs[contigId] = {'contigReadCount':int(header[1].split(' ')[2]),'readPairs':[],'identities':[],'consensusSequence':sequence,'consensusQuality':quality,'annotations':{}}
		contigs[contigId] = nonSingletonContigs[contigId]
	    else: raise ValueError
	    if not splitOk: contigs[contigId]['annotations']['splitNotOk'] = True

	#
	# Load what readpairs are in each cluster
	#
	for line in clstrFile:
	    line = line.rstrip()
	    if line[0] == '>':
		contigId = int(line.split(' ')[1])
		continue
	    elif line[0] == '0':
		readId = int(line.split('>')[1].split('.')[0])
		identity = 'seed'
		assert line.split(' ')[-1] == '*', 'Error in file format of clstr file'
	    else:
		readId = int(line.split('>')[1].split('.')[0])
		identity = float(line.split(' ')[-1].split('/')[-1].split('%')[0])
	    contigs[contigId]['readPairs'].append(readId)
	    contigs[contigId]['identities'].append(identity)
	
	self.nonSingletonContigs = nonSingletonContigs
	
	import os
	for filename in self.filesCreated: os.remove(filename)
	
	return 0

    def findContigs(self, ):
	
	if SEAseqPipeLine.settings.mode == 'RAC':
	
	    if True:#'CDHIT'
		self.contigsByConcatenateCDHIT()
		
	    elif False:#'ASSEMBLY'
		self.generateReadPairsFastq(oneFile=False, concatenate=False,r1Part='nobc')
	
	elif SEAseqPipeLine.mode == 'WFA':
	    if False:#'ASSEMBLY'
		self.generateReadPairsFastq(oneFile=False, concatenate=False,r1Part='nobc')
	    elif False:#'MAPPINGandReferenceAidedASSEMBLY'
		self.generateReadPairsFastq(oneFile=False, concatenate=False,r1Part='nobc')
	    else: pass
	
	return 0

    def saveContigInfoToDatabase(self, ):

	import sqlite3
	try:
	    addValues = []
	    SEAseqPipeLine.database.getConnection()
	    if self.nonSingletonContigs:
		for contigId, data in self.nonSingletonContigs.iteritems():
		    for readPairId in data['readPairs']:
			for annotation, value in self.readPairsById[int(readPairId)].annotations.iteritems(): data['annotations'][annotation]=value
		    #                  contigId, contigTotalReadCount,      readPairsList,     readPairIdentitiesList,    consensusSequence,       consensusQuality,    clusterId,annotations
		    addValues.append( (contigId,data['contigReadCount'],str(data['readPairs']),str(data['identities']),data['consensusSequence'],data['consensusQuality'],self.id,str(data['annotations'])) )
		    self.contigIdsList.append(contigId)
		SEAseqPipeLine.database.c.executemany('INSERT INTO contigs VALUES (?,?,?,?,?,?,?,?)', addValues)
		SEAseqPipeLine.database.c.execute('UPDATE barcodeClusters SET contigSequencesList=? WHERE clusterId=?', (str(self.contigIdsList), self.id))
		#print self.id, self.contigIdsList
		SEAseqPipeLine.database.commitAndClose()
		#print 'Write OK!!'
	except sqlite3.OperationalError: print 'ERROR: BarcodeCluster.saveContigInfoToDatabase() is giving a sqlite3.OperationalError!!'

	return 0

    def findiSeqBarcodes(self, ):
	
	import re
	import operator
	
	assert SEAseqPipeLine.settings.mode == 'iSeq', 'Error: this type of analysis is not supported by the current analysis mode.'

	#
	# load readpairs if needed
	#
	if not self.readPairs: self.loadReadPairs()
	
	#
	# check readcount
	#
	assert len(self.readPairs) == self.readPairCount, 'BarcodeCluster.readcount does not match number of reads for cluster='+self.id
	
	#
	# find iSeqBarcodes in read population
	#
	self.iSeqBarcodes = {}
	for readPair in self.readPairs:
	    try: self.iSeqBarcodes[readPair.annotations['iSeqBarcode']].append(readPair.id)
	    except KeyError: self.iSeqBarcodes[readPair.annotations['iSeqBarcode']] = [readPair.id]
	assert sum( [len(LIST) for LIST in self.iSeqBarcodes.values()] ) == self.readPairCount, '\n\n#\n# Error, the number of annotations for iseqbarcodes does not match the number of reads ('+str(len(self.iSeqBarcodes))+' != '+str(self.readPairCount)+')\n#\n'
	
	self.iSeqBarcodesPassingFilter = []
	self.iSeqOutString =  'BarcodeSequenceCluster #'+str(self.id)+', barcode '+str(self.barcodeSequence)+'\n'
	self.iSeqOutString += 'Has '+str(self.readPairCount)+' read pairs in total'+'\n'
	self.iSeqOutString += 'The following iSeq barcodes were found:\n'
	for iSeqBarcode,readPairsList in self.iSeqBarcodes.iteritems():
	    total = len(readPairsList)
	    percentage = round(100*float(total)/float(self.readPairCount),2)
	    self.iSeqOutString += '\t'+iSeqBarcode+'\t'+str(total)+'\t'+str(percentage)+'%\n'
	    if percentage >= 10.0 and total >= 2 and not re.match('(OTHER)|(ShortBarcode)|(LongBarcode)|(NoPEMatch)|(HandleSequencesNotFound)',iSeqBarcode):
		self.iSeqBarcodesPassingFilter.append(iSeqBarcode)
	self.iSeqBarcodesPassingFilter.sort()
	self.iSeqOutString += '\n\n'

	#
	# correct the weights for iSeqBarcodes using UMIs
	#
	doUmiCorrection = True
	if doUmiCorrection:
	    tmpCount = 0
	    self.iSeqOutString += 'Correcting for UMIs, trashing everything where UMI NA:\n'
	    self.iSeqBarcodesUMI = {}
	    for readPair in self.readPairs:
		tmpCount += 1
		if readPair.annotations['umi'] == 'NotIdentified': continue
		try: self.iSeqBarcodesUMI[readPair.annotations['iSeqBarcode']][readPair.annotations['umi']].append(readPair.id)
		except KeyError:
		    try:
		     self.iSeqBarcodesUMI[readPair.annotations['iSeqBarcode']][readPair.annotations['umi']] = [readPair.id]
		    except KeyError:
		     self.iSeqBarcodesUMI[readPair.annotations['iSeqBarcode']] = {readPair.annotations['umi']:[readPair.id]}
	    
	    #for umiDict in self.iSeqBarcodes.values():
	    #    tmpCount += sum( [len(LIST) for LIST in umiDict.values()] )
	    assert tmpCount == self.readPairCount, '\n\n#\n# Error, the number of annotations for iseqbarcodes does not match the number of reads ('+str(tmpCount)+' != '+str(self.readPairCount)+')\n#\n'
	    
	    
	    preFiletOut = True
	    if preFiletOut: #Raw UMI output
		umiBcComboCount = sum( [len(DICT) for DICT in self.iSeqBarcodesUMI.values()] )
		self.iSeqOutString += 'Has '+str(umiBcComboCount)+' unique combinatiuons of barcodes and UMIs before filtering'+'\n'
		self.iSeqOutString += 'namely the following:\n'
		self.iSeqOutString += '#\tiSeqBarcode\tUMIcount\tUMI%\n'
		for iSeqBarcode,umiDict in self.iSeqBarcodesUMI.iteritems():
		    total = len(umiDict)
		    percentage = round(100*float(total)/float(umiBcComboCount),2)
		    self.iSeqOutString += '\t'+iSeqBarcode+'\t'+str(total)+'\t'+str(percentage)+'%\t'+str([umi+'='+str(len(readPairsList)) for umi,readPairsList in umiDict.iteritems()])+'\n'
	    
	    #
	    # "cluster" UMIs ie allow for mismatches
	    #
	    tmpDict = {}
	    maxMisMatch = SEAseqPipeLine.settings.umiMaxMisMatch
	    self.iSeqOutString += '\nMerging UMI pairs with hamming distance '+str(maxMisMatch)+' or lower:\n'
	    for iSeqBarcode,umiDict in self.iSeqBarcodesUMI.iteritems():
		umiDictLength = 9e99
		
		while umiDictLength > len(umiDict):
		    self.iSeqOutString += 'New Merging Round for '+iSeqBarcode+' '+str(umiDictLength)+' > '+str(len(umiDict))+' ... \n'
		    umiDictLength = len(umiDict)
		    clusteredUmiDict = {}
		    mergedUMIs = []


		    #create the umisByReadCount dict
		    umisByReadCount = {}		
		    for umi,readPairsList in umiDict.iteritems():
			try: umisByReadCount[len(readPairsList)].append(umi)
			except KeyError:
			     umisByReadCount[len(readPairsList)] = [umi]
		    
		    # get umis in read count order
		    for count, umis in sorted(umisByReadCount.iteritems(), key=operator.itemgetter(0))[::-1]:
			for umi in umis:
			    if umi in mergedUMIs: continue
			    else:
			        assert count == len(umiDict[umi]), 'Error: the umi read count does not match expected! (1)'
				clusteredUmiDict[umi] = umiDict[umi]
				clusteringOccured = False
				for lowerCount, umisLower in sorted(umisByReadCount.iteritems(), key=operator.itemgetter(0))[::-1]:
				    for umiLower in umisLower:
					#assert lowerCount == len(umiDict[umiLower]), 'Error: the umi read count does not match expected! (2)'
					if umiLower == umi or lowerCount >= count or umiLower in mergedUMIs: continue
					else:
					    if hamming_distance(umi,umiLower) <= maxMisMatch:
						clusteredUmiDict[umi] += umiDict[umiLower]
						mergedUMIs.append(umiLower)
						self.iSeqOutString += '\tmerging UMI '+umi+' ('+str(count)+') with '+str(umiLower)+' ('+str(lowerCount)+')\n'
					    else: pass #distance to large for merging check next
		    umiDict = clusteredUmiDict
		
		tmpDict[iSeqBarcode] = umiDict
	    
	    #
	    #remove all umis below readCutoff
	    #
	    tmpDict = {}
	    readsPerUmiCutOff = SEAseqPipeLine.settings.readsPerUmiCutOff#reads
	    self.readsPerRawUmiList=[]
	    self.iSeqOutString += 'Removing all UMIs with less than '+str(readsPerUmiCutOff)+' reads ... \n'
	    for iSeqBarcode,umiDict in self.iSeqBarcodesUMI.iteritems():
		tmpDict[iSeqBarcode] = {}
		for umi,readPairsList in umiDict.iteritems():
		    self.readsPerRawUmiList.append(len(readPairsList))
		    if len(readPairsList) >= readsPerUmiCutOff:
			tmpDict[iSeqBarcode][umi] = readPairsList
		    else: self.iSeqOutString += '\tremoved:'+iSeqBarcode+', umi='+umi+' '+str(len(readPairsList))+' reads\n'
	    tmpDict2 = {}
	    for iSeqBarcode,umiDict in tmpDict.iteritems():
		if len(umiDict): tmpDict2[iSeqBarcode] = tmpDict[iSeqBarcode]
		else: self.iSeqOutString += '\tremoved:'+iSeqBarcode+' no umis PF\n'
	    self.iSeqBarcodesUMI = tmpDict2
	    
	    #Get AbBcs with ok UMIs over cutoffs
	    umiBcComboCount = sum( [len(DICT) for DICT in self.iSeqBarcodesUMI.values()] )
	    self.iSeqBarcodesPassingFilter = []
	    self.iSeqOutString += 'Has '+str(umiBcComboCount)+' unique combinatiuons of barcodes and UMIs in total'+'\n'
	    self.iSeqOutString += 'After UMI correction the following iSeq barcodes were found:\n'
	    self.iSeqOutString += '#\tiSeqBarcode\tUMIcount\tUMI%\n'
	    for iSeqBarcode,umiDict in self.iSeqBarcodesUMI.iteritems():
		total = len(umiDict)
		percentage = round(100*float(total)/float(umiBcComboCount),2)
		self.iSeqOutString += '\t'+iSeqBarcode+'\t'+str(total)+'\t'+str(percentage)+'%\t'+str([umi+'='+str(len(readPairsList)) for umi,readPairsList in umiDict.iteritems()])+'\n'
		if percentage >= 0.0 and total >= 1 and not re.match('(OTHER)|(ShortBarcode)|(LongBarcode)|(NoPEMatch)|(HandleSequencesNotFound)',iSeqBarcode):
		    self.iSeqBarcodesPassingFilter.append(iSeqBarcode)
	    self.iSeqBarcodesPassingFilter.sort()
	    #self.iSeqOutString += '\n\n'
	
	return 0

class Database(object):
    
    def __init__(self, dbPath):
        self.path = dbPath
    
    def getConnection(self,):
        #
        # Import useful stuff
        #
        import sqlite3
        import sys

        #
        # Create database and set
        #
        try: self.conn = sqlite3.connect(self.path)
        except sqlite3.OperationalError:
            print 'ERROR: Trouble with the database, plase check your commandline.'
            sys.exit()
        self.c = self.conn.cursor()
    
    def commitAndClose(self,):
        #
        # commit changes and close connection
        #
        self.conn.commit()
        self.conn.close()
    
    def create(self,):
	""" creates the database holding all information used in the analysis """
	
	self.getConnection()
	
	#
	# Create tables
	#
	self.c.execute('''CREATE TABLE runs (startTime,command,commandLine,finishedSuccessfully,masterPid)''')
	self.c.execute('''CREATE TABLE fastqs (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength,PRIMARY KEY (filePairId))''');
	self.c.execute('''CREATE TABLE settings (variableName,defaultValue,value,setTime,PRIMARY KEY (variableName))''')
	self.c.execute('''CREATE TABLE results (resultName,defaultValue,value,setTime,PRIMARY KEY (resultName))''')
	
	self.commitAndClose()

    def addToRunsTable(self, startTime, command, commandLine, finishedSuccessfully, masterPid):
        
        self.getConnection()
        
        #
        # check if pid already in database
        #
        t = (masterPid,)
        data = self.c.execute('SELECT masterPid, startTime FROM runs WHERE masterPid=?',t).fetchall()        
        if data:
            for tmp1,tmp2 in data:

        #
        # if pid and startTime matches update the "finishedSuccessfully" entry
        #
                if tmp1 == masterPid and tmp2 == startTime:
                    values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
                    self.c.execute('UPDATE runs SET finishedSuccessfully=? WHERE masterPid=? AND startTime=?', (finishedSuccessfully,masterPid,startTime))
        
        #
        # if not in the database add a new row
        #
        else:
            values = (startTime, command, commandLine, finishedSuccessfully, masterPid)
            self.c.execute('INSERT INTO runs VALUES (?,?,?,?,?)', values)
        
        self.commitAndClose()
        
        return 0
    
    def addFastqs(self, fastq1, fastq2):
        
        #
        # Imports
        #
        import sys
        
        #
        # open connection to database
        #
        self.getConnection()
        
        filePairId = None
        filePairIds = []
        
        #
        # check if any of the fastqs already in database
        #
        data = self.c.execute('SELECT filePairId,fastq1,fastq2 FROM fastqs').fetchall()
        if data:
            for filePair in data:
                filePairId = int(filePair[0])
                filePairIds.append(filePairId)
                for fastq in [fastq1, fastq2]:
                    if fastq in filePair:
                        message = 'ERROR: '+fastq+' already in the database.\nExiting after error.'
                        print message
                        SEAseqPipeLine.logfile.write(message+'\n')
                        sys.exit(1)
        #
        # if not in the database add a new row
        #
        SEAseqPipeLine.logfile.write('Getting readcount for file'+fastq1+' ... \n')
        readCount = bufcount(fastq1)/4 #one read is four lines
        SEAseqPipeLine.logfile.write('...done. The file has '+str(readCount)+' reads.\n')
        addedToReadsTable = False#SEAseqPipeLine.startTimeStr
        minReadLength = 'NA'

        if filePairIds: filePairId = max(filePairIds)+1
        else: filePairId = 0
        values = (filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength)
        self.c.execute('INSERT INTO fastqs VALUES (?,?,?,?,?,?)', values)
        
        self.commitAndClose()
        
        return 0
   
    def addReads(self, readsToAdd):

        #
        # Imports
        #
        import sys
        
        #
        # open connection to database
        #
        self.getConnection()
        
        #
        # add the data in readsToAdd to the reads table
        #

	self.c.executemany('INSERT INTO reads VALUES (?,?,?,?,?,?,?,?,?,?)', readsToAdd)
        
        self.commitAndClose()
        
        return 0

    def getFastqs(self,):
        #
        # Imports
        #
        import sys
        
        #
        # open connection to database
        #
        self.getConnection()
                
        #
        # get att data in fastqs table
        #
        filePairs = self.c.execute('SELECT filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength FROM fastqs').fetchall()
        
        self.commitAndClose()
        
        #return [[readCount,fastq1,fastq2] if (not addedToReadsTable) else None for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength in filePairs]
	return [[filePairId,readCount,fastq1,fastq2] for filePairId,fastq1,fastq2,readCount,addedToReadsTable,minReadLength in filePairs]

    def getAllReadPairs(self,):
        #
        # Imports
        #
        import sys
        
        #
        # open connection to database
        #
        self.getConnection()
                
        #
        # get att data in fastqs table
        #
        readPairs = self.c.execute('SELECT id,header,sequence1,sequence2,quality1,quality2,handleCoordinates,clusterId,annotation,fromFastq FROM reads')
        
	while True:
	    
	    rows = readPairs.fetchmany()#size=readPairs.arraysize)
	    
	    if not rows: break
	    
	    for row in rows:
		pairId,header,sequence1,sequence2,qual1,qual2,handleCoordinates,clusterId,annotations,fromFastq = row
		yield ReadPair(pairId, header, header, sequence1, sequence2, qual1, qual2,eval(handleCoordinates),clusterId,eval(annotations), fromFastq)
	
        self.commitAndClose()

    def getReadPairs(self, listOfIds):
        #
        # Imports
        #
        import sys
        
        #
        # open connection to database
        #
        self.getConnection()
                
        #
        # get att data in fastqs table
        #
#        readPairs = self.c.execute('SELECT id,header,sequence1,sequence2,quality1,quality2,handleCoordinates,clusterId,annotation,fromFastq FROM reads WHERE id IN ('+','.join([str(readPairId) for readPairId in listOfIds])+')')
#        
#	while True:
#	    
#	    rows = readPairs.fetchmany()#size=readPairs.arraysize)
#	    
#	    if not rows: break
#	    
#	    for row in rows:
#		pairId,header,sequence1,sequence2,qual1,qual2,handleCoordinates,clusterId,annotations,fromFastq = row
#		yield ReadPair(pairId, header, header, sequence1, sequence2, qual1, qual2,eval(handleCoordinates),clusterId,eval(annotations), fromFastq)
	
	#
	# alternatively this
	#
	for readPairId in listOfIds:
	    row = self.c.execute('SELECT id,header,sequence1,sequence2,quality1,quality2,handleCoordinates,clusterId,annotation,fromFastq FROM reads WHERE id=?', (int(readPairId), ) ).fetchone()
	    pairId,header,sequence1,sequence2,qual1,qual2,handleCoordinates,clusterId,annotations,fromFastq = row
	    yield ReadPair(pairId, header, header, sequence1, sequence2, qual1, qual2,eval(handleCoordinates),clusterId,eval(annotations), fromFastq)
	
        self.commitAndClose()
 
    def getRuns(self, runTypes):
        
        self.getConnection()
        
        runsInfo = []
        data = self.c.execute('SELECT * FROM runs').fetchall()
        for startTime, command, commandLine, finishedSuccessfully, masterPid in data:
            if command in runTypes: runsInfo.append([startTime, command, commandLine, finishedSuccessfully, masterPid])
        
        self.commitAndClose()
        
        return runsInfo

class Results(object,):
    
    def __init__(self, ):
        """ object holding the results of the analysis """
	
	self.defaultValues = {
	    'totalReadCount':None,
	    'uniqueBarcodeSequences':None,
	    'readPairsHasBarcode':None,
	    'readPairsAreIlluminaAdapters':None,
	    'barcodeClusterCount':None,
	    'singeltonBarcodeClusters':None,
	    'minR1readLength':None,
	    'minR2readLength':None
	}
	self.explenations = {
	    'totalReadCount':'The number of reads totally included in the analysis.',
	    'uniqueBarcodeSequences':'The total number of uniqu barcode sequences identified.',
	    'readPairsHasBarcode':'Total number of readpairs where a barcode could be identified.',
	    'readPairsAreIlluminaAdapters':'Total number of readpairs where a illumina adapter sequence could be identified.',
	    'barcodeClusterCount':'Total number of barcode clusters identified.',
	    'singeltonBarcodeClusters':'Total number of barcode clusters of only one read pair identified.',
	    'minR1readLength':'Minimum read length found in infiles',
	    'minR2readLength':'Minimum read length found in infiles'
	}
	self.isDefault = {}
	self.setTime = {}

	self.totalReadCount = None
	self.uniqueBarcodeSequences = None
	self.readPairsHasBarcode = None
	self.readPairsAreIlluminaAdapters = None
	self.barcodeClusterCount = None
	self.singeltonBarcodeClusters = None
	self.minR1readLength = None
	self.minR2readLength = None
	
	self.setDefaults()

    def setDefaults(self,):
	for resultName, value in self.defaultValues.iteritems():
	    self.__dict__[resultName] = value
	    self.isDefault[resultName] = True
	    self.setTime[resultName] = None
	return 0

    def loadFromDb(self,):
	
	#
	# Get the connection
	#
	SEAseqPipeLine.database.getConnection()
	
	#
	# Select data
	#
	data = SEAseqPipeLine.database.c.execute('SELECT resultName,defaultValue,value,setTime FROM results').fetchall()
	
	#
	# Parse data and add to object __dict__
	#
	if data:
	    for resultName,default,value,setTime in data:
		self.__dict__[resultName]  = value
		self.isDefault[resultName] = default
		self.setTime[resultName]   = setTime
	
	#
	# close connection
	#
	SEAseqPipeLine.database.commitAndClose()

    def setResult(self,resultName,value):
	import time
	assert resultName in self.explenations,'Error: you are trying to set a value for an undefined result.\n'
	self.__dict__[resultName]  = value
	self.isDefault[resultName] = False
	self.setTime[resultName]   = time.time()
	return 0

    def saveToDb(self,):
	
	#
	# imports
	#
	import time
	
	#
	# get connection
	#
	SEAseqPipeLine.database.getConnection()
	
        #
        # Look whats already in database, update it if older or default and set what is not
        #
	SEAseqPipeLine.logfile.write('checking results in db.\n')
        alreadyInDb = {}
	data = SEAseqPipeLine.database.c.execute('SELECT resultName,defaultValue,value,setTime FROM results').fetchall()
        if data:
            for resultName,default,value,setTime in data:
		SEAseqPipeLine.logfile.write('processing result '+resultName+'')
		alreadyInDb[resultName] = True
		
		if resultName in self.__dict__:
		    if default and not self.isDefault[resultName] or setTime < self.setTime[resultName]:
			if type(self.__dict__[resultName]) in [dict,list]: self.__dict__[resultName] = str(self.__dict__[resultName])
			SEAseqPipeLine.logfile.write(', updating from '+str(value)+' to '+str(self.__dict__[resultName])+', old_setTime '+str(setTime)+' new_setTime '+str(self.setTime[resultName])+'.\n')
			SEAseqPipeLine.database.c.execute('UPDATE results SET defaultValue=?, value=?, setTime=? WHERE resultName=?', (self.isDefault[resultName],self.__dict__[resultName],self.setTime[resultName],resultName))
		    else: SEAseqPipeLine.logfile.write(' no update needed.\n')
        
        #
        # Add new vars to database
        #
	SEAseqPipeLine.logfile.write('adding new results to db:\n')
        for resultName in self.__dict__:
	    if resultName in ['explenations','defaultValues','isDefault','setTime']:continue
	    if resultName not in alreadyInDb and not self.isDefault[resultName]:
		if type(self.__dict__[resultName]) in [dict,list]: self.__dict__[resultName] = str(self.__dict__[resultName])
		values = (resultName,self.isDefault[resultName],self.__dict__[resultName],self.setTime[resultName])
		SEAseqPipeLine.database.c.execute('INSERT INTO results VALUES (?,?,?,?)', values)
		SEAseqPipeLine.logfile.write('result '+resultName+' added to db with value '+str(self.__dict__[resultName])+'\n')
		#if self.isDefault[resultName]:SEAseqPipeLine.logfile.write(' this is the default value.\n')
		#else:SEAseqPipeLine.logfile.write(' non-default value.\n')
	    else: pass#SEAseqPipeLine.logfile.write('variable\t'+resultName+'\talready in db.\n')
        
	SEAseqPipeLine.logfile.write('commiting changes to database.\n')
        SEAseqPipeLine.database.commitAndClose()
        
        return 0

class Settings(object,):
    
    def __init__(self, ):
        """ object holding the settings used for each part of the analysis """
	
	self.defaultValues = {
	    'debug':False,
	    'uppmaxProject':'b2014005',
	    'parallelProcesses':16,
	    'mode':'RAC',
	    'handleSequence':'CTAAGTCCATCCGCACTCCT',
	    'maxHandleMissMatches':0,
	    'barcodeLength':15,
	    'analysisParts':None,
	    'barcodeMissmatch':0,
	    'readsPerUmiCutOff':5,
	    'umiMaxMisMatch':2,
	    'readsPerClusterCutOff':100

	}
	self.explenations = {
	    'debug':'Flag for running the scripts in multiprocessing or as single process run [True/False] (default=False)',
	    'uppmaxProject':'Project id used at uppmax for sbatch scripts [bXXXXXXX] (default=b2014005)',
	    'parallelProcesses':'Number of process to run when doing multiprocess parts of analysis (defaul=16)',
	    'mode':'Type of analysis either Whole Fragment Analysis (WFA) or rRNA Amplicon Classification (RAC) [WFA/RAC/iSeq] (default=RAC)',
	    'handleSequence':'The sequence that sepperate the barcode from the "specific" read sequence (default=CTAAGTCCATCCGCACTCCT)',
	    'barcodeLength':'The length of the bead barcode (default=15)',
	    'analysisParts':'Parts of the analysis to run specific for each run.',
	    'barcodeMissmatch':'Number of missmatches allowed in the barcode sequence',
	    'maxHandleMissMatches':'Number of missmatches allowed in the handle sequence',
	    'readsPerUmiCutOff':'Number of reads supporting one UMI for it to passs filters',
	    'umiMaxMisMatch':'Number of missmatches allowed in the UMI sequence',
	    'readsPerClusterCutOff':'Number of reads supporting a barcode sequence cluster for it to passs filters'
	}
	self.isDefault = {}
	self.setTime = {}

	self.debug = None
	self.uppmaxProject = None
	self.parallelProcesses = None
	self.mode = None
	self.handleSequence = None
	self.maxHandleMissMatches = None
	self.barcodeLength = None
	self.analysisParts = None
	self.barcodeMissmatch = None
	self.readsPerUmiCutOff = None
	self.umiMaxMisMatch = None
	self.readsPerClusterCutOff = None
	
	self.setDefaults()

    def setDefaults(self,):
	for variableName, value in self.defaultValues.iteritems():
	    self.__dict__[variableName] = value
	    self.isDefault[variableName] = True
	    self.setTime[variableName] = None
	return 0

    def loadFromDb(self,):
	
	#
	# Get the connection
	#
	SEAseqPipeLine.database.getConnection()
	
	#
	# Select data
	#
	data = SEAseqPipeLine.database.c.execute('SELECT variableName,defaultValue,value,setTime FROM settings').fetchall()
	
	#
	# Parse data and add to object __dict__
	#
	if data:
	    for variableName,default,value,setTime in data:
		self.__dict__[variableName]  = value
		self.isDefault[variableName] = default
		self.setTime[variableName]   = setTime
	
	#
	# close connection
	#
	SEAseqPipeLine.database.commitAndClose()

    def setVariable(self,variableName,value):
	import time
	assert variableName in self.explenations,'Error: you are trying to set an undefined variable.\n'
	self.__dict__[variableName]  = value
	self.isDefault[variableName] = False
	self.setTime[variableName]   = time.time()
	return 0

    def saveToDb(self,):
	
	#
	# imports
	#
	import time
	
	#
	# get connection
	#
	SEAseqPipeLine.database.getConnection()
	
        #
        # Look whats already in database, update it if older or default and set what is not
        #
	SEAseqPipeLine.logfile.write('checking whats in db.\n')
        alreadyInDb = {}
	data = SEAseqPipeLine.database.c.execute('SELECT variableName,defaultValue,value,setTime FROM settings').fetchall()
        if data:
            for variableName,default,value,setTime in data:
		SEAseqPipeLine.logfile.write('processing variable '+variableName+'')
		alreadyInDb[variableName] = True
		
		if variableName in self.__dict__:
		    if default and not self.isDefault[variableName] or setTime < self.setTime[variableName]:
			if type(self.__dict__[variableName]) in [dict,list]: self.__dict__[variableName] = str(self.__dict__[variableName])
			SEAseqPipeLine.logfile.write(', updating from '+str(value)+' to '+str(self.__dict__[variableName])+', old_setTime '+str(setTime)+' new_setTime '+str(self.setTime[variableName])+'.\n')
			SEAseqPipeLine.database.c.execute('UPDATE settings SET defaultValue=?, value=?, setTime=? WHERE variableName=?', (self.isDefault[variableName],self.__dict__[variableName],self.setTime[variableName],variableName))
		    else: SEAseqPipeLine.logfile.write(' no update needed.\n')
        
        #
        # Add new vars to database
        #
	SEAseqPipeLine.logfile.write('adding new vars to db:\n')
        for variableName in self.__dict__:
	    if variableName in ['explenations','defaultValues','isDefault','setTime']:continue
	    if variableName not in alreadyInDb:
		if type(self.__dict__[variableName]) in [dict,list]: self.__dict__[variableName] = str(self.__dict__[variableName])
		values = (variableName,self.isDefault[variableName],self.__dict__[variableName],self.setTime[variableName])
		SEAseqPipeLine.database.c.execute('INSERT INTO settings VALUES (?,?,?,?)', values)
		SEAseqPipeLine.logfile.write('variable '+variableName+' added to db with value '+str(self.__dict__[variableName])+',')
		if self.isDefault[variableName]:SEAseqPipeLine.logfile.write(' this is the default value.\n')
		else:SEAseqPipeLine.logfile.write(' non-default value.\n')
	    else: pass#SEAseqPipeLine.logfile.write('variable\t'+variableName+'\talready in db.\n')
        
	SEAseqPipeLine.logfile.write('commiting changes to database.\n')
        SEAseqPipeLine.database.commitAndClose()
        
        return 0

class FastqToDatabseConverter(object):
    """ reads fastq files (read one and two) and identifies the coordinates for the Barcode sequence etc and adds the data to the database """
    
    def __init__(self,):
	#
	# Setting initial values
	#
	self.filesAdded = {}
	self.readsToAdd = []
	self.currentLocation = 1
	self.appendChunkSize = 500000
        self.getFilenames()
        self.totalReadcount = 0
        self.currentRead = 0
	self.grandTotal = 0
	self.minR1ReadLength = 10000000000
	self.minR2ReadLength = 10000000000

	#
	# get the grand total readcount in all files to be added to the database
	#
	for filePairId, readcount, fastq1, fastq2 in self.infiles:
	    self.grandTotal += readcount
	SEAseqPipeLine.logfile.write('Going to add a self.grandTotal of '+str(self.grandTotal)+' reads to the database.\n')

	self.progress = Progress(self.grandTotal, logfile=SEAseqPipeLine.logfile, unit='reads-post-processed', mem=True)

    def getFilenames(self,):
        self.infiles = SEAseqPipeLine.database.getFastqs()

    def readPairGenerator(self,):

	#
	# imports
	#
	import gzip

        #
        # Loop through infiles
        #
	readfromdiskProgress = Progress(self.grandTotal, logfile=SEAseqPipeLine.logfile, unit='reads-read-from-disk', mem=True)
	with readfromdiskProgress:
	    for filePairId, readcount, fastq1, fastq2 in self.infiles:
		SEAseqPipeLine.logfile.write(str(self.currentRead)+' read pairs read from infiles, now starting to read from '+fastq1+'.\n')
		self.totalReadcount += readcount
		
		#
		# Open the files
		#
		if fastq1.split('.')[-1] in ['gz','gzip']: file1 = gzip.open(fastq1)
		else: file1 = open(fastq1,'r')
		if fastq2.split('.')[-1] in ['gz','gzip']: file2 = gzip.open(fastq2)
		else: file2 = open(fastq2,'r')
		
		while 'NOT EOFError':
		    try:
			header1 = file1.readline().rstrip()
			header2 = file2.readline().rstrip()
			sequence1 = file1.readline().rstrip()
			sequence2 = file2.readline().rstrip()
			trash = file1.readline().rstrip()
			trash = file2.readline().rstrip()
			qual1 = file1.readline().rstrip()
			qual2 = file2.readline().rstrip()
			if not header1: break
			self.currentRead += 1
			# data base has following info:
			#    (id,header,sequence1,sequence2,quality1,quality2,barcodeSequence,clusterId,annotation,fromFastq)
			barcodeSequence = None
			clusterId = None
			annotations = {}
			pair = ReadPair(self.currentRead, header1, header2, sequence1, sequence2, qual1, qual2,barcodeSequence,clusterId,annotations, filePairId)#fastq1)
			readfromdiskProgress.update()
			yield pair#self.currentRead, header1, header2, sequence1, sequence2, qual1, qual2, fastq1
		    except EOFError: break
		assert self.totalReadcount == self.currentRead, 'Error while reading infiles: Read count after file '+fastq1+' is '+str(self.currentRead)+' should theoretically be '+str(self.totalReadcount)+'.\n'
		SEAseqPipeLine.logfile.write('Reached the end of '+fastq1+'.\n')
	SEAseqPipeLine.logfile.write(str(self.grandTotal)+' read pairs read from infiles.\n')
	SEAseqPipeLine.results.setResult('totalReadCount',self.grandTotal)

    def foreachProcessedPair(self, pair):
	#
	# append to chunk
	#
	self.readsToAdd.append(pair.databaseTuple)

	#
	# check read pair file origin
	#
	if pair.fileOrigin not in self.filesAdded:
	    SEAseqPipeLine.logfile.write('Starting post process of read pair #'+str(pair.id)+' (from file '+str(pair.fileOrigin)+').\n')
	    self.filesAdded[pair.fileOrigin] = True

	#
	# add chunk to db
	#
	if len(self.readsToAdd) >= self.appendChunkSize: self.chunkToDb()
	
	#
	# check read length
	#
	self.minR1ReadLength = min([self.minR1ReadLength,len(pair.r1Seq)])
	self.minR2ReadLength = min([self.minR2ReadLength,len(pair.r2Seq)])
	
	self.progress.update()
	
	return 0

    def chunkToDb(self, ):
	#
	# Imports
	#
	import time

	#chunkStartTime = time.time()
	chunkStartTime = time.time()
	#SEAseqPipeLine.logfile.write('Appending reads '+str(self.currentLocation)+'-'+str(self.currentLocation+self.appendChunkSize-1)+' to db.\n')
	SEAseqPipeLine.logfile.write('Staring to append reads '+str(self.currentLocation)+'-'+str(self.currentLocation+len(self.readsToAdd)-1)+' to db.\n')

	#SEAseqPipeLine.database.addReads(self.readsToAdd)
	SEAseqPipeLine.database.addReads(self.readsToAdd)

	#chunkTime = time.time()-chunkStartTime
	chunkTime = time.time()-chunkStartTime

	#SEAseqPipeLine.logfile.write('Reads '+str(self.currentLocation)+'-'+str(self.currentLocation+self.appendChunkSize-1)+' done after '+str(int(round(chunkTime,0)/60))+' minutes and '+str(round(chunkTime,0)%60)+' seconds.\n')
	SEAseqPipeLine.logfile.write('Reads '+str(self.currentLocation)+'-'+str(self.currentLocation+len(self.readsToAdd)-1)+' appended to db after '+str(int(round(chunkTime,0)/60))+' minutes and '+str(round(chunkTime,0)%60)+' seconds.\n')

	self.currentLocation += len(self.readsToAdd) #self.appendChunkSize

	self.readsToAdd = []
	
	return 0

    def run(self,):
	
	#
	# Write log message
	#
	SEAseqPipeLine.logfile.write('Adding reads to database table.\n')
	
	#
	# Parse through files and add to database in chunks of "self.appendChunkSize" reads
	#
	with self.progress:
	    for pair in self.readPairGenerator():
		#
		# foreach read do this before adding to db, this part could be done in parallel
		#
		pair = foreachReadPairFromFastq(pair)
		
		#
		# post procesing for each read, preparing and adding to db
		#
		self.foreachProcessedPair(pair)


	#
	# add final chunk to db
	#
	if self.readsToAdd: self.chunkToDb()
	
	#
	# Done write to log and return
	#
	SEAseqPipeLine.logfile.write('Files '+', '.join([key for key in self.filesAdded.keys()])+' added sucesfully to the database.\n')
	
	return 0

    def runParallel(self,):
	
	#
	# imports
	#
	import multiprocessing
	
	#
	# drop old data and create table for new data
	#
	SEAseqPipeLine.logfile.write('Create reads table (and drop old one if needed) ...\n')
	SEAseqPipeLine.database.getConnection()
	SEAseqPipeLine.database.c.execute("DROP TABLE IF EXISTS reads")
	SEAseqPipeLine.database.c.execute('''CREATE TABLE reads (id,header,sequence1,sequence2,quality1,quality2,handleCoordinates,clusterId,annotation,fromFastq,PRIMARY KEY (id))''')
	SEAseqPipeLine.logfile.write('commiting changes to database.\n')
        SEAseqPipeLine.database.commitAndClose()
	
	#
	# Write log message
	#
	SEAseqPipeLine.logfile.write('Adding reads to database table (working in parallel).\n')
	
	poolOfProcesses = multiprocessing.Pool(SEAseqPipeLine.settings.parallelProcesses-1,maxtasksperchild=100000000)
	self.parallelResults = poolOfProcesses.imap_unordered(foreachReadPairFromFastq,self.readPairGenerator(),chunksize=10000)
	
	#
	# Parse through files and add to database in chunks of "self.appendChunkSize" reads
	#
	adapterCount = 0
	with self.progress:
	    for pair in self.parallelResults:	    
		#
		# post procesing for each read, preparing and adding to db
		#
		self.foreachProcessedPair(pair)
		if 'Read1IsIlluminaAdapter' in pair.annotations or 'Read2IsIlluminaAdapter' in pair.annotations: adapterCount += 1
	SEAseqPipeLine.results.setResult('readPairsAreIlluminaAdapters',adapterCount)

	#
	# add final chunk to db
	#
	if self.readsToAdd: self.chunkToDb()
	poolOfProcesses.close()
	poolOfProcesses.join()
	
	#
	# Done write to log and return
	#
	SEAseqPipeLine.logfile.write('Files '+', '.join([str(key) for key in self.filesAdded.keys()])+' added sucesfully to the database.\n')
	SEAseqPipeLine.logfile.write('Saving totalReadCount etc to results table in database.\n')
	SEAseqPipeLine.results.setResult('minR1readLength',self.minR1ReadLength)
	SEAseqPipeLine.results.setResult('minR2readLength',self.minR2ReadLength)
	SEAseqPipeLine.results.saveToDb()
	
	return 0

class SEAseqPipeLine(object):
    
    def __init__(self):
        """ Intitates the SEAseqPipeLine program run instance """
        
        #
        # imports
        #
        import time
        from socket import gethostname
        import commands
        
        #
        # Declare variables and set standard default values
        #
        SEAseqPipeLine.database = None
        self.analysisPath = None
        self.command = None
        self.commandLine = None
        self.commandLineList = None
        SEAseqPipeLine.settings = None
	SEAseqPipeLine.results = None
        SEAseqPipeLine.logfile = None
        SEAseqPipeLine.startTime = time.time()
        SEAseqPipeLine.startTimeStr = time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())
        self.availableCommands = {
            'initiateAnalysis':self.initiateAnalysis,
            'addData':self.addData,
            'changeSettings':self.changeSettings,
            'startAnalysis':self.startAnalysis,
            'commandLog':self.commandLog,
	    'listPairAnnotations':self.listPairAnnotations,
            'help':self.printHelp,
        }
        
        #
        # Get information from commandline
        #
        self.getComandAndPath()

        if gethostname().split('.')[1] == 'uppmax': self.onUppmax = True
        else: self.onUppmax = False
        
	tempFolderName = 'SEAseq2temporaryFiles'
        
	if self.onUppmax: SEAseqPipeLine.tempFileFolder = os.path.abspath(commands.getoutput('echo $SNIC_TMP'))+'/'+tempFolderName
        else:             SEAseqPipeLine.tempFileFolder = self.analysisPath+'/'+tempFolderName
        
	if not os.path.isdir(SEAseqPipeLine.tempFileFolder): os.makedirs(SEAseqPipeLine.tempFileFolder)

        self.doCurrentTask()
    
    def doCurrentTask(self,):
        """ identifies which and starts the part of the analysis pipeline the user wants the software perform """
        
        #
        # Imports
        #
        import sys
        
        #
        # Set the database path and create the settings object
        #
        SEAseqPipeLine.database = Database(self.analysisPath+'/dataSettingsAndResults.db')
	SEAseqPipeLine.settings = Settings()
	SEAseqPipeLine.results = Results()
	if self.command != 'initiateAnalysis':
	    SEAseqPipeLine.settings.loadFromDb()
	    SEAseqPipeLine.results.loadFromDb()

        #
        # call current command
        #
        try:
            self.availableCommands[self.command].__call__()
            #except AttributeError:
            #    print 'ERROR: the command "'+self.command+'" is not implemented yet try again in the future.'
            #    sys.exit(1)
        except KeyError:
            print 'ERROR: command "'+self.command+'" is not valid.\nAvialable commands are: '+', '.join(self.availableCommands.keys()[:-1])+' and '+self.availableCommands.keys()[-1]+'.\nUse: "SEAseq2 help" to get help\n'
            return 1
    
    def printHelp(self,):
        """ prints the help message """
        print man
        return 0
    
    def commandLog(self,):
        """ print all commands performed so far """
        
        #
        # get optional arguments from commandline
        #
        self.getComandLineOptions()

        #
        # Add run to runs table and open connection to logfile
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, False, MASTER)
        self.openLogfileConnection()
        SEAseqPipeLine.logfile.write(self.createLogHeader())
        
        # default all types of commands run
        runTypes = self.availableCommands.keys()
        
        SEAseqPipeLine.logfile.write('Writing commandLog to standard out.\n')
        print 'Getting runs performed with the following commands '+', '.join(runTypes[:-1])+' or '+runTypes[-1]+'.'
        print '# StartTime:                    \tFinished:\tCommand:'
        for startTime, command, commandLine, finishedSuccessfully, masterPid in SEAseqPipeLine.database.getRuns(runTypes):
            print str(startTime)+' \t'+str(bool(finishedSuccessfully))+'      \t'+str(commandLine)
        
        #
        # update runs table
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, True, MASTER)
        
        SEAseqPipeLine.logfile.write('Finished exiting.\n')

    def listPairAnnotations(self,):
        """ listPairAnnotations """
        
        #
        # get optional arguments from commandline
        #
        self.getComandLineOptions()

        #
        # Add run to runs table and open connection to logfile
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, False, MASTER)
        self.openLogfileConnection()
        SEAseqPipeLine.logfile.write(self.createLogHeader())
        
        # default all types of commands run
        runTypes = self.availableCommands.keys()
        
        SEAseqPipeLine.logfile.write('listing read pair annotations to standard out.\n')
        self.getReadPairAnnotations()
        
        #
        # update runs table
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, True, MASTER)
        
        SEAseqPipeLine.logfile.write('Finished exiting.\n')
    
    def initiateAnalysis(self,):
        """ sets the type of and path for the analysis """

        #
        # Imports
        #
        import os
        import sys

        #
        # get optional arguments from commandline
        #
        self.getComandLineOptions()
        
        #
        # for logmessages
        #
        tmpLogMessages = ['----------------\n']
        tmpLogMessage = self.createLogHeader()
        tmpLogMessages.append(tmpLogMessage)
        #print tmpLogMessage
        
        #
        # check analysis path
        #
        if os.path.isdir(self.analysisPath):
            tmpLogMessage = 'WARNING: the analysis path already exists.\n'
            print tmpLogMessage
            tmpLogMessages.append(tmpLogMessage)
        else:
            tmpLogMessage = 'Creating directory "'+self.analysisPath+'".\n'
            #print tmpLogMessage
            tmpLogMessages.append(tmpLogMessage)
            os.makedirs(self.analysisPath)
        
        #
        # create the logfile
        #
        tmpLogMessages += self.openLogfileConnection()
        
        #
        # write tmpLogMessages to logfile
        #
        SEAseqPipeLine.logfile.write(''.join(tmpLogMessages))
        
        #
        # create the database
        #
        SEAseqPipeLine.database.create()
        
	#
	# setting default settings
	#
	SEAseqPipeLine.settings.loadFromDb()
	SEAseqPipeLine.results.loadFromDb()
	SEAseqPipeLine.settings.setVariable('mode',self.mode)
	if self.mode == 'iSeq': SEAseqPipeLine.settings.setVariable('handleSequence','AATGGATACTAACTAGCACCACTATAGGC')
	SEAseqPipeLine.logfile.write('Saving settings to database.\n')
	SEAseqPipeLine.settings.saveToDb()
	
        #
        # add run to runs table
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, True, MASTER)
        
        return 0
    
    def addData(self,):
        
        #
        # get optional arguments from commandline
        #
        self.getComandLineOptions()

        #
        # Add run to runs table and open connection to logfile
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, False, MASTER)
        self.openLogfileConnection()
        SEAseqPipeLine.logfile.write(self.createLogHeader())

	#
	# Add the fastq files to the database list of infiles
	#
        SEAseqPipeLine.database.addFastqs(self.fastq1,self.fastq2)

        #
        # update runs table
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, True, MASTER)
        
        SEAseqPipeLine.logfile.write('Finished exiting.\n')
    
    def changeSettings(self,):
        print 'Under development ...'
	
	#
        # get optional arguments from commandline
        #
        self.getComandLineOptions()

        #
        # Add run to runs table and open connection to logfile
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, False, MASTER)
        self.openLogfileConnection()
        SEAseqPipeLine.logfile.write(self.createLogHeader())

        #
	# update new settings
	#
	SEAseqPipeLine.settings.saveToDb()
	
	#
        # update runs table
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, True, MASTER)
        
        SEAseqPipeLine.logfile.write('Finished exiting.\n')        

    def getReadPairAnnotations(self, ):
	
	uniqAnnotations = {}
	uniqAnnotationValues = {}
	
	SEAseqPipeLine.logfile.write('Loading read annotations ...\n')
	for pair in SEAseqPipeLine.database.getAllReadPairs():
	    for annotation, value in pair.annotations.iteritems():
		try:            uniqAnnotations[annotation] += 1
		except KeyError:uniqAnnotations[annotation] = 1
		try:            uniqAnnotationValues[annotation+'='+str(value)] += 1
		except KeyError:uniqAnnotationValues[annotation+'='+str(value)] = 1
	SEAseqPipeLine.logfile.write('Done.\n')
	
	print '\n\n#### uniq annotations:'
	for annotation,count in uniqAnnotations.iteritems():
	    print annotation+'\t'+str(count)

	print '\n\n#### uniq annotationValues:'
	for annotation,count in uniqAnnotationValues.iteritems():
	    print annotation+'\t'+str(count)

    def generateBarcodeFastq(self, ):
	
	#
	# imports
	#
	import operator
	
	SEAseqPipeLine.logfile.write('Generating barcode fastq ...\n')
	
	uniqBarcodeSequences = {}
	temporaryDict = {}
	barcodeCounter = 0
	totalReadPairCounter = 0
	qualities = {}
	readPairHasBarcodeCounter = 0
	
	SEAseqPipeLine.logfile.write('Loading read pairs ...\n')
	progress = Progress(SEAseqPipeLine.results.totalReadCount, logfile=SEAseqPipeLine.logfile, unit='reads-loaded-from-db', mem=True)
	with progress:
	    for pair in SEAseqPipeLine.database.getAllReadPairs():
		if pair.handleCoordinates:
		    readPairHasBarcodeCounter += 1
		    barcodeSequence = pair.r1Seq[0:pair.handleCoordinates[0]]
		    qualities[pair.id] = pair.r1Qual[0:pair.handleCoordinates[0]]
		    try:            uniqBarcodeSequences[barcodeSequence].append(pair.id)
		    except KeyError:uniqBarcodeSequences[barcodeSequence] = [pair.id]
		progress.update()
	SEAseqPipeLine.logfile.write('Done.\n')
	SEAseqPipeLine.results.setResult('uniqueBarcodeSequences',len(uniqBarcodeSequences))
	print SEAseqPipeLine.results.uniqueBarcodeSequences,'uniq barcode sequences found within the read pair population.'

	SEAseqPipeLine.logfile.write('Sorting the barcodes by number of reads/sequence.\n')
	SEAseqPipeLine.logfile.write('Building the sorting dictionary ...\n')
	for barcode, idList in uniqBarcodeSequences.iteritems():
		try:		temporaryDict[len(idList)].append(barcode)
		except KeyError:temporaryDict[len(idList)] = [barcode]
	
	
	SEAseqPipeLine.logfile.write('Creating output ... \n')
	barcodeFastqFile = open(self.analysisPath+'/rawBarcodeSequencesSortedByAbundance.fq','w')
	progress = Progress(readPairHasBarcodeCounter, logfile=SEAseqPipeLine.logfile, unit='reads-to-fastq', mem=True)
	with progress:
	    for count, barcodes in sorted(temporaryDict.iteritems(), key=operator.itemgetter(0))[::-1]:
		for barcode in barcodes:
		    barcodeCounter += 1
		    readPairCounter = 0
		    for readPairId in uniqBarcodeSequences[barcode]:
			readPairCounter += 1
			totalReadPairCounter += 1
			barcodeFastqFile.write('@'+str(readPairId)+' bc='+str(barcodeCounter)+' rp='+str(readPairCounter)+' bctrp='+str(count)+'\n'+barcode+'\n+\n'+qualities[readPairId]+'\n')
			progress.update()
	barcodeFastqFile.close()	
	
	return readPairHasBarcodeCounter
    
    def runBarcodeClusteringPrograms(self, ):

	#
	# imports
	#
	import subprocess

	clusteringProgramsLogfile = open(self.analysisPath+'/cdHitBarcodeClustering.log.txt','w')
	
	# cluster raw barcodes
	command = ['cd-hit-454',
		    '-i',self.analysisPath+'/rawBarcodeSequencesSortedByAbundance.fq',
		    '-o',self.analysisPath+'/clusteredBarcodeSequences',
		    '-g','1',      # mode
		    '-n','3',      # wrodsize
		    '-M','0',      # memory limit
		    '-t',str(SEAseqPipeLine.settings.parallelProcesses),      # threads
		    '-gap','-100', # disallow gaps
		    '-c',str(1.000-float(SEAseqPipeLine.settings.barcodeMissmatch)/float(SEAseqPipeLine.settings.barcodeLength)) # identity cutoff
		    ]
	SEAseqPipeLine.logfile.write('Starting command: '+' '.join(command)+'\n')
	cdhit = subprocess.Popen(command,stdout=clusteringProgramsLogfile,stderr=subprocess.PIPE )
	errdata = cdhit.communicate()
	if cdhit.returncode != 0:
		print 'cmd: '+' '.join( command )
		print 'cd-hit view Error code', cdhit.returncode, errdata
		sys.exit()
	
	# Build consensus sequences for read pair clusters
	command = ['cdhit-cluster-consensus',
		self.analysisPath+'/clusteredBarcodeSequences.clstr',
		self.analysisPath+'/rawBarcodeSequencesSortedByAbundance.fq',
		self.analysisPath+'/clusteredBarcodeSequences.consensus',
		self.analysisPath+'/clusteredBarcodeSequences.aligned'
		]
	SEAseqPipeLine.logfile.write('Starting command: '+' '.join(command)+'\n')
	ccc = subprocess.Popen(command,stdout=clusteringProgramsLogfile,stderr=subprocess.PIPE )
	errdata = ccc.communicate()
	if ccc.returncode != 0:
		print 'cmd: '+' '.join( command )
		print 'cdhit-cluster-consensus view Error code', ccc.returncode, errdata
		sys.exit()
	clusteringProgramsLogfile.close()
	
	return 0

    def parseBarcodeClusteringOutput(self, readPairsHasBarcode):
	
	#
	# inititate variables
	#
	totalClusterCount = 0
	barcodeClusters = {}
	singletonClusters = {}
	nonSingletonClusters = {}
	
	#
	# open file connections
	#
	consensusFile = open(self.analysisPath+'/clusteredBarcodeSequences.consensus.fastq')
	clstrFile = open(self.analysisPath+'/clusteredBarcodeSequences.clstr')
	
	#
	# load cluster ids and consensus sequences
	#
	SEAseqPipeLine.logfile.write('\nLoading barcode clusters and a barcode consesnsus sequences for each cluster ...\n')
	while True:
	    header = consensusFile.readline().rstrip()
	    barcodeSequence = consensusFile.readline().rstrip()
	    junk = consensusFile.readline().rstrip()
	    barcodeQuality = consensusFile.readline().rstrip()
	    if header == '': break
	    totalClusterCount += 1
	    header = header.split('_cluster_')
	    clusterId = int(header[1].split(' ')[0])
	    if header[0][:2] == '@s':
		singletonClusters[clusterId] = {'clusterReadCount':1,'readPairs':[],'identities':[],'clusterBarcodeSequence':barcodeSequence,'clusterBarcodeQuality':barcodeQuality}
		barcodeClusters[clusterId] = singletonClusters[clusterId]
	    elif header[0][:2] == '@c':
		nonSingletonClusters[clusterId] = {'clusterReadCount':int(header[1].split(' ')[2]),'readPairs':[],'identities':[],'clusterBarcodeSequence':barcodeSequence,'clusterBarcodeQuality':barcodeQuality}
		barcodeClusters[clusterId] = nonSingletonClusters[clusterId]
	    else: raise ValueError
	SEAseqPipeLine.results.setResult('barcodeClusterCount',totalClusterCount)
	SEAseqPipeLine.results.setResult('singeltonBarcodeClusters',len(singletonClusters))
	SEAseqPipeLine.logfile.write('A total of '+str(totalClusterCount)+' clusters of barcode sequences were loaded into memory.\n')

	#
	# Load what readpairs are in each cluster
	#
	SEAseqPipeLine.logfile.write('\nLoading read pair to barcode cluster connections ...\n')
	progress = Progress(readPairsHasBarcode, logfile=SEAseqPipeLine.logfile, unit='reads-loaded', mem=True)
	with progress:
	    for line in clstrFile:
		line = line.rstrip()
		if line[0] == '>':
		    clusterId = int(line.split(' ')[1])
		    continue
		elif line[0] == '0':
		    readId = line.split('>')[1].split('.')[0]
		    identity = 'seed'
		    assert line.split(' ')[-1] == '*', 'Error in file format of clstr file'
		else:
		    readId = line.split('>')[1].split('.')[0]
		    identity = float(line.split(' ')[-1].split('/')[-1].split('%')[0])
		barcodeClusters[clusterId]['readPairs'].append(readId)
		barcodeClusters[clusterId]['identities'].append(identity)
		progress.update()
	SEAseqPipeLine.logfile.write('All read pair to barcode cluster connections loaded.\n')
	
	return barcodeClusters

    def addBarcodeClusterInfoToDatabase(self, barcodeClusters):

	#
	# set initial values
	#
	tmpUpdateValues = {}
	updateValues = []
	updateChunks = []
	updateChunkSize = 10000
	addValues = [(None,0,'[]','[]','NoneCluster',None,None,None)]
	
	#
	# drop old data and create table for new data
	#
	SEAseqPipeLine.logfile.write('Create barcodeClusters table (and drop old one if needed) ...\n')
	SEAseqPipeLine.database.getConnection()
	SEAseqPipeLine.database.c.execute("DROP TABLE IF EXISTS barcodeClusters")
	SEAseqPipeLine.database.c.execute('''CREATE TABLE barcodeClusters (clusterId,clusterTotalReadCount,readPairsList,readBarcodeIdentitiesList,clusterBarcodeSequence,clusterBarcodeQuality,contigSequencesList,annotations,PRIMARY KEY (clusterId))''')
	SEAseqPipeLine.logfile.write('commiting changes to database.\n')
        SEAseqPipeLine.database.commitAndClose()
	
	#
	# Convert the dictionary to be able to add info to database
	#
	SEAseqPipeLine.logfile.write('Converting the data ... \n')
	for clusterId,data in barcodeClusters.iteritems():
	    for readPairId in data['readPairs']: tmpUpdateValues[readPairId]=clusterId
	    addValues.append( (clusterId,data['clusterReadCount'],str(data['readPairs']),str(data['identities']),data['clusterBarcodeSequence'],data['clusterBarcodeQuality'],None,None) )
	for readPairId in sorted(tmpUpdateValues.keys()):
	    updateValues.append( (int(tmpUpdateValues[readPairId]),int(readPairId)) )
	    if len(updateValues) == updateChunkSize:
		updateChunks.append(updateValues)
		updateValues = []
	updateChunks.append(updateValues)

	#
	# Add the barcodeClusters
	#	
	SEAseqPipeLine.logfile.write('Adding cluster info to database ... \n')
	SEAseqPipeLine.database.getConnection()	
	SEAseqPipeLine.database.c.executemany('INSERT INTO barcodeClusters VALUES (?,?,?,?,?,?,?,?)', addValues)
	SEAseqPipeLine.logfile.write('commiting changes to database.\n')
        SEAseqPipeLine.database.commitAndClose()

	#
	# Update the reads table
	#
	SEAseqPipeLine.logfile.write('Updating read pair info in the database ... \n')
	progress = Progress(len(tmpUpdateValues), logfile=SEAseqPipeLine.logfile, unit='reads-updated', mem=True)
	with progress:
	    for updateValues in updateChunks:
		SEAseqPipeLine.database.getConnection()	
		SEAseqPipeLine.database.c.executemany('UPDATE reads SET clusterId=? WHERE id=?', updateValues)
		SEAseqPipeLine.database.commitAndClose()
		for i in xrange(updateChunkSize): progress.update()
	    
	return 0
    
    def clusterBarcodeSequences(self):
	
	#
	# imports
	#
	import subprocess
	
	#
	# generate a fastq file with barcode sequences
	#
	readPairsHasBarcode = self.generateBarcodeFastq()
	SEAseqPipeLine.results.setResult('readPairsHasBarcode',readPairsHasBarcode)
	
	#
	# Run clustering programs
	#
	self.runBarcodeClusteringPrograms()
	
	#
	# Parse the output
	#
	barcodeClusters = self.parseBarcodeClusteringOutput(readPairsHasBarcode)
	
	#
	# compress files
	#
	a = [
		self.analysisPath+'/clusteredBarcodeSequences',
		self.analysisPath+'/clusteredBarcodeSequences.clstr',
		self.analysisPath+'/rawBarcodeSequencesSortedByAbundance.fq',
		self.analysisPath+'/clusteredBarcodeSequences.consensus.fastq',
		self.analysisPath+'/clusteredBarcodeSequences.aligned',
		]
	processes = [] 
	for fileName in a:
	    SEAseqPipeLine.logfile.write('Starting process:'+' gzip -v9 '+fileName+'\n')
	    p=subprocess.Popen(['gzip','-v9',fileName],stdout=sys.stdout)
	    processes.append(p)
	
	#
	# save clustering info to database
	#
	self.addBarcodeClusterInfoToDatabase(barcodeClusters)
	
	#
	# Save results
	#
	SEAseqPipeLine.results.saveToDb()

	#
	# wait for file compression to finish
	#
	SEAseqPipeLine.logfile.write('Waiting for the compression of clustering files to finish.\n')
	for p in processes: p.wait()
	
	return 0

    def getBarcodeClusterIds(self, shuffle=True,byMixedClusterReadCount=True):
	
	import random
	
	SEAseqPipeLine.database.getConnection()
	clusterIds = SEAseqPipeLine.database.c.execute('SELECT clusterId FROM barcodeClusters').fetchall()
	if shuffle: random.shuffle(clusterIds)
	SEAseqPipeLine.database.commitAndClose()
	
	if byMixedClusterReadCount:
	    
	    id2ReadCountDist = {}
	    yielded = {}
	    normal = []
	    large = []
	    huge = []
	    humongous = []
	    outofthescale = []
	    
	    SEAseqPipeLine.logfile.write('Sorting cluster ids to groups of varying read pair counts ...\n')
	    for clusterId in clusterIds:
		if clusterId[0] == None: continue
		clusterId = int(clusterId[0])
		cluster = BarcodeCluster(clusterId)
		cluster.loadClusterInfo()
		id2ReadCountDist[cluster.id] = cluster.readPairCount
		yielded[cluster.id] = False
		if   cluster.readPairCount <  1000 and cluster.readPairCount >= 0: normal.append(cluster.id)
		elif cluster.readPairCount <  5000 and cluster.readPairCount >= 1000: large.append(cluster.id)
		elif cluster.readPairCount < 35000 and cluster.readPairCount >= 5000: huge.append(cluster.id)
		elif cluster.readPairCount <500000 and cluster.readPairCount >= 35000: humongous.append(cluster.id)
		else: outofthescale.append(cluster.id)
	    SEAseqPipeLine.logfile.write('Done.\n')
	    
	    tmpCounter0 = 0
	    yieldNow = None
	    while False in yielded.values():
		if outofthescale and tmpCounter0 <= 0:
		    yieldNow = outofthescale[0]
		    outofthescale = outofthescale[1:]
		    yielded[yieldNow] = True
		    tmpCounter0 += 1
		    yield yieldNow
		elif humongous and tmpCounter0 <= 0:
		    yieldNow = humongous[0]
		    humongous = humongous[1:]
		    yielded[yieldNow] = True
		    tmpCounter0 += 1
		    yield yieldNow
		elif huge and tmpCounter0 <= 0:
		    yieldNow = huge[0]
		    huge = huge[1:]
		    yielded[yieldNow] = True
		    tmpCounter0 += 1
		    yield yieldNow
		elif large and tmpCounter0 <= 1:
		    yieldNow = large[0]
		    large = large[1:]
		    yielded[yieldNow] = True
		    tmpCounter0 += 1
		    yield yieldNow
		elif normal:
		    yieldNow = normal[0]
		    normal = normal[1:]
		    yielded[yieldNow] = True
		    tmpCounter0 += 1
		    yield yieldNow
		#SEAseqPipeLine.logfile.write( str(tmpCounter0)+'\t')
		#SEAseqPipeLine.logfile.write( str(yieldNow)+'\t')
		#SEAseqPipeLine.logfile.write( str(id2ReadCountDist[yieldNow])+'.\n')
		if tmpCounter0 == 6: tmpCounter0 = 0
		#if yielded.values().count(True) >= 100: break
	else:
	    for clusterId in clusterIds:
		if clusterId[0] == None: continue
		try: yield int(clusterId[0])
		except TypeError: yield clusterId[0]

    def findContigs(self, ):
	print 'under heavy development'
	
	#
	# drop old data and create table for new data
	#
	SEAseqPipeLine.logfile.write('Create contigs table (and drop old one if needed) ...\n')
	SEAseqPipeLine.database.getConnection()
	SEAseqPipeLine.database.c.execute("DROP TABLE IF EXISTS contigs")
	SEAseqPipeLine.database.c.execute('''CREATE TABLE contigs (contigId,contigTotalReadCount,readPairsList,readPairIdentitiesList,consensusSequence,consensusQuality,clusterId,annotations,PRIMARY KEY (contigId,clusterId))''')
	SEAseqPipeLine.logfile.write('commiting changes to database.\n')
        SEAseqPipeLine.database.commitAndClose()
	
	# initiate parallel processing of clusters
	import multiprocessing
	poolOfProcesses = multiprocessing.Pool(SEAseqPipeLine.settings.parallelProcesses,maxtasksperchild=100000000)
	results = poolOfProcesses.imap_unordered(foreachFindContigsClusters,self.getBarcodeClusterIds(),chunksize=5)
	
	progress = Progress(SEAseqPipeLine.results.barcodeClusterCount,logfile=SEAseqPipeLine.logfile,unit='clusters',mem=True)
	doSome =0
	with progress:
	#    for clusterId in self.getBarcodeClusterIds():
	#	progress.update()
	#	if clusterId:
	#	    cluster = foreachFindContigsClusters(clusterId)
	#	    doSome +=1
	#	    #if doSome > 100:break
	    for cluster in results:
		if cluster:
		    cluster.saveContigInfoToDatabase()
		    progress.update()
	
	return 0

    def findiSeqBarcodes(self, ):
	
	#
	# imports
	#
	import multiprocessing
	
	#
	# Setup worker pool
	#
	poolOfProcesses = multiprocessing.Pool(SEAseqPipeLine.settings.parallelProcesses-1,maxtasksperchild=100000000)
	results = poolOfProcesses.imap_unordered(foreachiSeqCluster,self.getBarcodeClusterIds(),chunksize=5)
	
	#
	# initiate outfiles
	#
	outfile = open(self.analysisPath+'/iSeqPerBarcodeSequenceCluster.out.txt','w')
	outfile2 = open(self.analysisPath+'/iSeqBarcodeCombos.out.txt','w')
	outfile3 = open(self.analysisPath+'/iSeqReadPairsPerBarcodeSequenceCluster.out.txt','w')
	outfile4 = open(self.analysisPath+'/iSeqReadPairsPerUMI.out.txt','w')
	outfile5 = open(self.analysisPath+'/iSeqUMIsPerAb.MonoAbClustersOnly.out.txt','w')

	#
	# initiate counters etc
	#
	barcodeCombos = {}
	iSeqBarcodeAnnotations = {}
	readPairsPerBarcodeSequenceCluster = {}
	
	#
	# process the results from the multiprocess part
	#
	progress = Progress(SEAseqPipeLine.results.barcodeClusterCount, logfile=SEAseqPipeLine.logfile, unit='barcodeSequenceClusters', mem=True)
	with progress:
	    for tmp in results:
		flag,cluster = tmp

		#write cluster speceific ooutput
		outfile.write(flag+'\n')
		outfile.write(cluster.iSeqOutString+'\n')
		
		if flag == 'OK': # if flag ok get barcode combo
		    combo = ', '.join(cluster.iSeqBarcodesPassingFilter)
		    if len(cluster.iSeqBarcodesPassingFilter) == 0: combo = 'NonePF'
		    outfile.write( 'passing filter => '+ combo +'\n') 
		    
		    try: barcodeCombos[combo]+=1
		    except KeyError: barcodeCombos[combo]=1
		    
		    for umiReadCount in cluster.readsPerRawUmiList: outfile4.write(str(umiReadCount)+'\n')
		    
		    if len(cluster.iSeqBarcodesPassingFilter) == 1: outfile5.write(combo + '\t' + str(len(cluster.iSeqBarcodesUMI[combo])) +'\n')
		
		# get annotation for each read pair
		for iSeqBarcode,readPairsList in cluster.iSeqBarcodes.iteritems():
		    total = len(readPairsList)
		    try: iSeqBarcodeAnnotations[iSeqBarcode] += total
		    except KeyError: iSeqBarcodeAnnotations[iSeqBarcode] = total
		
		# save the number of reads for each barcodeSequenceCluster
		try: readPairsPerBarcodeSequenceCluster[cluster.readPairCount] += 1
		except KeyError: readPairsPerBarcodeSequenceCluster[cluster.readPairCount] = 1
		
		outfile.write('-----\n'+'\n')

		progress.update()
		
	#
	# Write summaries to outfiles
	#
	outfile2.write( 'iSeqBarcodeCombo\t#barcodeSequenceClusters\n')
	for combo, count in barcodeCombos.iteritems():
	    outfile2.write( combo+'\t'+str(count) +'\n')
	outfile2.write('\n')
	
	outfile2.write( 'iSeqBarcodeAnnotation\t#readPairs\n')
	for combo, count in iSeqBarcodeAnnotations.iteritems():
	    outfile2.write( combo+'\t'+str(count) +'\n')
	
	outfile3.write( 'ReadPairsPerCluster\t#BarcodeSeuenceClusters\n')
	for perCluster, numberOfClusters in readPairsPerBarcodeSequenceCluster.iteritems():
	    outfile3.write( str(perCluster)+'\t'+str(numberOfClusters) +'\n')
	
	#
	# close outfiles
	#
	outfile.write('\n')
	outfile.close()
	outfile2.write('\n')
	outfile2.close()
	outfile3.write('\n')
	outfile3.close()
	outfile4.write('\n')
	outfile4.close()
	outfile5.write('\n')
	outfile5.close()
	
	return 0
    
    def readFastqs(self,):
	#
	# Add the reads from fastq files to the database reads table
	#
	converter = FastqToDatabseConverter()
	#converter.run()
	converter.runParallel()
	SEAseqPipeLine.logfile.write('Saving settings to database.\n')
	SEAseqPipeLine.settings.saveToDb()
	SEAseqPipeLine.logfile.write('readFastqs finished.\n')

    def startAnalysis(self,):

	#
	# Imports
	#
	import time

	#
        # get optional arguments from commandline
        #
        self.getComandLineOptions()

        #
        # Add run to runs table and open connection to logfile
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, False, MASTER)
        self.openLogfileConnection()
        SEAseqPipeLine.logfile.write(self.createLogHeader())

	#
	# check parts to run and start them
	#
	for part in SEAseqPipeLine.settings.analysisParts: assert part in ['all','readFastqs','clusterBarcodeSequences','findContigs','findiSeqBarcodes'], 'Error: part '+part+' is not a valid option.\n'
	
	if 'all' in SEAseqPipeLine.settings.analysisParts or 'readFastqs' in SEAseqPipeLine.settings.analysisParts:
	    readFastqsStartTime = time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())
	    self.readFastqs()
	    
	if 'all' in SEAseqPipeLine.settings.analysisParts or 'clusterBarcodeSequences' in SEAseqPipeLine.settings.analysisParts:
	    clusterBarcodeSequencesStartTime = time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())
	    self.clusterBarcodeSequences()

	if 'all' in SEAseqPipeLine.settings.analysisParts or 'findContigs' in SEAseqPipeLine.settings.analysisParts:
	    clusterBarcodeSequencesStartTime = time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())
	    self.findContigs()

	if 'all' in SEAseqPipeLine.settings.analysisParts or 'findiSeqBarcodes' in SEAseqPipeLine.settings.analysisParts:
	    clusterBarcodeSequencesStartTime = time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())
	    self.findiSeqBarcodes()

        #
        # update runs table
        #
        SEAseqPipeLine.database.addToRunsTable(self.startTimeStr, self.command, self.commandLine, True, MASTER)
        
        SEAseqPipeLine.logfile.write('Finished exiting.\n')        

    def openLogfileConnection(self,):
        """ open a connection to the logfile, creates a logfile if none is present """
        
        #
        # Imports
        #
        import sys
        import time
        import os
        
        #
        # for logmessages
        #        
        tmpLogMessages = []
        
        #
        # check if logfile present open connection or create
        #
        SEAseqPipeLine.logfile = self.analysisPath + '/logfile.txt'
        if os.path.isfile(SEAseqPipeLine.logfile):
            if self.command == 'initiateAnalysis':
                print 'ERROR: the logfile already exists please use another path to initiate the analysis.\n'
                sys.exit(1)
            else:
                SEAseqPipeLine.logfile = open(SEAseqPipeLine.logfile,'a',1)
                SEAseqPipeLine.logfile.write('----------------\nConnection to logfile '+SEAseqPipeLine.logfile.name+' opened.\n')
                return 0
        else:
            tmpLogMessage = 'Creating the logfile "'+SEAseqPipeLine.logfile+'".\n'
            tmpLogMessages.append(tmpLogMessage)
            print tmpLogMessage
            SEAseqPipeLine.logfile = open(SEAseqPipeLine.logfile,'w',1)
        
        return tmpLogMessages

    def getComandAndPath(self,):
        """ get the current command """

        #
        # Import packages
        #
        import re
        import sys
        import os

        #
        # get the actual info from input
        #
        self.commandLineList = sys.argv
        self.commandLine = ' '.join(sys.argv)
        try: self.command = self.commandLineList[1]
        except IndexError:
            print 'ERROR: Please supply a command.\nUse: "SEAseq2 help" to get help\n'
            sys.exit(1)

        #
        # check program name
        #
        assert self.commandLineList[0].split('/')[-1] == 'SEAseq2', 'ERROR: program name is SEAseq2 please do not rename the file.\n'

        #
        # look for help request
        #
        if re.search('^-{0,2}[hH][eE]?[lL]?[pP]?$',self.command):
            print man
            sys.exit(0)

        #
        # get the analysis path
        #
        try:
            self.analysisPath = os.path.abspath(sys.argv[2])
        except IndexError:
            print '\nERROR: please supply a path for the analysis.\nUse: "SEAseq2 help" to get help\n'
            sys.exit(1)

        return 0
    
    def getComandLineOptions(self,):
        """ function that gets the indata from the commandline """

        import argparse
        import os
        import sys
        import re
        
        indata = None
        
        #if re.search('(\ -h\ |$)|(\ --help\ |$)',self.commandLine): print man
        
        # commandLine arguments parsing
        if self.command == 'initiateAnalysis': prog = 'SEAseq2 initiateAnalysis <path> <type>'
        if self.command == 'commandLog': prog = 'SEAseq2 commandLog <path>'
	if self.command == 'listPairAnnotations': prog = 'SEAseq2 commandLog <path>'
        if self.command == 'addData': prog = 'SEAseq2 addData <path>'
	if self.command == 'changeSettings': prog = 'SEAseq2 changeSettings <path>'
	if self.command == 'startAnalysis': prog = 'SEAseq2 startAnalysis <path> <part>'
        argparser = argparse.ArgumentParser(prog=prog, description='', epilog='Use: "SEAseq2 help" to get more detailed help.', formatter_class=argparse.RawTextHelpFormatter)
    
        # All programs
        argparser.add_argument('--debug', dest='debug', action='store_true', required=False, help='Run the program in debug-mode, single process python script (SLOW).')
        argparser.add_argument(	'-p', dest='cpus', metavar='N',	type=int, required=False,help='The number of processes to run in parallel (default 1, current value='+str(SEAseqPipeLine.settings.parallelProcesses)+').')

        if self.command == 'commandLog' or self.command == 'listPairAnnotations':
            try: indata = argparser.parse_args(self.commandLineList[3:])
            except IndexError: pass
            
        if self.command == 'initiateAnalysis':
            try: self.mode = self.commandLineList[3]
            except IndexError:
                print 'ERROR: no analysis mode supplied.'
                sys.exit(1)
            try: indata = argparser.parse_args(self.commandLineList[4:])
            except IndexError: pass

        if self.command == 'changeSettings':
	    argparser.add_argument('-bm', dest='barcodeMissmatch', metavar='N',	type=int, required=False, help='The number of missmatches allowed in the bead barcode sequence (default 0, current value='+str(SEAseqPipeLine.settings.barcodeMissmatch)+').')
	    argparser.add_argument('-hm', dest='maxHandleMissMatches', metavar='N',	type=int, required=False, help='The number of missmatches allowed in the handle sequence (default 0, current value='+str(SEAseqPipeLine.settings.maxHandleMissMatches)+').')
	    argparser.add_argument('-rpc', dest='readsPerClusterCutOff', metavar='N',	type=int, required=False, help='TThe number of reads required for a barcode sequence cluster to pass filter (default 100, current value='+str(SEAseqPipeLine.settings.readsPerClusterCutOff)+', so far only used by iSeq).')
	    argparser.add_argument('-rpu', dest='readsPerUmiCutOff', metavar='N',	type=int, required=False, help='The number of reads required for UMI to pass filter (default 5, current value='+str(SEAseqPipeLine.settings.readsPerUmiCutOff)+', so far only used by iSeq).')
	    argparser.add_argument('-umm', dest='umiMaxMisMatch', metavar='N',	type=int, required=False, help='The number of missmatches allowed in the UMI sequence (default 2, current value='+str(SEAseqPipeLine.settings.umiMaxMisMatch)+', so far only used by iSeq).')
	    try:
                indata = argparser.parse_args(self.commandLineList[3:])
		if indata.barcodeMissmatch != None:     SEAseqPipeLine.settings.setVariable('barcodeMissmatch',indata.barcodeMissmatch)
		if indata.maxHandleMissMatches != None: SEAseqPipeLine.settings.setVariable('maxHandleMissMatches',indata.maxHandleMissMatches)
		if indata.readsPerClusterCutOff != None:SEAseqPipeLine.settings.setVariable('readsPerClusterCutOff',indata.readsPerClusterCutOff)
		if indata.readsPerUmiCutOff != None:    SEAseqPipeLine.settings.setVariable('readsPerUmiCutOff',indata.readsPerUmiCutOff)
		if indata.umiMaxMisMatch != None:       SEAseqPipeLine.settings.setVariable('umiMaxMisMatch',indata.umiMaxMisMatch)
	    except IndexError: pass
    
        if self.command == 'startAnalysis':
	    argparser.add_argument('parts', metavar='<part>', type=str, nargs='+', help='parts of the analysis to perform')
            if self.onUppmax:
                argparser.add_argument('-prj','-project',dest='project',metavar='<b20xxxxx>',	type=str,	required=False,	help='uppmaxproject (default b2011011, current value='+str(SEAseqPipeLine.settings.uppmaxProject)+')')
                #argparser.add_argument('--send',	dest='send', 	action='store_true', 			required=False,	default=False,	help='Send sbatch scripts to job-queue.')
                #argparser.add_argument('--sendonly',	dest='sendonly',action='store_true', 			required=False,	default=False,	help='Do not generate the files only Send sbatch scripts to job-queue.')
                #argparser.add_argument('--small',	dest='small', 	action='store_true', 			required=False,	default=False,	help='make for smaller dataset job-queue.')
            try:
                indata = argparser.parse_args(self.commandLineList[3:])
                if indata.project != None: SEAseqPipeLine.settings.setVariable('uppmaxProject',indata.project)
		SEAseqPipeLine.settings.setVariable('analysisParts',indata.parts)
            except IndexError: pass
            
        if self.command == 'addData':
            argparser.add_argument('-r1',dest='fastq1',	metavar='FILE',type=file,required=True, help='Indata "fastq"-file read1.')
            argparser.add_argument('-r2',dest='fastq2',	metavar='FILE',type=file,required=True,	help='Indata "fastq"-file read2.')
            try:
                indata = argparser.parse_args(self.commandLineList[3:])
                self.fastq1 = os.path.abspath(indata.fastq1.name)
                self.fastq2 = os.path.abspath(indata.fastq2.name)
            except IndexError: pass
            
        if indata.debug != None: SEAseqPipeLine.settings.setVariable('debug',indata.debug)
        if indata.cpus != None:  SEAseqPipeLine.settings.setVariable('parallelProcesses',indata.cpus)

    def createLogHeader(self,):
        """ creates the header lines for each new logfile entry """
        
        #
        # Imports
        #
        import sys
        import getpass
        import commands
        from socket import gethostname
        
        #
        # get information
        #
        username = getpass.getuser()
        computer = gethostname()
        
        #
        # create the header
        #
        output = ''
        output += 'Running program: '+self.commandLine+'.\n'
        output += 'time: '+self.startTimeStr+'\n'
        output += 'Master process id='+str(MASTER)+'\n'
        output += 'Started by user = '+username+' on host = '+computer+'\n'
        if self.onUppmax: output += 'Program is run on uppmax, any temporary files will be placed in '+commands.getoutput('echo $SNIC_TMP')+' .\n'
        
        return output

class Progress():

	def __init__(self,total, verb='full', logfile=sys.stderr, unit='reads' ,mem=False, printint=0):
		import time
		self.total = total
		self.current = 0
		self.type = verb
		self.logfile = logfile
		self.ltime = time.time()
		self.lcurrent = self.current
		self.lpercentage = 0
		if verb == 'full': self.printint = 5
		elif verb == 'minimal':self.printint = 5
		self.unit = unit
		self.mem = mem
		if printint: self.printint = printint

	def __enter__(self):
		if self.type == 'minimal': self.logfile.write('0%                 50%                 100%\n')
		#                                              ....................................................................................

	def update(self):
		import time
		self.current += 1
		self.percentage = int(round(100*float(self.current)/self.total))
		if self.percentage % self.printint == 0 and self.percentage != self.lpercentage:
			self.stf=int(round((self.total-self.current)/((self.current-self.lcurrent)/(time.time()-self.ltime))))
			if self.type == 'full': self.logfile.write(
				'#Progress => '+str(self.percentage)+'%, '+
				str( round((self.current-self.lcurrent)/(time.time()-self.ltime),2) )+' '+self.unit+'/second, '+
				time.strftime("%A, %d %b %Y %H:%M:%S",time.localtime())+
				', left: '+str(self.stf/60/60)+'h '+str(self.stf/60%60)+'min '+str(self.stf%60)+'s')
			if self.mem:
				import resource
				self.logfile.write(', using '+str((resource.getrusage(resource.RUSAGE_SELF).ru_maxrss+resource.getrusage(resource.RUSAGE_CHILDREN).ru_maxrss)/1024)+' ('+str(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024)+') MB.\n')
			else:	self.logfile.write('\n')
			if self.type == 'minimal': self.logfile.write('..')
			self.ltime = time.time()
			self.lcurrent = self.current
			self.lpercentage = self.percentage

	def __exit__(self, *args):
		self.logfile.write('\n')

##############################
#  Check if run or imported  #
##############################

if __name__ == "__main__":
    main()

###################
#  END of script  #
###################